{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Enhanced Analytics-Ready Features with SQL Semantics in Memgraph\n\nThis notebook explores the enhanced analytics-ready optimization features with **SQL semantics metadata** - essential for migration agents and automated code generation.\n\n## üÜï Enhanced for Migration Agents\n- **SQL Semantics Materialized Views**: Pre-computed JOIN relationships and column aliases\n- **Migration-Ready Performance**: Sub-second access to complex SQL analysis\n- **Platform-Specific Insights**: Organized data for Spark, dbt, Pandas code generation\n- **Categories Table Validation**: Verify the critical table extraction improvements\n\n## What We'll Learn\n- How to access SQL semantics from materialized views\n- Performance comparison: Enhanced views vs raw SQL parsing\n- Migration agent data consumption patterns\n- JOIN relationship analysis for automated code generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import mgclient\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to Memgraph\n",
    "mg = mgclient.connect(host='localhost', port=7687, username='', password='')\n",
    "print(\"‚úÖ Connected to Memgraph\")\n",
    "\n",
    "def execute_query(query, description=None, time_it=False):\n",
    "    \"\"\"Execute a Cypher query and return results as DataFrame.\"\"\"\n",
    "    if description:\n",
    "        print(f\"\\nüîç {description}\")\n",
    "        if not time_it:\n",
    "            print(f\"Query: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time() if time_it else None\n",
    "    \n",
    "    cursor = mg.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    end_time = time.time() if time_it else None\n",
    "    \n",
    "    if time_it:\n",
    "        execution_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "        print(f\"‚è±Ô∏è Execution time: {execution_time:.2f}ms\")\n",
    "    \n",
    "    if results:\n",
    "        columns = [desc.name for desc in cursor.description] if cursor.description else ['result']\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "        print(f\"Found {len(df)} results\")\n",
    "        return df, execution_time if time_it else None\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "        return pd.DataFrame(), execution_time if time_it else None\n",
    "\n",
    "def pretty_print_json(data, max_length=1000):\n",
    "    \"\"\"Pretty print JSON data.\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        try:\n",
    "            data = json.loads(data)\n",
    "        except:\n",
    "            print(data[:max_length] + \"...\" if len(data) > max_length else data)\n",
    "            return\n",
    "    \n",
    "    json_str = json.dumps(data, indent=2)\n",
    "    if len(json_str) > max_length:\n",
    "        json_str = json_str[:max_length] + \"\\n...truncated...\"\n",
    "    print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Verify Analytics-Ready Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the graph is analytics-ready\n",
    "metadata_df, _ = execute_query(\n",
    "    \"\"\"MATCH (m:Node {node_type: 'graph_metadata'})\n",
    "       RETURN m.name as metadata_name,\n",
    "              m.id as metadata_id,\n",
    "              m.properties as properties\"\"\",\n",
    "    \"Checking analytics-ready status\"\n",
    ")\n",
    "\n",
    "if not metadata_df.empty:\n",
    "    metadata = metadata_df.iloc[0]\n",
    "    \n",
    "    # Parse the properties JSON\n",
    "    try:\n",
    "        props = json.loads(metadata['properties']) if isinstance(metadata['properties'], str) else metadata['properties']\n",
    "        ready_status = props.get('ready_for_applications', 'unknown')\n",
    "        optimization_level = props.get('optimization_level', 'unknown')\n",
    "        use_cases = props.get('supported_use_cases', [])\n",
    "    except:\n",
    "        ready_status = 'unknown'\n",
    "        optimization_level = 'unknown'\n",
    "        use_cases = []\n",
    "    \n",
    "    print(f\"\\nüéâ Graph Analytics Status:\")\n",
    "    print(f\"  ‚úÖ Ready for Applications: {ready_status}\")\n",
    "    print(f\"  üöÄ Optimization Level: {optimization_level}\")\n",
    "    \n",
    "    if use_cases:\n",
    "        print(f\"  üéØ Supported Use Cases:\")\n",
    "        for use_case in use_cases:\n",
    "            print(f\"    ‚Ä¢ {use_case.replace('_', ' ').title()}\")\n",
    "else:\n",
    "    print(\"‚ùå Graph is not analytics-ready. No metadata found.\")\n",
    "    print(\"Run: METAZCODE_DB_BACKEND=memgraph metazcode full --path ssis_northwind\")\n",
    "\n",
    "display(metadata_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed metadata information\n",
    "if not metadata_df.empty:\n",
    "    detailed_metadata_df, _ = execute_query(\n",
    "        \"\"\"MATCH (m:Node {node_type: 'graph_metadata'})\n",
    "           RETURN m.properties as full_metadata\"\"\",\n",
    "        \"Detailed graph metadata\"\n",
    "    )\n",
    "    \n",
    "    if not detailed_metadata_df.empty:\n",
    "        metadata_props = detailed_metadata_df.iloc[0]['full_metadata']\n",
    "        print(\"\\nüìä Complete Graph Metadata:\")\n",
    "        pretty_print_json(metadata_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Explore Materialized Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available materialized views\n",
    "views_df, _ = execute_query(\n",
    "    \"\"\"MATCH (v:Node {node_type: 'materialized_view'})\n",
    "       RETURN v.name as view_name,\n",
    "              v.id as view_id,\n",
    "              v.properties as record_count,\n",
    "              v.properties as created_at,\n",
    "              v.properties as version\n",
    "       ORDER BY v.name\"\"\",\n",
    "    \"Available materialized views\"\n",
    ")\n",
    "\n",
    "if not views_df.empty:\n",
    "    print(f\"\\nüìã Found {len(views_df)} Materialized Views:\")\n",
    "    for _, view in views_df.iterrows():\n",
    "        print(f\"  ‚Ä¢ {view['view_name']}: {view['record_count']} records (v{view['version']})\")\n",
    "    \n",
    "    display(views_df)\n",
    "else:\n",
    "    print(\"‚ùå No materialized views found.\")\n",
    "    print(\"The graph may not be analytics-ready or may need to be regenerated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using Materialized Views\n",
    "\n",
    "Let's explore each materialized view and understand what data it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to access materialized view data\n",
    "def get_materialized_view_data(view_name, description=None):\n",
    "    \"\"\"Get data from a materialized view.\"\"\"\n",
    "    query = f\"\"\"MATCH (v:Node {{id: 'view:{view_name}'}})\n",
    "                RETURN v.properties as view_data,\n",
    "                       v.properties as record_count\"\"\"\n",
    "    \n",
    "    df, exec_time = execute_query(query, description or f\"Accessing {view_name} materialized view\", time_it=True)\n",
    "    \n",
    "    if not df.empty and df.iloc[0]['view_data']:\n",
    "        try:\n",
    "            view_data = json.loads(df.iloc[0]['view_data'])\n",
    "            record_count = df.iloc[0]['record_count']\n",
    "            print(f\"üìä Records in view: {record_count}\")\n",
    "            return view_data, exec_time\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing view data: {e}\")\n",
    "            return None, exec_time\n",
    "    else:\n",
    "        print(\"‚ùå View not found or empty\")\n",
    "        return None, exec_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SQL Operations Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore SQL Operations Catalog\n",
    "sql_ops_data, sql_ops_time = get_materialized_view_data('sql_operations_catalog', \n",
    "                                                        'SQL Operations Catalog - Pre-computed SQL transformations')\n",
    "\n",
    "if sql_ops_data:\n",
    "    print(f\"\\nüîç Sample SQL Operations:\")\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    sql_ops_df = pd.DataFrame(sql_ops_data)\n",
    "    \n",
    "    if not sql_ops_df.empty:\n",
    "        print(f\"\\nColumns available: {list(sql_ops_df.columns)}\")\n",
    "        \n",
    "        # Show first few operations\n",
    "        for i, op in sql_ops_df.head(3).iterrows():\n",
    "            print(f\"\\nüìù Operation: {op.get('operation_name', 'Unknown')}\")\n",
    "            print(f\"   Type: {op.get('sql_type', 'Unknown')}\")\n",
    "            print(f\"   Tables: {len(op.get('affected_tables', []))}\")\n",
    "            if op.get('complexity_indicators'):\n",
    "                complexity = op['complexity_indicators']\n",
    "                print(f\"   Complexity: {complexity.get('table_count', 0)} tables, \"\n",
    "                      f\"Joins: {complexity.get('has_joins', False)}, \"\n",
    "                      f\"Subqueries: {complexity.get('has_subqueries', False)}\")\n",
    "        \n",
    "        display(sql_ops_df.head())\n",
    "    else:\n",
    "        print(\"No SQL operations found in the catalog.\")\n",
    "else:\n",
    "    print(\"SQL Operations Catalog is empty or not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cross-Package Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Cross-Package Dependencies\n",
    "deps_data, deps_time = get_materialized_view_data('cross_package_dependencies',\n",
    "                                                 'Cross-Package Dependencies - Pre-computed package relationships')\n",
    "\n",
    "if deps_data:\n",
    "    deps_df = pd.DataFrame(deps_data)\n",
    "    \n",
    "    if not deps_df.empty:\n",
    "        print(f\"\\nüîó Package Dependencies:\")\n",
    "        for _, dep in deps_df.iterrows():\n",
    "            print(f\"  ‚Ä¢ {dep.get('source_package', 'Unknown')} depends on {dep.get('target_package', 'Unknown')}\")\n",
    "            print(f\"    Risk Level: {dep.get('risk_level', 'Unknown')}\")\n",
    "            if dep.get('shared_resources'):\n",
    "                print(f\"    Shared Resources: {len(dep['shared_resources'])}\")\n",
    "        \n",
    "        display(deps_df)\n",
    "    else:\n",
    "        print(\"No cross-package dependencies found.\")\n",
    "        print(\"This means packages are likely independent.\")\n",
    "else:\n",
    "    print(\"Cross-Package Dependencies view is empty or not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Shared Resources Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Shared Resources Analysis\n",
    "shared_data, shared_time = get_materialized_view_data('shared_resources_analysis',\n",
    "                                                     'Shared Resources Analysis - Resources used by multiple packages')\n",
    "\n",
    "if shared_data:\n",
    "    shared_df = pd.DataFrame(shared_data)\n",
    "    \n",
    "    if not shared_df.empty:\n",
    "        print(f\"\\n‚ö†Ô∏è Shared Resources (Potential Bottlenecks):\")\n",
    "        for _, resource in shared_df.iterrows():\n",
    "            print(f\"  ‚Ä¢ {resource.get('resource_name', 'Unknown')}\")\n",
    "            print(f\"    Used by {resource.get('package_count', 0)} packages: {resource.get('sharing_packages', [])}\")\n",
    "            print(f\"    Contention Risk: {resource.get('contention_risk', 'Unknown')}\")\n",
    "            print()\n",
    "        \n",
    "        # Create a visualization of shared resources\n",
    "        if 'contention_risk' in shared_df.columns:\n",
    "            risk_counts = shared_df['contention_risk'].value_counts()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            risk_counts.plot(kind='pie', autopct='%1.1f%%', title='Shared Resources by Risk Level')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            shared_df['package_count'].hist(bins=10, title='Distribution of Package Usage')\n",
    "            plt.xlabel('Number of Packages Using Resource')\n",
    "            plt.ylabel('Number of Resources')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        display(shared_df)\n",
    "    else:\n",
    "        print(\"No shared resources found.\")\n",
    "        print(\"This means each resource is used by only one package.\")\n",
    "else:\n",
    "    print(\"Shared Resources Analysis view is empty or not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Lineage Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Data Lineage Catalog\n",
    "lineage_data, lineage_time = get_materialized_view_data('data_lineage_catalog',\n",
    "                                                       'Data Lineage Catalog - Complete data flow mapping')\n",
    "\n",
    "if lineage_data:\n",
    "    lineage_df = pd.DataFrame(lineage_data)\n",
    "    \n",
    "    if not lineage_df.empty:\n",
    "        print(f\"\\nüîÑ Data Lineage Summary:\")\n",
    "        print(f\"Total lineage relationships: {len(lineage_df)}\")\n",
    "        \n",
    "        if 'lineage_direction' in lineage_df.columns:\n",
    "            direction_counts = lineage_df['lineage_direction'].value_counts()\n",
    "            print(f\"\\nLineage directions:\")\n",
    "            for direction, count in direction_counts.items():\n",
    "                print(f\"  ‚Ä¢ {direction}: {count} relationships\")\n",
    "        \n",
    "        if 'relationship_type' in lineage_df.columns:\n",
    "            rel_type_counts = lineage_df['relationship_type'].value_counts()\n",
    "            print(f\"\\nRelationship types:\")\n",
    "            for rel_type, count in rel_type_counts.items():\n",
    "                print(f\"  ‚Ä¢ {rel_type}: {count} relationships\")\n",
    "        \n",
    "        print(f\"\\nüìù Sample lineage relationships:\")\n",
    "        for _, lineage in lineage_df.head(5).iterrows():\n",
    "            print(f\"  ‚Ä¢ {lineage.get('source_name', 'Unknown')} --[{lineage.get('relationship_type', 'UNKNOWN')}]--> {lineage.get('target_name', 'Unknown')}\")\n",
    "        \n",
    "        display(lineage_df.head(10))\n",
    "    else:\n",
    "        print(\"No data lineage relationships found.\")\n",
    "else:\n",
    "    print(\"Data Lineage Catalog view is empty or not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Business Rules Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Business Rules Catalog\n",
    "rules_data, rules_time = get_materialized_view_data('business_rules_catalog',\n",
    "                                                   'Business Rules Catalog - Extracted business logic')\n",
    "\n",
    "if rules_data:\n",
    "    rules_df = pd.DataFrame(rules_data)\n",
    "    \n",
    "    if not rules_df.empty:\n",
    "        print(f\"\\nüìã Business Rules Summary:\")\n",
    "        print(f\"Operations with business rules: {len(rules_df)}\")\n",
    "        \n",
    "        total_rules = sum([len(op.get('rules', [])) for _, op in rules_df.iterrows()])\n",
    "        print(f\"Total business rules: {total_rules}\")\n",
    "        \n",
    "        print(f\"\\nüìù Sample business rules:\")\n",
    "        for _, operation in rules_df.head(3).iterrows():\n",
    "            print(f\"\\nüîß Operation: {operation.get('operation_name', 'Unknown')}\")\n",
    "            rules = operation.get('rules', [])\n",
    "            print(f\"   Rules count: {len(rules)}\")\n",
    "            \n",
    "            for rule in rules[:2]:  # Show first 2 rules\n",
    "                print(f\"   ‚Ä¢ Type: {rule.get('rule_type', 'Unknown')}\")\n",
    "                print(f\"     Description: {rule.get('description', 'No description')[:100]}...\")\n",
    "        \n",
    "        display(rules_df[['operation_name', 'rule_count']].head())\n",
    "    else:\n",
    "        print(\"No business rules found.\")\n",
    "        print(\"This could mean operations don't contain complex business logic or it wasn't extracted.\")\n",
    "else:\n",
    "    print(\"Business Rules Catalog view is empty or not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Graph Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Graph Summary Statistics\n",
    "stats_data, stats_time = get_materialized_view_data('graph_summary_stats',\n",
    "                                                   'Graph Summary Statistics - High-level system metrics')\n",
    "\n",
    "if stats_data:\n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    \n",
    "    if not stats_df.empty:\n",
    "        stats = stats_df.iloc[0]  # Should be just one summary record\n",
    "        \n",
    "        print(f\"\\nüìä Graph Summary Statistics:\")\n",
    "        print(f\"Metric: {stats.get('metric_name', 'Unknown')}\")\n",
    "        \n",
    "        if 'statistics' in stats:\n",
    "            statistics = stats['statistics']\n",
    "            print(f\"\\nüìà System Metrics:\")\n",
    "            for metric, value in statistics.items():\n",
    "                print(f\"  ‚Ä¢ {metric.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "        display(stats_df)\n",
    "    else:\n",
    "        print(\"No summary statistics found.\")\n",
    "else:\n",
    "    print(\"Graph Summary Statistics view is empty or not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Complexity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Complexity Metrics\n",
    "complexity_data, complexity_time = get_materialized_view_data('complexity_metrics',\n",
    "                                                            'Complexity Metrics - Migration complexity scoring')\n",
    "\n",
    "if complexity_data:\n",
    "    complexity_df = pd.DataFrame(complexity_data)\n",
    "    \n",
    "    if not complexity_df.empty:\n",
    "        complexity = complexity_df.iloc[0]  # Should be one complexity record\n",
    "        \n",
    "        print(f\"\\nüéØ Migration Complexity Analysis:\")\n",
    "        print(f\"Metric: {complexity.get('metric_name', 'Unknown')}\")\n",
    "        \n",
    "        metrics = ['package_count', 'operation_count', 'cross_package_dependencies', \n",
    "                  'shared_resource_count', 'complexity_score']\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in complexity:\n",
    "                print(f\"  ‚Ä¢ {metric.replace('_', ' ').title()}: {complexity[metric]}\")\n",
    "        \n",
    "        # Create a simple complexity visualization\n",
    "        if 'complexity_score' in complexity:\n",
    "            score = complexity['complexity_score']\n",
    "            \n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.bar(['Complexity Score'], [score], color='skyblue')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title(f'SSIS System Complexity Score: {score}')\n",
    "            \n",
    "            # Add interpretation\n",
    "            if score < 5:\n",
    "                interpretation = \"Low Complexity - Easy to migrate\"\n",
    "                color = 'green'\n",
    "            elif score < 15:\n",
    "                interpretation = \"Medium Complexity - Moderate migration effort\"\n",
    "                color = 'orange'\n",
    "            else:\n",
    "                interpretation = \"High Complexity - Significant migration effort\"\n",
    "                color = 'red'\n",
    "            \n",
    "            plt.text(0, score + 0.1, interpretation, ha='center', color=color, weight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        display(complexity_df)\n",
    "    else:\n",
    "        print(\"No complexity metrics found.\")\n",
    "else:\n",
    "    print(\"Complexity Metrics view is empty or not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Performance Comparison\n",
    "\n",
    "Let's compare the performance of using materialized views vs. computing the same data from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Materialized view vs raw computation\n",
    "print(\"‚è±Ô∏è Performance Comparison: Materialized Views vs Raw Queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Using materialized view (fast)\n",
    "print(\"\\nüöÄ Method 1: Using Materialized View\")\n",
    "start_time = time.time()\n",
    "view_data, _ = get_materialized_view_data('sql_operations_catalog')\n",
    "view_time = (time.time() - start_time) * 1000\n",
    "view_count = len(view_data) if view_data else 0\n",
    "print(f\"Found {view_count} SQL operations in {view_time:.2f}ms\")\n",
    "\n",
    "# Method 2: Computing from scratch (slower)\n",
    "print(\"\\nüêå Method 2: Computing from Raw Graph Data\")\n",
    "raw_query = \"\"\"\n",
    "    MATCH (n:Node {node_type: 'operation'})\n",
    "    WHERE n.properties CONTAINS 'sql_transformation'\n",
    "    RETURN n.id as operation_id,\n",
    "           n.name as operation_name,\n",
    "           n.properties as operation_details\n",
    "\"\"\"\n",
    "\n",
    "raw_df, raw_time = execute_query(raw_query, \"Computing SQL operations from scratch\", time_it=True)\n",
    "raw_count = len(raw_df) if not raw_df.empty else 0\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nüìä Performance Results:\")\n",
    "print(f\"  ‚Ä¢ Materialized View: {view_count} results in {view_time:.2f}ms\")\n",
    "print(f\"  ‚Ä¢ Raw Query: {raw_count} results in {raw_time:.2f}ms\" if raw_time else \"  ‚Ä¢ Raw Query: Failed or no results\")\n",
    "\n",
    "if raw_time and view_time > 0:\n",
    "    speedup = raw_time / view_time\n",
    "    print(f\"  ‚Ä¢ Speedup: {speedup:.1f}x faster with materialized views\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    methods = ['Materialized View', 'Raw Query']\n",
    "    times = [view_time, raw_time]\n",
    "    colors = ['green', 'red']\n",
    "    \n",
    "    bars = plt.bar(methods, times, color=colors, alpha=0.7)\n",
    "    plt.ylabel('Execution Time (ms)')\n",
    "    plt.title('Performance Comparison: Materialized Views vs Raw Queries')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.01, \n",
    "                f'{time_val:.1f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.text(0.5, max(times)*0.8, f'{speedup:.1f}x Faster', ha='center', \n",
    "            transform=plt.gca().transAxes, fontsize=16, fontweight='bold', \n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Could not compare performance - one method failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building a Simple Migration Tool\n",
    "\n",
    "Let's create a simple example of how a migration engineer would use these materialized views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple Migration Assessment Tool\n",
    "class SSISMigrationAssessment:\n",
    "    \"\"\"Simple migration assessment using materialized views.\"\"\"\n",
    "    \n",
    "    def __init__(self, mg_connection):\n",
    "        self.mg = mg_connection\n",
    "    \n",
    "    def get_view_data(self, view_name):\n",
    "        \"\"\"Get data from a materialized view.\"\"\"\n",
    "        query = f\"\"\"MATCH (v:Node {{id: 'view:{view_name}'}})\n",
    "                    RETURN v.properties as data\"\"\"\n",
    "        cursor = self.mg.cursor()\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchone()\n",
    "        \n",
    "        if result and result[0]:\n",
    "            return json.loads(result[0])\n",
    "        return []\n",
    "    \n",
    "    def assess_migration_complexity(self):\n",
    "        \"\"\"Assess overall migration complexity.\"\"\"\n",
    "        print(\"üéØ SSIS Migration Assessment Report\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Get complexity metrics\n",
    "        complexity_data = self.get_view_data('complexity_metrics')\n",
    "        if complexity_data:\n",
    "            complexity = complexity_data[0]\n",
    "            print(f\"\\nüìä System Overview:\")\n",
    "            print(f\"  ‚Ä¢ Total Packages: {complexity.get('package_count', 0)}\")\n",
    "            print(f\"  ‚Ä¢ Total Operations: {complexity.get('operation_count', 0)}\")\n",
    "            print(f\"  ‚Ä¢ Cross-Package Dependencies: {complexity.get('cross_package_dependencies', 0)}\")\n",
    "            print(f\"  ‚Ä¢ Shared Resources: {complexity.get('shared_resource_count', 0)}\")\n",
    "            print(f\"  ‚Ä¢ Complexity Score: {complexity.get('complexity_score', 0):.1f}\")\n",
    "        \n",
    "        # Analyze SQL operations\n",
    "        sql_ops = self.get_view_data('sql_operations_catalog')\n",
    "        if sql_ops:\n",
    "            print(f\"\\nüîç SQL Operations Analysis:\")\n",
    "            print(f\"  ‚Ä¢ Total SQL Operations: {len(sql_ops)}\")\n",
    "            \n",
    "            # Analyze complexity\n",
    "            complex_ops = [op for op in sql_ops if op.get('complexity_indicators', {}).get('has_joins', False)]\n",
    "            subquery_ops = [op for op in sql_ops if op.get('complexity_indicators', {}).get('has_subqueries', False)]\n",
    "            \n",
    "            print(f\"  ‚Ä¢ Operations with JOINs: {len(complex_ops)}\")\n",
    "            print(f\"  ‚Ä¢ Operations with Subqueries: {len(subquery_ops)}\")\n",
    "        \n",
    "        # Check shared resources\n",
    "        shared_resources = self.get_view_data('shared_resources_analysis')\n",
    "        if shared_resources:\n",
    "            high_risk = [r for r in shared_resources if r.get('contention_risk') == 'HIGH']\n",
    "            print(f\"\\n‚ö†Ô∏è Risk Assessment:\")\n",
    "            print(f\"  ‚Ä¢ Total Shared Resources: {len(shared_resources)}\")\n",
    "            print(f\"  ‚Ä¢ High-Risk Resources: {len(high_risk)}\")\n",
    "            \n",
    "            if high_risk:\n",
    "                print(f\"  ‚Ä¢ High-Risk Resources List:\")\n",
    "                for resource in high_risk[:3]:  # Show top 3\n",
    "                    print(f\"    - {resource.get('resource_name', 'Unknown')}: {resource.get('package_count', 0)} packages\")\n",
    "        \n",
    "        # Migration recommendations\n",
    "        print(f\"\\nüí° Migration Recommendations:\")\n",
    "        if complexity_data and complexity_data[0].get('complexity_score', 0) < 5:\n",
    "            print(f\"  ‚Ä¢ ‚úÖ Low complexity - Good candidate for automated migration\")\n",
    "        elif complexity_data and complexity_data[0].get('complexity_score', 0) < 15:\n",
    "            print(f\"  ‚Ä¢ ‚ö†Ô∏è Medium complexity - Plan for manual review of complex operations\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ üö® High complexity - Recommend phased migration approach\")\n",
    "        \n",
    "        if shared_resources and len([r for r in shared_resources if r.get('contention_risk') == 'HIGH']) > 0:\n",
    "            print(f\"  ‚Ä¢ ‚ö†Ô∏è Address shared resource dependencies before migration\")\n",
    "        \n",
    "        print(f\"  ‚Ä¢ üìã Review {len(sql_ops) if sql_ops else 0} SQL operations for cloud compatibility\")\n",
    "\n",
    "# Run the assessment\n",
    "assessment = SSISMigrationAssessment(mg)\n",
    "assessment.assess_migration_complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Understanding the Analytics-Ready Advantage\n",
    "\n",
    "Let's summarize what we've learned about the analytics-ready features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of analytics-ready benefits\n",
    "print(\"üéâ Analytics-Ready Features Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Collect timing data from our tests\n",
    "view_access_times = {\n",
    "    'SQL Operations': sql_ops_time if 'sql_ops_time' in locals() else None,\n",
    "    'Dependencies': deps_time if 'deps_time' in locals() else None,\n",
    "    'Shared Resources': shared_time if 'shared_time' in locals() else None,\n",
    "    'Data Lineage': lineage_time if 'lineage_time' in locals() else None,\n",
    "    'Business Rules': rules_time if 'rules_time' in locals() else None,\n",
    "    'Summary Stats': stats_time if 'stats_time' in locals() else None,\n",
    "    'Complexity': complexity_time if 'complexity_time' in locals() else None\n",
    "}\n",
    "\n",
    "print(f\"\\n‚ö° Performance Results:\")\n",
    "total_time = 0\n",
    "successful_queries = 0\n",
    "\n",
    "for view_name, exec_time in view_access_times.items():\n",
    "    if exec_time is not None:\n",
    "        print(f\"  ‚Ä¢ {view_name}: {exec_time:.1f}ms\")\n",
    "        total_time += exec_time\n",
    "        successful_queries += 1\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ {view_name}: Not available\")\n",
    "\n",
    "if successful_queries > 0:\n",
    "    avg_time = total_time / successful_queries\n",
    "    print(f\"\\nüìä Performance Summary:\")\n",
    "    print(f\"  ‚Ä¢ Average access time: {avg_time:.1f}ms per view\")\n",
    "    print(f\"  ‚Ä¢ Total time for all views: {total_time:.1f}ms\")\n",
    "    print(f\"  ‚Ä¢ Successful views accessed: {successful_queries}/7\")\n",
    "\n",
    "print(f\"\\nüéØ Key Benefits of Analytics-Ready Optimization:\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Sub-second access to pre-computed analysis\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ No need to rebuild indexes for each application\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Consistent data structure across tools\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Ready for migration, compliance, and governance applications\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Supports multiple downstream tools simultaneously\")\n",
    "\n",
    "print(f\"\\nüìö What You've Learned:\")\n",
    "print(f\"  ‚Ä¢ How to verify if a graph is analytics-ready\")\n",
    "print(f\"  ‚Ä¢ How to access and use materialized views\")\n",
    "print(f\"  ‚Ä¢ Performance advantages of pre-computed data\")\n",
    "print(f\"  ‚Ä¢ Building simple migration tools with the data\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"  ‚Ä¢ Open 04_advanced_queries.ipynb for complex analysis patterns\")\n",
    "print(f\"  ‚Ä¢ Open 05_migration_analysis.ipynb for practical migration scenarios\")\n",
    "print(f\"  ‚Ä¢ Build your own migration tools using these materialized views!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What Analytics-Ready Means:\n",
    "1. **Pre-computed Views**: Data is processed once during analysis, not every time you query\n",
    "2. **Instant Access**: Migration tools get results in milliseconds, not minutes\n",
    "3. **Consistent Structure**: All applications use the same optimized data format\n",
    "4. **No Rebuilding**: Unlike search indexes, materialized views persist between sessions\n",
    "\n",
    "### Available Materialized Views:\n",
    "- **SQL Operations Catalog**: Ready-to-use SQL transformation analysis\n",
    "- **Cross-Package Dependencies**: Pre-computed dependency relationships  \n",
    "- **Shared Resources Analysis**: Bottleneck and risk identification\n",
    "- **Data Lineage Catalog**: Complete data flow mapping\n",
    "- **Business Rules Catalog**: Extracted business logic\n",
    "- **Graph Summary Statistics**: High-level system metrics\n",
    "- **Complexity Metrics**: Migration effort estimation\n",
    "\n",
    "### For Migration Engineers:\n",
    "This analytics-ready approach means you can build interactive migration tools that:\n",
    "- Start instantly (no 3-minute wait times)\n",
    "- Access rich, pre-processed data\n",
    "- Focus on migration logic, not data processing\n",
    "- Scale to enterprise-size SSIS environments\n",
    "\n",
    "The \"Week 1 vs Week 2\" performance problem is solved - your tools work fast immediately!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}