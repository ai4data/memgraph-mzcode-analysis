{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SSIS to Modern Platform Migration Analysis with SQL Semantics\n\nThis notebook demonstrates practical migration scenarios using **enhanced SQL semantics metadata** for automated code generation.\n\n## ðŸŽ¯ Migration Agent Focus\n- **Real-world Migration Scenarios**: Product.dtsx with Categories table JOIN\n- **Platform Code Generation**: Spark, dbt, Pandas migration examples\n- **SQL Semantics Consumption**: How migration agents use JOIN relationships\n- **Categories Table Success**: Validate the critical extraction fix\n\n## Migration Platforms Covered\n- **Apache Spark (PySpark)**: DataFrame operations with JOIN relationships\n- **dbt (Data Build Tool)**: SQL model generation with proper semantics\n- **Python/Pandas**: DataFrame merge operations with aliases\n- **Migration Assessment**: Effort estimation and complexity analysis\n\n## Prerequisites\n- Enhanced SSIS Northwind data with SQL semantics in Memgraph\n- Understanding of graph structure from previous notebooks\n- Basic knowledge of target migration platforms"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup for Migration Analysis\nimport mgclient\nimport pandas as pd\nimport json\nimport time\nfrom datetime import datetime\n\n# Connect to Memgraph\nmg = mgclient.connect(host='localhost', port=7687, username='', password='')\nprint(\"âœ… Connected to Memgraph for Migration Analysis\")\n\ndef execute_query(query, description=None):\n    \"\"\"Execute a Cypher query for migration analysis.\"\"\"\n    if description:\n        print(f\"\\nðŸ” {description}\")\n        print(f\"Query: {query}\")\n        print(\"-\" * 50)\n    \n    cursor = mg.cursor()\n    cursor.execute(query)\n    results = cursor.fetchall()\n    \n    if results:\n        columns = [desc.name for desc in cursor.description] if cursor.description else ['result']\n        df = pd.DataFrame(results, columns=columns)\n        print(f\"Found {len(df)} results\")\n        return df\n    else:\n        print(\"No results found.\")\n        return pd.DataFrame()\n\ndef generate_migration_code(sql_semantics, platform=\"spark\"):\n    \"\"\"Generate platform-specific migration code from SQL semantics.\"\"\"\n    if not sql_semantics or not sql_semantics.get('joins'):\n        return f\"# No JOIN relationships found for {platform} migration\"\n    \n    join = sql_semantics['joins'][0]  # Use first join\n    tables = sql_semantics.get('tables', [])\n    columns = sql_semantics.get('columns', [])\n    \n    if platform == \"spark\":\n        return f\"\"\"# Generated Spark Migration Code\n# Original SQL: {sql_semantics.get('original_query', 'N/A')[:80]}...\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n# Load DataFrames\ndf_{tables[0]['name'].lower()} = spark.table('{tables[0]['name']}').alias('{tables[0]['alias']}')\ndf_{tables[1]['name'].lower()} = spark.table('{tables[1]['name']}').alias('{tables[1]['alias']}')\n\n# Perform JOIN\nresult_df = df_{tables[0]['name'].lower()}.join(\n    df_{tables[1]['name'].lower()},\n    col('{join['condition'].split('=')[0].strip()}') == col('{join['condition'].split('=')[1].strip()}'),\n    'inner'\n)\n\n# Select columns\nresult_df = result_df.select({', '.join([f\"col('{col['expression']}')\" + (f\".alias('{col['alias']}')\" if col.get('alias') else \"\") for col in columns[:5]])})\n\nresult_df.show()\"\"\"\n    \n    elif platform == \"dbt\":\n        return f\"\"\"-- Generated dbt Model\n-- Original SQL: {sql_semantics.get('original_query', 'N/A')[:80]}...\n\nSELECT\n{', '.join([f\"    {col['expression']}\" + (f\" AS {col['alias']}\" if col.get('alias') else \"\") for col in columns[:5]])}\nFROM {{{{ ref('{tables[0]['name'].lower()}') }}}} {tables[0]['alias']}\n{join['join_type']} {{{{ ref('{tables[1]['name'].lower()}') }}}} {tables[1]['alias']}\n    ON {join['condition']}\"\"\"\n    \n    elif platform == \"pandas\":\n        return f\"\"\"# Generated Pandas Migration Code\n# Original SQL: {sql_semantics.get('original_query', 'N/A')[:80]}...\n\nimport pandas as pd\n\n# Load DataFrames (replace with actual data loading)\ndf_{tables[0]['name'].lower()} = pd.read_sql(\"SELECT * FROM {tables[0]['name']}\", connection)\ndf_{tables[1]['name'].lower()} = pd.read_sql(\"SELECT * FROM {tables[1]['name']}\", connection)\n\n# Perform JOIN\nresult_df = pd.merge(\n    df_{tables[0]['name'].lower()},\n    df_{tables[1]['name'].lower()},\n    left_on='{join['condition'].split('=')[0].strip().split('.')[-1]}',\n    right_on='{join['condition'].split('=')[1].strip().split('.')[-1]}',\n    how='inner',\n    suffixes=('_{tables[0]['alias']}', '_{tables[1]['alias']}')\n)\n\nprint(result_df.head())\"\"\"\n    \n    return f\"# Platform {platform} not implemented\"\n\nprint(\"ðŸš€ Migration analysis toolkit ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Migration Complexity Assessment\n",
    "\n",
    "Assess the overall complexity of migrating the SSIS system to a new platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive complexity metrics using materialized view\n",
    "complexity_query = \"\"\"\n",
    "MATCH (v:Node {id: 'view:complexity_metrics'})\n",
    "RETURN JSON_EXTRACT(v.properties, '$.data') as complexity_data\n",
    "\"\"\"\n",
    "\n",
    "complexity_result = execute_and_fetch(complexity_query)\n",
    "\n",
    "if complexity_result and complexity_result[0][0]:\n",
    "    complexity_data = json.loads(complexity_result[0][0])\n",
    "    \n",
    "    print(\"ðŸ“Š Migration Complexity Assessment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze complexity distribution\n",
    "    complexity_levels = {'LOW': 0, 'MEDIUM': 0, 'HIGH': 0, 'CRITICAL': 0}\n",
    "    total_effort_days = 0\n",
    "    high_risk_packages = []\n",
    "    \n",
    "    for item in complexity_data:\n",
    "        level = item.get('complexity_level', 'UNKNOWN')\n",
    "        if level in complexity_levels:\n",
    "            complexity_levels[level] += 1\n",
    "        \n",
    "        # Estimate effort in days\n",
    "        effort_days = item.get('estimated_effort_days', 0)\n",
    "        total_effort_days += effort_days\n",
    "        \n",
    "        if level in ['HIGH', 'CRITICAL']:\n",
    "            high_risk_packages.append({\n",
    "                'name': item.get('package_name', 'Unknown'),\n",
    "                'level': level,\n",
    "                'effort': effort_days,\n",
    "                'reasons': item.get('complexity_reasons', [])\n",
    "            })\n",
    "    \n",
    "    total_packages = sum(complexity_levels.values())\n",
    "    \n",
    "    print(f\"ðŸ“¦ Total Packages: {total_packages}\")\n",
    "    print(f\"â±ï¸  Estimated Total Effort: {total_effort_days} days ({total_effort_days/5:.1f} weeks)\")\n",
    "    print(f\"ðŸ’° Estimated Cost (at $800/day): ${total_effort_days * 800:,}\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Complexity Distribution:\")\n",
    "    for level, count in complexity_levels.items():\n",
    "        percentage = (count / total_packages * 100) if total_packages > 0 else 0\n",
    "        print(f\"  {level:8}: {count:2} packages ({percentage:4.1f}%)\")\n",
    "    \n",
    "    # Risk assessment\n",
    "    high_risk_ratio = (complexity_levels['HIGH'] + complexity_levels['CRITICAL']) / total_packages\n",
    "    if high_risk_ratio > 0.3:\n",
    "        risk_level = \"ðŸ”´ HIGH RISK\"\n",
    "        recommendation = \"Consider phased migration approach\"\n",
    "    elif high_risk_ratio > 0.15:\n",
    "        risk_level = \"ðŸŸ¡ MEDIUM RISK\"\n",
    "        recommendation = \"Plan additional testing and validation\"\n",
    "    else:\n",
    "        risk_level = \"ðŸŸ¢ LOW RISK\"\n",
    "        recommendation = \"Suitable for standard migration approach\"\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Migration Risk Level: {risk_level}\")\n",
    "    print(f\"ðŸ’¡ Recommendation: {recommendation}\")\n",
    "    \n",
    "    # Show high-risk packages\n",
    "    if high_risk_packages:\n",
    "        print(f\"\\nâš ï¸ High-Risk Packages ({len(high_risk_packages)}):\")\n",
    "        for pkg in high_risk_packages[:5]:  # Show top 5\n",
    "            print(f\"  â€¢ {pkg['name']} ({pkg['level']}) - {pkg['effort']} days\")\n",
    "            if pkg['reasons']:\n",
    "                print(f\"    Reasons: {', '.join(pkg['reasons'][:2])}\")\n",
    "        \n",
    "        if len(high_risk_packages) > 5:\n",
    "            print(f\"    ... and {len(high_risk_packages) - 5} more\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No complexity metrics available. Run full analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execution Sequence Planning\n",
    "\n",
    "Determine the optimal order for migrating packages based on dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cross-package dependencies for execution planning\n",
    "dependencies_query = \"\"\"\n",
    "MATCH (v:Node {id: 'view:cross_package_dependencies'})\n",
    "RETURN JSON_EXTRACT(v.properties, '$.data') as dependencies_data\n",
    "\"\"\"\n",
    "\n",
    "deps_result = execute_and_fetch(dependencies_query)\n",
    "\n",
    "if deps_result and deps_result[0][0]:\n",
    "    dependencies_data = json.loads(deps_result[0][0])\n",
    "    \n",
    "    print(\"ðŸ”— Migration Execution Sequence Planning\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Build dependency graph\n",
    "    package_deps = defaultdict(set)  # packages that this package depends on\n",
    "    package_dependents = defaultdict(set)  # packages that depend on this package\n",
    "    all_packages = set()\n",
    "    \n",
    "    for dep in dependencies_data:\n",
    "        source = dep.get('source_package', '')\n",
    "        target = dep.get('target_package', '')\n",
    "        \n",
    "        if source and target:\n",
    "            package_deps[source].add(target)  # source depends on target\n",
    "            package_dependents[target].add(source)  # target is depended on by source\n",
    "            all_packages.add(source)\n",
    "            all_packages.add(target)\n",
    "    \n",
    "    # Get all packages (including those without dependencies)\n",
    "    all_packages_query = \"\"\"\n",
    "    MATCH (p:Node {node_type: 'PIPELINE'})\n",
    "    RETURN p.name as package_name\n",
    "    \"\"\"\n",
    "    all_pkg_results = execute_and_fetch(all_packages_query)\n",
    "    for pkg_result in all_pkg_results:\n",
    "        all_packages.add(pkg_result[0])\n",
    "    \n",
    "    # Perform topological sort to determine execution order\n",
    "    def topological_sort(packages, dependencies):\n",
    "        \"\"\"Returns packages in dependency order (independent first)\"\"\"\n",
    "        result = []\n",
    "        remaining = set(packages)\n",
    "        \n",
    "        while remaining:\n",
    "            # Find packages with no unresolved dependencies\n",
    "            ready = [pkg for pkg in remaining \n",
    "                    if not (dependencies.get(pkg, set()) & remaining)]\n",
    "            \n",
    "            if not ready:\n",
    "                # Circular dependency - break it by taking package with fewest deps\n",
    "                ready = [min(remaining, key=lambda p: len(dependencies.get(p, set())))]\n",
    "                print(f\"  âš ï¸ Breaking circular dependency with {ready[0]}\")\n",
    "            \n",
    "            result.extend(ready)\n",
    "            remaining -= set(ready)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    execution_order = topological_sort(all_packages, package_deps)\n",
    "    \n",
    "    # Group into migration waves\n",
    "    wave_size = max(3, len(execution_order) // 5)  # 3-5 waves\n",
    "    waves = []\n",
    "    \n",
    "    for i in range(0, len(execution_order), wave_size):\n",
    "        wave = execution_order[i:i + wave_size]\n",
    "        waves.append(wave)\n",
    "    \n",
    "    print(f\"ðŸ“‹ Recommended Migration Sequence ({len(waves)} waves):\")\n",
    "    \n",
    "    total_estimated_time = 0\n",
    "    \n",
    "    for wave_num, wave_packages in enumerate(waves, 1):\n",
    "        print(f\"\\n  Wave {wave_num}: {len(wave_packages)} packages\")\n",
    "        \n",
    "        wave_complexity = {'LOW': 0, 'MEDIUM': 0, 'HIGH': 0, 'CRITICAL': 0}\n",
    "        wave_effort = 0\n",
    "        \n",
    "        for pkg in wave_packages:\n",
    "            # Find complexity info for this package\n",
    "            pkg_complexity = 'MEDIUM'  # Default\n",
    "            pkg_effort = 2  # Default days\n",
    "            \n",
    "            if complexity_result and complexity_result[0][0]:\n",
    "                for item in complexity_data:\n",
    "                    if item.get('package_name') == pkg:\n",
    "                        pkg_complexity = item.get('complexity_level', 'MEDIUM')\n",
    "                        pkg_effort = item.get('estimated_effort_days', 2)\n",
    "                        break\n",
    "            \n",
    "            wave_complexity[pkg_complexity] += 1\n",
    "            wave_effort += pkg_effort\n",
    "            \n",
    "            # Show dependencies\n",
    "            deps = package_deps.get(pkg, set())\n",
    "            deps_str = f\" (depends on: {', '.join(list(deps)[:2])})\" if deps else \"\"\n",
    "            \n",
    "            print(f\"    â€¢ {pkg} ({pkg_complexity}, {pkg_effort}d){deps_str}\")\n",
    "        \n",
    "        total_estimated_time += wave_effort\n",
    "        \n",
    "        print(f\"    Wave {wave_num} Total: {wave_effort} days\")\n",
    "        complexity_summary = ', '.join([f\"{k}: {v}\" for k, v in wave_complexity.items() if v > 0])\n",
    "        print(f\"    Complexity: {complexity_summary}\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸ Total Estimated Migration Time: {total_estimated_time} days ({total_estimated_time/5:.1f} weeks)\")\n",
    "    print(f\"ðŸ”„ Recommended Approach: Execute waves sequentially with 1-2 day buffer between waves\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No dependency data available. Run full analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Risk Analysis and Mitigation\n",
    "\n",
    "Identify potential risks and suggest mitigation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive risk analysis\n",
    "def analyze_migration_risks():\n",
    "    risks = []\n",
    "    \n",
    "    # 1. Shared Resource Contention Risk\n",
    "    shared_resources_query = \"\"\"\n",
    "    MATCH (v:Node {id: 'view:shared_resources_analysis'})\n",
    "    RETURN JSON_EXTRACT(v.properties, '$.data') as shared_resources\n",
    "    \"\"\"\n",
    "    \n",
    "    shared_result = execute_and_fetch(shared_resources_query)\n",
    "    if shared_result and shared_result[0][0]:\n",
    "        shared_data = json.loads(shared_result[0][0])\n",
    "        high_contention = [r for r in shared_data if r.get('contention_risk') == 'HIGH']\n",
    "        \n",
    "        if high_contention:\n",
    "            risks.append({\n",
    "                'type': 'Resource Contention',\n",
    "                'severity': 'HIGH',\n",
    "                'description': f'{len(high_contention)} shared resources with high contention risk',\n",
    "                'impact': 'Migration failures, data inconsistency, performance issues',\n",
    "                'mitigation': 'Migrate packages sharing resources in the same wave, implement resource pooling',\n",
    "                'details': [r['resource_name'] for r in high_contention[:3]]\n",
    "            })\n",
    "    \n",
    "    # 2. Complex SQL Operations Risk\n",
    "    sql_complexity_query = \"\"\"\n",
    "    MATCH (v:Node {id: 'view:sql_operations_catalog'})\n",
    "    RETURN JSON_EXTRACT(v.properties, '$.data') as sql_operations\n",
    "    \"\"\"\n",
    "    \n",
    "    sql_result = execute_and_fetch(sql_complexity_query)\n",
    "    if sql_result and sql_result[0][0]:\n",
    "        sql_data = json.loads(sql_result[0][0])\n",
    "        complex_sql = [op for op in sql_data \n",
    "                      if op.get('complexity_indicators', {}).get('has_subqueries') \n",
    "                      or (op.get('complexity_indicators', {}).get('table_count', 0) > 5)]\n",
    "        \n",
    "        if complex_sql:\n",
    "            risks.append({\n",
    "                'type': 'SQL Complexity',\n",
    "                'severity': 'MEDIUM',\n",
    "                'description': f'{len(complex_sql)} operations with complex SQL patterns',\n",
    "                'impact': 'Longer migration time, potential for logic errors in translation',\n",
    "                'mitigation': 'Manual review of complex SQL, automated testing, gradual rollout',\n",
    "                'details': [op['operation_name'] for op in complex_sql[:3]]\n",
    "            })\n",
    "    \n",
    "    # 3. Circular Dependencies Risk\n",
    "    if deps_result and deps_result[0][0]:\n",
    "        circular_deps = 0\n",
    "        for dep in dependencies_data:\n",
    "            source = dep.get('source_package')\n",
    "            target = dep.get('target_package')\n",
    "            # Check if there's a reverse dependency\n",
    "            reverse_exists = any(d.get('source_package') == target and d.get('target_package') == source \n",
    "                               for d in dependencies_data)\n",
    "            if reverse_exists:\n",
    "                circular_deps += 1\n",
    "        \n",
    "        if circular_deps > 0:\n",
    "            risks.append({\n",
    "                'type': 'Circular Dependencies',\n",
    "                'severity': 'HIGH',\n",
    "                'description': f'{circular_deps} potential circular dependencies detected',\n",
    "                'impact': 'Cannot determine safe migration order, rollback complications',\n",
    "                'mitigation': 'Break circular dependencies by refactoring, implement staged rollouts',\n",
    "                'details': ['Requires detailed dependency analysis']\n",
    "            })\n",
    "    \n",
    "    # 4. Data Volume Risk (estimated)\n",
    "    data_assets_query = \"\"\"\n",
    "    MATCH (da:Node {node_type: 'DATA_ASSET'})\n",
    "    WHERE da.asset_type = 'Table'\n",
    "    RETURN count(da) as table_count\n",
    "    \"\"\"\n",
    "    \n",
    "    data_result = execute_and_fetch(data_assets_query)\n",
    "    if data_result and data_result[0][0] > 50:\n",
    "        risks.append({\n",
    "            'type': 'Data Volume',\n",
    "            'severity': 'MEDIUM',\n",
    "            'description': f'{data_result[0][0]} tables involved in migration',\n",
    "            'impact': 'Extended migration time, storage requirements, data validation complexity',\n",
    "            'mitigation': 'Implement incremental data migration, validate data integrity at each step',\n",
    "            'details': ['Consider data archiving strategies']\n",
    "        })\n",
    "    \n",
    "    return risks\n",
    "\n",
    "risks = analyze_migration_risks()\n",
    "\n",
    "print(\"âš ï¸ Migration Risk Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not risks:\n",
    "    print(\"âœ… No significant risks identified. Migration should proceed smoothly.\")\n",
    "else:\n",
    "    print(f\"ðŸŽ¯ Identified {len(risks)} risk categories:\")\n",
    "    \n",
    "    # Sort by severity\n",
    "    severity_order = {'CRITICAL': 4, 'HIGH': 3, 'MEDIUM': 2, 'LOW': 1}\n",
    "    risks.sort(key=lambda r: severity_order.get(r['severity'], 0), reverse=True)\n",
    "    \n",
    "    for i, risk in enumerate(risks, 1):\n",
    "        severity_icon = {'CRITICAL': 'ðŸ”´', 'HIGH': 'ðŸŸ ', 'MEDIUM': 'ðŸŸ¡', 'LOW': 'ðŸŸ¢'}.get(risk['severity'], 'âšª')\n",
    "        \n",
    "        print(f\"\\n{i}. {risk['type']} {severity_icon} {risk['severity']}\")\n",
    "        print(f\"   Description: {risk['description']}\")\n",
    "        print(f\"   Impact: {risk['impact']}\")\n",
    "        print(f\"   Mitigation: {risk['mitigation']}\")\n",
    "        \n",
    "        if risk['details']:\n",
    "            print(f\"   Examples: {', '.join(risk['details'])}\")\n",
    "    \n",
    "    # Overall risk assessment\n",
    "    high_risks = sum(1 for r in risks if r['severity'] in ['CRITICAL', 'HIGH'])\n",
    "    if high_risks > 2:\n",
    "        overall_risk = \"ðŸ”´ HIGH RISK - Consider postponing until risks are mitigated\"\n",
    "    elif high_risks > 0:\n",
    "        overall_risk = \"ðŸŸ¡ MEDIUM RISK - Proceed with caution and additional planning\"\n",
    "    else:\n",
    "        overall_risk = \"ðŸŸ¢ LOW RISK - Safe to proceed with standard precautions\"\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Overall Migration Risk: {overall_risk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Platform Analysis\n",
    "\n",
    "Analyze compatibility with different target platforms (Azure Data Factory, AWS Glue, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SSIS features and their compatibility with target platforms\n",
    "def analyze_platform_compatibility():\n",
    "    # Get operation types from the graph\n",
    "    operations_query = \"\"\"\n",
    "    MATCH (op:Node {node_type: 'OPERATION'})\n",
    "    RETURN DISTINCT op.operation_type as operation_type, count(*) as count\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    operations = execute_and_fetch(operations_query)\n",
    "    \n",
    "    # Platform compatibility matrix\n",
    "    compatibility_matrix = {\n",
    "        'Azure Data Factory': {\n",
    "            'Data Flow Task': {'compatible': True, 'effort': 'LOW', 'notes': 'Native equivalent available'},\n",
    "            'Execute SQL Task': {'compatible': True, 'effort': 'LOW', 'notes': 'Direct mapping to SQL activities'},\n",
    "            'Script Task': {'compatible': False, 'effort': 'HIGH', 'notes': 'Requires custom Azure Functions'},\n",
    "            'OLEDB Source': {'compatible': True, 'effort': 'LOW', 'notes': 'Maps to dataset connectors'},\n",
    "            'OLEDB Destination': {'compatible': True, 'effort': 'LOW', 'notes': 'Maps to dataset connectors'},\n",
    "            'Lookup Transformation': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Requires data flow configuration'},\n",
    "            'Conditional Split': {'compatible': True, 'effort': 'LOW', 'notes': 'Native conditional split available'},\n",
    "            'File System Task': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Use Azure Storage activities'},\n",
    "        },\n",
    "        'AWS Glue': {\n",
    "            'Data Flow Task': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Convert to Glue ETL jobs'},\n",
    "            'Execute SQL Task': {'compatible': True, 'effort': 'LOW', 'notes': 'Use Glue connections to databases'},\n",
    "            'Script Task': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Convert to Python/Scala in Glue'},\n",
    "            'OLEDB Source': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Configure Glue connections'},\n",
    "            'OLEDB Destination': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Configure Glue connections'},\n",
    "            'Lookup Transformation': {'compatible': True, 'effort': 'HIGH', 'notes': 'Implement as joins in Spark'},\n",
    "            'Conditional Split': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Implement with Spark conditions'},\n",
    "            'File System Task': {'compatible': True, 'effort': 'LOW', 'notes': 'Native S3 operations'},\n",
    "        },\n",
    "        'Apache Airflow': {\n",
    "            'Data Flow Task': {'compatible': True, 'effort': 'HIGH', 'notes': 'Requires custom operators'},\n",
    "            'Execute SQL Task': {'compatible': True, 'effort': 'LOW', 'notes': 'Native SQL operators available'},\n",
    "            'Script Task': {'compatible': True, 'effort': 'LOW', 'notes': 'Python/Bash operators available'},\n",
    "            'OLEDB Source': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Use database hooks'},\n",
    "            'OLEDB Destination': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Use database hooks'},\n",
    "            'Lookup Transformation': {'compatible': True, 'effort': 'HIGH', 'notes': 'Custom implementation required'},\n",
    "            'Conditional Split': {'compatible': True, 'effort': 'MEDIUM', 'notes': 'Use branching operators'},\n",
    "            'File System Task': {'compatible': True, 'effort': 'LOW', 'notes': 'Native file system operators'},\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸŽ¯ Target Platform Compatibility Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"ðŸ“Š SSIS Operations Found in Northwind:\")\n",
    "    for op_type, count in operations:\n",
    "        print(f\"  â€¢ {op_type}: {count} instances\")\n",
    "    \n",
    "    # Analyze each platform\n",
    "    for platform_name, platform_compat in compatibility_matrix.items():\n",
    "        print(f\"\\nðŸ¢ {platform_name} Compatibility:\")\n",
    "        \n",
    "        total_operations = sum(count for _, count in operations)\n",
    "        compatible_operations = 0\n",
    "        total_effort_score = 0\n",
    "        effort_scores = {'LOW': 1, 'MEDIUM': 2, 'HIGH': 3}\n",
    "        \n",
    "        for op_type, count in operations:\n",
    "            if op_type in platform_compat:\n",
    "                compat_info = platform_compat[op_type]\n",
    "                compatible = compat_info['compatible']\n",
    "                effort = compat_info['effort']\n",
    "                notes = compat_info['notes']\n",
    "                \n",
    "                status_icon = \"âœ…\" if compatible else \"âŒ\"\n",
    "                effort_icon = {\"LOW\": \"ðŸŸ¢\", \"MEDIUM\": \"ðŸŸ¡\", \"HIGH\": \"ðŸ”´\"}.get(effort, \"âšª\")\n",
    "                \n",
    "                print(f\"  {status_icon} {op_type} ({count}x) - {effort_icon} {effort}\")\n",
    "                print(f\"     {notes}\")\n",
    "                \n",
    "                if compatible:\n",
    "                    compatible_operations += count\n",
    "                    total_effort_score += count * effort_scores.get(effort, 2)\n",
    "            else:\n",
    "                print(f\"  â“ {op_type} ({count}x) - âšª UNKNOWN\")\n",
    "                print(f\"     Requires investigation\")\n",
    "        \n",
    "        # Calculate platform score\n",
    "        compatibility_ratio = compatible_operations / total_operations if total_operations > 0 else 0\n",
    "        avg_effort = total_effort_score / compatible_operations if compatible_operations > 0 else 3\n",
    "        \n",
    "        # Overall platform rating\n",
    "        if compatibility_ratio > 0.9 and avg_effort < 1.5:\n",
    "            rating = \"ðŸŸ¢ EXCELLENT\"\n",
    "        elif compatibility_ratio > 0.8 and avg_effort < 2:\n",
    "            rating = \"ðŸŸ¡ GOOD\"\n",
    "        elif compatibility_ratio > 0.6:\n",
    "            rating = \"ðŸŸ  FAIR\"\n",
    "        else:\n",
    "            rating = \"ðŸ”´ CHALLENGING\"\n",
    "        \n",
    "        print(f\"  \\n  ðŸ“ˆ Platform Rating: {rating}\")\n",
    "        print(f\"     Compatibility: {compatibility_ratio*100:.1f}% of operations\")\n",
    "        print(f\"     Average Effort: {avg_effort:.1f}/3\")\n",
    "    \n",
    "    return operations, compatibility_matrix\n",
    "\n",
    "operations, compat_matrix = analyze_platform_compatibility()\n",
    "\n",
    "# Generate recommendation\n",
    "print(\"\\nðŸ’¡ Platform Recommendation:\")\n",
    "print(\"Based on the SSIS Northwind analysis:\")\n",
    "print(\"  1. Azure Data Factory: Best for organizations already on Azure\")\n",
    "print(\"  2. AWS Glue: Good for AWS-native organizations, handles most patterns\")\n",
    "print(\"  3. Apache Airflow: Most flexible but requires more custom development\")\n",
    "print(\"\\nConsider hybrid approaches for complex transformations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Lineage Impact Assessment\n",
    "\n",
    "Analyze how changes to one component affect the entire system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data lineage impact analysis for migration planning\n",
    "def analyze_lineage_impact(source_table=None):\n",
    "    \"\"\"Analyze the impact of migrating a specific data source\"\"\"\n",
    "    \n",
    "    if not source_table:\n",
    "        # Find the most connected table as an example\n",
    "        connectivity_query = \"\"\"\n",
    "        MATCH (da:Node {node_type: 'DATA_ASSET'})\n",
    "        WHERE da.asset_type = 'Table'\n",
    "        WITH da, \n",
    "             size([(da)<-[:READS_FROM]-() | 1]) as readers,\n",
    "             size([(da)<-[:WRITES_TO]-() | 1]) as writers\n",
    "        RETURN da.name as table_name, (readers + writers) as connections\n",
    "        ORDER BY connections DESC\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        result = execute_and_fetch(connectivity_query)\n",
    "        if result:\n",
    "            source_table = result[0][0]\n",
    "        else:\n",
    "            source_table = \"Orders\"  # Fallback\n",
    "    \n",
    "    print(f\"ðŸ” Lineage Impact Analysis: {source_table}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find all downstream impacts\n",
    "    downstream_query = \"\"\"\n",
    "    MATCH (source:Node {node_type: 'DATA_ASSET'})\n",
    "    WHERE source.name CONTAINS $table_name\n",
    "    MATCH path = (source)-[*1..4]->(downstream:Node)\n",
    "    WHERE downstream.node_type IN ['DATA_ASSET', 'OPERATION', 'PIPELINE']\n",
    "    RETURN DISTINCT\n",
    "        downstream.node_type as node_type,\n",
    "        downstream.name as name,\n",
    "        length(path) as distance,\n",
    "        [r in relationships(path) | type(r)] as relationship_path\n",
    "    ORDER BY distance, node_type, name\n",
    "    \"\"\"\n",
    "    \n",
    "    downstream_results = execute_and_fetch(downstream_query, {\"table_name\": source_table})\n",
    "    \n",
    "    if not downstream_results:\n",
    "        print(f\"âŒ No lineage found for table containing '{source_table}'\")\n",
    "        return\n",
    "    \n",
    "    # Group by distance (migration waves)\n",
    "    impact_by_distance = defaultdict(list)\n",
    "    for result in downstream_results:\n",
    "        node_type, name, distance, rel_path = result\n",
    "        impact_by_distance[distance].append({\n",
    "            'type': node_type,\n",
    "            'name': name,\n",
    "            'path': rel_path\n",
    "        })\n",
    "    \n",
    "    print(f\"ðŸ“Š Impact Analysis for '{source_table}':\")\n",
    "    print(f\"   Total Affected Components: {len(downstream_results)}\")\n",
    "    \n",
    "    total_operations = 0\n",
    "    total_assets = 0\n",
    "    total_packages = 0\n",
    "    \n",
    "    for distance in sorted(impact_by_distance.keys()):\n",
    "        components = impact_by_distance[distance]\n",
    "        \n",
    "        operations = [c for c in components if c['type'] == 'OPERATION']\n",
    "        assets = [c for c in components if c['type'] == 'DATA_ASSET']\n",
    "        packages = [c for c in components if c['type'] == 'PIPELINE']\n",
    "        \n",
    "        total_operations += len(operations)\n",
    "        total_assets += len(assets)\n",
    "        total_packages += len(packages)\n",
    "        \n",
    "        print(f\"\\n  ðŸ“ Distance {distance} ({len(components)} components):\")\n",
    "        \n",
    "        if packages:\n",
    "            print(f\"     ðŸ“¦ Packages ({len(packages)}): {', '.join([p['name'] for p in packages])}\")\n",
    "        \n",
    "        if operations:\n",
    "            print(f\"     âš™ï¸  Operations ({len(operations)}): {', '.join([o['name'][:30] for o in operations[:3]])}{'...' if len(operations) > 3 else ''}\")\n",
    "        \n",
    "        if assets:\n",
    "            print(f\"     ðŸ—ƒï¸  Data Assets ({len(assets)}): {', '.join([a['name'] for a in assets[:3]])}{'...' if len(assets) > 3 else ''}\")\n",
    "    \n",
    "    # Risk assessment\n",
    "    print(f\"\\nâš ï¸ Migration Impact Assessment:\")\n",
    "    \n",
    "    if total_packages > 5:\n",
    "        risk_level = \"ðŸ”´ HIGH\"\n",
    "        recommendation = \"Requires careful coordination across multiple packages\"\n",
    "    elif total_packages > 2:\n",
    "        risk_level = \"ðŸŸ¡ MEDIUM\"\n",
    "        recommendation = \"Plan migration with affected packages\"\n",
    "    else:\n",
    "        risk_level = \"ðŸŸ¢ LOW\"\n",
    "        recommendation = \"Can be migrated with minimal coordination\"\n",
    "    \n",
    "    print(f\"   Risk Level: {risk_level}\")\n",
    "    print(f\"   Affected Packages: {total_packages}\")\n",
    "    print(f\"   Affected Operations: {total_operations}\")\n",
    "    print(f\"   Affected Data Assets: {total_assets}\")\n",
    "    print(f\"   Recommendation: {recommendation}\")\n",
    "    \n",
    "    return {\n",
    "        'source_table': source_table,\n",
    "        'total_impact': len(downstream_results),\n",
    "        'packages': total_packages,\n",
    "        'operations': total_operations,\n",
    "        'assets': total_assets,\n",
    "        'risk_level': risk_level,\n",
    "        'max_distance': max(impact_by_distance.keys()) if impact_by_distance else 0\n",
    "    }\n",
    "\n",
    "# Analyze impact for a key table\n",
    "impact_analysis = analyze_lineage_impact()\n",
    "\n",
    "# Also analyze a few more tables to understand the overall system\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ System-Wide Impact Summary\")\n",
    "\n",
    "key_tables = ['Customer', 'Product', 'Order']\n",
    "impact_summary = []\n",
    "\n",
    "for table in key_tables:\n",
    "    try:\n",
    "        impact = analyze_lineage_impact(table)\n",
    "        if impact:\n",
    "            impact_summary.append(impact)\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if impact_summary:\n",
    "    print(\"\\nðŸŽ¯ Migration Planning Insights:\")\n",
    "    avg_impact = sum(i['total_impact'] for i in impact_summary) / len(impact_summary)\n",
    "    max_depth = max(i['max_distance'] for i in impact_summary)\n",
    "    \n",
    "    print(f\"   Average Impact per Table: {avg_impact:.1f} components\")\n",
    "    print(f\"   Maximum Propagation Depth: {max_depth} steps\")\n",
    "    print(f\"   High-Risk Tables: {sum(1 for i in impact_summary if 'HIGH' in i['risk_level'])}\")\n",
    "    \n",
    "    if avg_impact > 20:\n",
    "        print(\"   âš ï¸ System has high interconnectedness - plan carefully\")\n",
    "    elif avg_impact > 10:\n",
    "        print(\"   ðŸ’¡ Moderate interconnectedness - standard migration approach\")\n",
    "    else:\n",
    "        print(\"   âœ… Low interconnectedness - migration should be straightforward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Migration Report Generation\n",
    "\n",
    "Generate a comprehensive migration report combining all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive migration report\n",
    "def generate_migration_report():\n",
    "    report = {\n",
    "        'generated_at': datetime.now().isoformat(),\n",
    "        'system_name': 'SSIS Northwind',\n",
    "        'analysis_type': 'Migration Readiness Assessment'\n",
    "    }\n",
    "    \n",
    "    # Executive Summary\n",
    "    print(\"ðŸ“‹ SSIS NORTHWIND MIGRATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"System: SSIS Northwind\")\n",
    "    print(f\"Analysis Type: Migration Readiness Assessment\")\n",
    "    \n",
    "    # System Overview\n",
    "    overview_query = \"\"\"\n",
    "    MATCH (p:Node {node_type: 'PIPELINE'}) \n",
    "    WITH count(p) as packages\n",
    "    MATCH (op:Node {node_type: 'OPERATION'})\n",
    "    WITH packages, count(op) as operations\n",
    "    MATCH (da:Node {node_type: 'DATA_ASSET'})\n",
    "    WITH packages, operations, count(da) as data_assets\n",
    "    MATCH (conn:Node {node_type: 'CONNECTION'})\n",
    "    RETURN packages, operations, data_assets, count(conn) as connections\n",
    "    \"\"\"\n",
    "    \n",
    "    overview = execute_and_fetch(overview_query)\n",
    "    if overview:\n",
    "        packages, operations, data_assets, connections = overview[0]\n",
    "        \n",
    "        print(f\"\\nðŸ“Š SYSTEM OVERVIEW\")\n",
    "        print(f\"   SSIS Packages: {packages}\")\n",
    "        print(f\"   Operations: {operations}\")\n",
    "        print(f\"   Data Assets: {data_assets}\")\n",
    "        print(f\"   Connections: {connections}\")\n",
    "        \n",
    "        report['system_overview'] = {\n",
    "            'packages': packages,\n",
    "            'operations': operations,\n",
    "            'data_assets': data_assets,\n",
    "            'connections': connections\n",
    "        }\n",
    "    \n",
    "    # Complexity Assessment\n",
    "    if complexity_result and complexity_result[0][0]:\n",
    "        complexity_data = json.loads(complexity_result[0][0])\n",
    "        \n",
    "        complexity_distribution = {'LOW': 0, 'MEDIUM': 0, 'HIGH': 0, 'CRITICAL': 0}\n",
    "        total_effort = 0\n",
    "        \n",
    "        for item in complexity_data:\n",
    "            level = item.get('complexity_level', 'MEDIUM')\n",
    "            if level in complexity_distribution:\n",
    "                complexity_distribution[level] += 1\n",
    "            total_effort += item.get('estimated_effort_days', 2)\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ COMPLEXITY ASSESSMENT\")\n",
    "        for level, count in complexity_distribution.items():\n",
    "            percentage = (count / len(complexity_data) * 100) if complexity_data else 0\n",
    "            print(f\"   {level}: {count} packages ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"   Total Estimated Effort: {total_effort} days\")\n",
    "        print(f\"   Estimated Cost: ${total_effort * 800:,}\")\n",
    "        \n",
    "        report['complexity_assessment'] = {\n",
    "            'distribution': complexity_distribution,\n",
    "            'total_effort_days': total_effort,\n",
    "            'estimated_cost': total_effort * 800\n",
    "        }\n",
    "    \n",
    "    # Risk Summary\n",
    "    risks = analyze_migration_risks()\n",
    "    high_risks = [r for r in risks if r['severity'] in ['CRITICAL', 'HIGH']]\n",
    "    \n",
    "    print(f\"\\nâš ï¸ RISK ASSESSMENT\")\n",
    "    print(f\"   Total Risks Identified: {len(risks)}\")\n",
    "    print(f\"   High/Critical Risks: {len(high_risks)}\")\n",
    "    \n",
    "    if high_risks:\n",
    "        print(f\"   Key Risk Areas:\")\n",
    "        for risk in high_risks[:3]:\n",
    "            print(f\"     â€¢ {risk['type']} ({risk['severity']})\")\n",
    "    \n",
    "    report['risk_assessment'] = {\n",
    "        'total_risks': len(risks),\n",
    "        'high_risks': len(high_risks),\n",
    "        'risk_details': [{'type': r['type'], 'severity': r['severity'], 'description': r['description']} for r in risks]\n",
    "    }\n",
    "    \n",
    "    # Platform Recommendations\n",
    "    print(f\"\\nðŸŽ¯ PLATFORM RECOMMENDATIONS\")\n",
    "    print(f\"   Recommended Approach: Cloud-native ETL platform\")\n",
    "    print(f\"   Top Choices:\")\n",
    "    print(f\"     1. Azure Data Factory (if Azure-based)\")\n",
    "    print(f\"     2. AWS Glue (if AWS-based)\")\n",
    "    print(f\"     3. Apache Airflow (platform-agnostic)\")\n",
    "    \n",
    "    # Timeline Estimate\n",
    "    if 'complexity_assessment' in report:\n",
    "        effort_days = report['complexity_assessment']['total_effort_days']\n",
    "        weeks = effort_days / 5\n",
    "        \n",
    "        # Add buffer based on risk level\n",
    "        risk_buffer = 1.2 if len(high_risks) > 0 else 1.1\n",
    "        estimated_weeks = weeks * risk_buffer\n",
    "        \n",
    "        print(f\"\\nâ±ï¸ TIMELINE ESTIMATE\")\n",
    "        print(f\"   Base Effort: {effort_days} days ({weeks:.1f} weeks)\")\n",
    "        print(f\"   With Risk Buffer: {estimated_weeks:.1f} weeks\")\n",
    "        print(f\"   Recommended Timeline: {estimated_weeks + 2:.1f} weeks (including testing)\")\n",
    "        \n",
    "        report['timeline_estimate'] = {\n",
    "            'base_days': effort_days,\n",
    "            'base_weeks': weeks,\n",
    "            'with_buffer_weeks': estimated_weeks,\n",
    "            'recommended_weeks': estimated_weeks + 2\n",
    "        }\n",
    "    \n",
    "    # Success Criteria\n",
    "    print(f\"\\nâœ… SUCCESS CRITERIA\")\n",
    "    print(f\"   â–¡ All SSIS packages successfully migrated\")\n",
    "    print(f\"   â–¡ Data integrity validated across all tables\")\n",
    "    print(f\"   â–¡ Performance benchmarks met or exceeded\")\n",
    "    print(f\"   â–¡ Zero data loss during migration\")\n",
    "    print(f\"   â–¡ All business rules preserved\")\n",
    "    print(f\"   â–¡ User acceptance testing passed\")\n",
    "    print(f\"   â–¡ Rollback procedures tested and documented\")\n",
    "    \n",
    "    # Next Steps\n",
    "    print(f\"\\nðŸš€ RECOMMENDED NEXT STEPS\")\n",
    "    print(f\"   1. Stakeholder review and approval of migration plan\")\n",
    "    print(f\"   2. Detailed technical design for target platform\")\n",
    "    print(f\"   3. Setup development and testing environments\")\n",
    "    print(f\"   4. Begin with low-complexity packages for proof of concept\")\n",
    "    print(f\"   5. Develop automated testing and validation procedures\")\n",
    "    print(f\"   6. Create detailed rollback and recovery procedures\")\n",
    "    \n",
    "    # Overall Assessment\n",
    "    overall_score = 85  # Base score\n",
    "    if len(high_risks) > 2:\n",
    "        overall_score -= 20\n",
    "    elif len(high_risks) > 0:\n",
    "        overall_score -= 10\n",
    "    \n",
    "    if 'complexity_assessment' in report:\n",
    "        high_complexity_ratio = report['complexity_assessment']['distribution'].get('HIGH', 0) / report['system_overview']['packages']\n",
    "        if high_complexity_ratio > 0.3:\n",
    "            overall_score -= 10\n",
    "    \n",
    "    if overall_score >= 80:\n",
    "        readiness = \"ðŸŸ¢ READY FOR MIGRATION\"\n",
    "    elif overall_score >= 60:\n",
    "        readiness = \"ðŸŸ¡ MOSTLY READY - ADDRESS RISKS FIRST\"\n",
    "    else:\n",
    "        readiness = \"ðŸ”´ NOT READY - SIGNIFICANT WORK REQUIRED\"\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ OVERALL MIGRATION READINESS\")\n",
    "    print(f\"   Score: {overall_score}/100\")\n",
    "    print(f\"   Status: {readiness}\")\n",
    "    \n",
    "    report['overall_assessment'] = {\n",
    "        'score': overall_score,\n",
    "        'status': readiness\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸ“„ Report completed. System analysis suggests {readiness.split()[-1].lower()} for migration.\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate the comprehensive report\n",
    "migration_report = generate_migration_report()\n",
    "\n",
    "# Save report as JSON for further processing\n",
    "print(f\"\\nðŸ’¾ Report data structure created for programmatic access.\")\n",
    "print(f\"   Use 'migration_report' variable to access structured data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Migration Scenarios\n",
    "\n",
    "Test specific migration scenarios and their implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test migration scenarios\n",
    "def test_migration_scenario(scenario_name, package_subset=None):\n",
    "    \"\"\"Test a specific migration scenario\"\"\"\n",
    "    \n",
    "    print(f\"ðŸ§ª Testing Migration Scenario: {scenario_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if package_subset:\n",
    "        # Analyze specific packages\n",
    "        packages_filter = \"WHERE \" + \" OR \".join([f\"p.name CONTAINS '{pkg}'\" for pkg in package_subset])\n",
    "    else:\n",
    "        packages_filter = \"\"\n",
    "    \n",
    "    scenario_query = f\"\"\"\n",
    "    MATCH (p:Node {{node_type: 'PIPELINE'}})\n",
    "    {packages_filter}\n",
    "    OPTIONAL MATCH (p)-[:CONTAINS]->(op:Node {{node_type: 'OPERATION'}})\n",
    "    OPTIONAL MATCH (op)-[:READS_FROM|WRITES_TO]->(da:Node {{node_type: 'DATA_ASSET'}})\n",
    "    RETURN \n",
    "        p.name as package_name,\n",
    "        count(DISTINCT op) as operations,\n",
    "        count(DISTINCT da) as data_assets,\n",
    "        collect(DISTINCT op.operation_type) as operation_types\n",
    "    ORDER BY operations DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    results = execute_and_fetch(scenario_query)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âŒ No packages found for this scenario\")\n",
    "        return\n",
    "    \n",
    "    total_operations = sum(r[1] for r in results)\n",
    "    total_assets = sum(r[2] for r in results)\n",
    "    \n",
    "    print(f\"ðŸ“Š Scenario Analysis:\")\n",
    "    print(f\"   Packages in scope: {len(results)}\")\n",
    "    print(f\"   Total operations: {total_operations}\")\n",
    "    print(f\"   Total data assets: {total_assets}\")\n",
    "    \n",
    "    # Estimate effort for this scenario\n",
    "    base_effort_per_operation = 0.5  # days\n",
    "    estimated_effort = total_operations * base_effort_per_operation\n",
    "    \n",
    "    print(f\"   Estimated effort: {estimated_effort:.1f} days\")\n",
    "    \n",
    "    # Show package details\n",
    "    print(f\"\\nðŸ“¦ Packages in Scenario:\")\n",
    "    for pkg_name, ops, assets, op_types in results:\n",
    "        complexity = \"HIGH\" if ops > 10 else \"MEDIUM\" if ops > 5 else \"LOW\"\n",
    "        print(f\"   â€¢ {pkg_name}: {ops} ops, {assets} assets ({complexity} complexity)\")\n",
    "        if op_types:\n",
    "            unique_types = [t for t in op_types if t]  # Remove None values\n",
    "            print(f\"     Types: {', '.join(unique_types[:3])}{'...' if len(unique_types) > 3 else ''}\")\n",
    "    \n",
    "    # Check for dependencies outside the scenario\n",
    "    if deps_result and deps_result[0][0]:\n",
    "        dependencies_data = json.loads(deps_result[0][0])\n",
    "        scenario_packages = {r[0] for r in results}\n",
    "        \n",
    "        external_deps = []\n",
    "        for dep in dependencies_data:\n",
    "            source = dep.get('source_package', '')\n",
    "            target = dep.get('target_package', '')\n",
    "            \n",
    "            # Check if one package is in scenario but dependency is outside\n",
    "            if source in scenario_packages and target not in scenario_packages:\n",
    "                external_deps.append(f\"{source} depends on {target} (external)\")\n",
    "            elif target in scenario_packages and source not in scenario_packages:\n",
    "                external_deps.append(f\"{source} (external) affects {target}\")\n",
    "        \n",
    "        if external_deps:\n",
    "            print(f\"\\nâš ï¸ External Dependencies ({len(external_deps)}):\")\n",
    "            for dep in external_deps[:5]:\n",
    "                print(f\"   â€¢ {dep}\")\n",
    "            if len(external_deps) > 5:\n",
    "                print(f\"   ... and {len(external_deps) - 5} more\")\n",
    "        else:\n",
    "            print(f\"\\nâœ… No external dependencies - scenario is self-contained\")\n",
    "    \n",
    "    return {\n",
    "        'scenario': scenario_name,\n",
    "        'packages': len(results),\n",
    "        'operations': total_operations,\n",
    "        'assets': total_assets,\n",
    "        'estimated_effort': estimated_effort,\n",
    "        'external_dependencies': len(external_deps) if 'external_deps' in locals() else 0\n",
    "    }\n",
    "\n",
    "# Test different migration scenarios\n",
    "scenarios = [\n",
    "    (\"Customer Data Migration\", ['Customer', 'Order']),\n",
    "    (\"Product Catalog Migration\", ['Product', 'Category']),\n",
    "    (\"Reporting Systems Migration\", ['Report', 'Summary']),\n",
    "    (\"Full System Migration\", None)\n",
    "]\n",
    "\n",
    "scenario_results = []\n",
    "for scenario_name, package_filter in scenarios:\n",
    "    result = test_migration_scenario(scenario_name, package_filter)\n",
    "    if result:\n",
    "        scenario_results.append(result)\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "# Compare scenarios\n",
    "if scenario_results:\n",
    "    print(\"\\nðŸ“Š Scenario Comparison:\")\n",
    "    print(f\"{'Scenario':<30} {'Packages':<10} {'Operations':<12} {'Effort (days)':<15} {'Ext Deps':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for result in scenario_results:\n",
    "        print(f\"{result['scenario']:<30} {result['packages']:<10} {result['operations']:<12} {result['estimated_effort']:<15.1f} {result['external_dependencies']:<10}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Scenario Recommendations:\")\n",
    "    # Find the best starter scenario (low effort, few external deps)\n",
    "    starter_scenarios = [r for r in scenario_results if r['external_dependencies'] == 0 and r['estimated_effort'] < 10]\n",
    "    if starter_scenarios:\n",
    "        best_starter = min(starter_scenarios, key=lambda x: x['estimated_effort'])\n",
    "        print(f\"   ðŸš€ Best Starting Point: {best_starter['scenario']} ({best_starter['estimated_effort']:.1f} days)\")\n",
    "    \n",
    "    # Find the most complex scenario\n",
    "    most_complex = max(scenario_results, key=lambda x: x['estimated_effort'])\n",
    "    print(f\"   âš ï¸ Most Complex: {most_complex['scenario']} ({most_complex['estimated_effort']:.1f} days)\")\n",
    "    \n",
    "    print(f\"\\n   ðŸ’¡ Recommended approach: Start with self-contained scenarios and progress to more complex ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This comprehensive migration analysis notebook has covered:\n",
    "\n",
    "### ðŸŽ¯ Key Analysis Areas\n",
    "1. **Migration Complexity Assessment** - Overall effort estimation and cost analysis\n",
    "2. **Execution Sequence Planning** - Dependency-based migration waves\n",
    "3. **Risk Analysis and Mitigation** - Comprehensive risk identification\n",
    "4. **Target Platform Analysis** - Compatibility with Azure, AWS, and Airflow\n",
    "5. **Data Lineage Impact Assessment** - Understanding change propagation\n",
    "6. **Migration Report Generation** - Executive-level reporting\n",
    "7. **Custom Migration Scenarios** - Testing specific migration approaches\n",
    "\n",
    "### ðŸ“Š Key Insights for SSIS Northwind\n",
    "- System complexity and migration readiness\n",
    "- Optimal execution sequence based on dependencies\n",
    "- Risk factors that need attention\n",
    "- Platform compatibility analysis\n",
    "- Cost and timeline estimates\n",
    "\n",
    "### ðŸš€ How to Use This Analysis\n",
    "1. **For Migration Teams**: Use the execution sequence and risk analysis to plan your migration waves\n",
    "2. **For Architects**: Leverage platform compatibility analysis to choose the best target\n",
    "3. **For Project Managers**: Use timeline and cost estimates for project planning\n",
    "4. **For Stakeholders**: Review the executive summary and overall readiness assessment\n",
    "\n",
    "### ðŸ”„ Next Steps\n",
    "1. **Customize Analysis**: Modify queries and scenarios for your specific requirements\n",
    "2. **Integrate with Tools**: Use the structured report data in your migration tools\n",
    "3. **Monitor Progress**: Adapt the analysis as migration progresses\n",
    "4. **Scale Up**: Apply these techniques to larger SSIS environments\n",
    "\n",
    "### ðŸ’¡ Advanced Applications\n",
    "- **Automated Migration Planning**: Use the analysis to drive migration tool decisions\n",
    "- **Change Impact Analysis**: Assess impacts of modifications during migration\n",
    "- **Performance Optimization**: Identify bottlenecks and optimization opportunities\n",
    "- **Compliance Reporting**: Generate reports for audit and governance requirements\n",
    "\n",
    "The combination of graph analysis, materialized views, and migration-specific queries provides a powerful foundation for data-driven migration planning and execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}