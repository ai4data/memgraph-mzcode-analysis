{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-overview",
   "metadata": {},
   "source": [
    "# 02 - Exploring SSIS Package Structure\n",
    "\n",
    "This notebook demonstrates how to explore the structure of SSIS packages stored in Memgraph,\n",
    "with focus on the enhanced SQL semantics metadata for migration analysis.\n",
    "\n",
    "## Key Features Covered:\n",
    "- Package hierarchy and relationships\n",
    "- Operation types and their properties\n",
    "- Data flow analysis with SQL semantics\n",
    "- Connection and parameter mapping\n",
    "- Migration readiness assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import pymgclient\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Connection configuration\n",
    "HOST = \"localhost\"\n",
    "PORT = 7687\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"Create Memgraph connection.\"\"\"\n",
    "    return pymgclient.connect(host=HOST, port=PORT)\n",
    "\n",
    "def execute_query(query: str, params: Dict = None) -> pd.DataFrame:\n",
    "    \"\"\"Execute query and return results as DataFrame.\"\"\"\n",
    "    with get_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, params or {})\n",
    "        \n",
    "        columns = [desc[0] for desc in cursor.description] if cursor.description else []\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "package-overview",
   "metadata": {},
   "source": [
    "## 1. Package Structure Overview\n",
    "\n",
    "Let's start by examining the overall structure of SSIS packages in our graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "package-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get package overview\n",
    "package_overview = execute_query(\"\"\"\n",
    "    MATCH (p:Node)\n",
    "    WHERE p.node_type = 'pipeline'\n",
    "    RETURN \n",
    "        p.name as package_name,\n",
    "        p.properties.file_path as file_path,\n",
    "        p.properties.package_type as package_type\n",
    "    ORDER BY p.name\n",
    "\"\"\")\n",
    "\n",
    "print(\"üì¶ SSIS PACKAGES IN GRAPH:\")\n",
    "print(\"=\" * 60)\n",
    "display(package_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "package-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed package metrics\n",
    "package_metrics = execute_query(\"\"\"\n",
    "    MATCH (pkg:Node)-[:CONTAINS]->(op:Node)\n",
    "    WHERE pkg.node_type = 'pipeline' AND op.node_type = 'operation'\n",
    "    WITH pkg, count(op) as operation_count\n",
    "    OPTIONAL MATCH (pkg)-[:CONTAINS*]->(asset:Node)\n",
    "    WHERE asset.node_type = 'data_asset'\n",
    "    WITH pkg, operation_count, count(DISTINCT asset) as data_asset_count\n",
    "    OPTIONAL MATCH (pkg)-[:CONTAINS*]->(conn:Node)\n",
    "    WHERE conn.node_type = 'connection'\n",
    "    RETURN \n",
    "        pkg.name as package_name,\n",
    "        operation_count,\n",
    "        data_asset_count,\n",
    "        count(DISTINCT conn) as connection_count\n",
    "    ORDER BY operation_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä PACKAGE COMPLEXITY METRICS:\")\n",
    "print(\"=\" * 60)\n",
    "display(package_metrics)\n",
    "\n",
    "# Visualize package complexity\n",
    "if not package_metrics.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Operations per package\n",
    "    ax1.bar(package_metrics['package_name'], package_metrics['operation_count'])\n",
    "    ax1.set_title('Operations per Package')\n",
    "    ax1.set_xlabel('Package')\n",
    "    ax1.set_ylabel('Number of Operations')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Data assets per package\n",
    "    ax2.bar(package_metrics['package_name'], package_metrics['data_asset_count'])\n",
    "    ax2.set_title('Data Assets per Package')\n",
    "    ax2.set_xlabel('Package')\n",
    "    ax2.set_ylabel('Number of Data Assets')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operation-types",
   "metadata": {},
   "source": [
    "## 2. Operation Types and SQL Semantics Analysis\n",
    "\n",
    "Now let's examine the different types of operations and their enhanced SQL semantics metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operation-types-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze operation types\n",
    "operation_types = execute_query(\"\"\"\n",
    "    MATCH (op:Node)\n",
    "    WHERE op.node_type = 'operation'\n",
    "    WITH op.properties.operation_type as op_type, count(*) as count,\n",
    "         sum(CASE WHEN op.properties CONTAINS 'sql_semantics' THEN 1 ELSE 0 END) as with_sql_semantics\n",
    "    RETURN \n",
    "        op_type,\n",
    "        count as total_operations,\n",
    "        with_sql_semantics,\n",
    "        round(100.0 * with_sql_semantics / count, 1) as sql_semantics_percentage\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üîß OPERATION TYPES WITH SQL SEMANTICS:\")\n",
    "print(\"=\" * 70)\n",
    "display(operation_types)\n",
    "\n",
    "# Visualize operation types\n",
    "if not operation_types.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Operation type distribution\n",
    "    ax1.pie(operation_types['total_operations'], \n",
    "            labels=operation_types['op_type'],\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90)\n",
    "    ax1.set_title('Distribution of Operation Types')\n",
    "    \n",
    "    # SQL semantics coverage\n",
    "    x_pos = range(len(operation_types))\n",
    "    ax2.bar(x_pos, operation_types['total_operations'], label='Total Operations', alpha=0.7)\n",
    "    ax2.bar(x_pos, operation_types['with_sql_semantics'], label='With SQL Semantics', alpha=0.9)\n",
    "    ax2.set_title('SQL Semantics Coverage by Operation Type')\n",
    "    ax2.set_xlabel('Operation Type')\n",
    "    ax2.set_ylabel('Number of Operations')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(operation_types['op_type'], rotation=45)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sql-semantics-deep-dive",
   "metadata": {},
   "source": [
    "## 3. SQL Semantics Deep Dive\n",
    "\n",
    "Let's examine the enhanced SQL semantics metadata that enables accurate migration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sql-semantics-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get operations with rich SQL semantics\n",
    "sql_operations = execute_query(\"\"\"\n",
    "    MATCH (op:Node)\n",
    "    WHERE op.node_type = 'operation' AND op.properties CONTAINS 'sql_semantics'\n",
    "    RETURN \n",
    "        op.name as operation_name,\n",
    "        op.properties.operation_type as operation_type,\n",
    "        op.properties.sql_semantics as sql_semantics_raw\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"üîç OPERATIONS WITH SQL SEMANTICS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, row in sql_operations.iterrows():\n",
    "    print(f\"\\nüìã Operation: {row['operation_name']}\")\n",
    "    print(f\"   Type: {row['operation_type']}\")\n",
    "    \n",
    "    # Parse SQL semantics JSON\n",
    "    try:\n",
    "        sql_semantics = json.loads(row['sql_semantics_raw']) if isinstance(row['sql_semantics_raw'], str) else row['sql_semantics_raw']\n",
    "        \n",
    "        print(f\"   üìä Metadata:\")\n",
    "        migration_meta = sql_semantics.get('migration_metadata', {})\n",
    "        print(f\"      ‚Ä¢ Tables: {migration_meta.get('table_count', 0)}\")\n",
    "        print(f\"      ‚Ä¢ Joins: {migration_meta.get('join_count', 0)}\")\n",
    "        print(f\"      ‚Ä¢ Columns: {migration_meta.get('column_count', 0)}\")\n",
    "        print(f\"      ‚Ä¢ Has Aliases: {migration_meta.get('has_aliases', False)}\")\n",
    "        print(f\"      ‚Ä¢ Join Types: {migration_meta.get('join_types', [])}\")\n",
    "        \n",
    "        # Show original query snippet\n",
    "        if 'original_query' in sql_semantics:\n",
    "            query_snippet = sql_semantics['original_query'][:100]\n",
    "            print(f\"   üî§ Query Preview: {query_snippet}...\")\n",
    "        \n",
    "        # Show table references\n",
    "        if sql_semantics.get('tables'):\n",
    "            print(f\"   üìã Tables Referenced:\")\n",
    "            for table in sql_semantics['tables'][:3]:  # Show first 3\n",
    "                alias_info = f\" (as {table['alias']})\" if table.get('alias') else \"\"\n",
    "                schema_info = f\"{table['schema']}.\" if table.get('schema') else \"\"\n",
    "                print(f\"      ‚Ä¢ {schema_info}{table['name']}{alias_info}\")\n",
    "        \n",
    "        # Show JOIN relationships\n",
    "        if sql_semantics.get('joins'):\n",
    "            print(f\"   üîó JOIN Relationships:\")\n",
    "            for join in sql_semantics['joins'][:2]:  # Show first 2\n",
    "                left_table = join['left_table']['name']\n",
    "                right_table = join['right_table']['name']\n",
    "                join_type = join['join_type']\n",
    "                condition = join['condition'][:50] + \"...\" if len(join['condition']) > 50 else join['condition']\n",
    "                print(f\"      ‚Ä¢ {left_table} {join_type} {right_table} ON {condition}\")\n",
    "        \n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        print(f\"   ‚ùå Error parsing SQL semantics: {e}\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-flow-analysis",
   "metadata": {},
   "source": [
    "## 4. Data Flow Analysis\n",
    "\n",
    "Analyze data flow patterns and table relationships across packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-flow-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data flow patterns\n",
    "data_flow = execute_query(\"\"\"\n",
    "    MATCH (src:Node)-[r:READS_FROM|WRITES_TO]->(target:Node)\n",
    "    WHERE src.node_type = 'operation' AND target.node_type = 'data_asset'\n",
    "    RETURN \n",
    "        type(r) as relationship_type,\n",
    "        src.name as operation_name,\n",
    "        target.name as asset_name,\n",
    "        src.properties.operation_type as operation_type\n",
    "    ORDER BY relationship_type, asset_name\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä DATA FLOW PATTERNS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not data_flow.empty:\n",
    "    # Group by relationship type\n",
    "    flow_summary = data_flow.groupby(['relationship_type', 'asset_name']).size().reset_index(name='count')\n",
    "    flow_summary = flow_summary.sort_values(['relationship_type', 'count'], ascending=[True, False])\n",
    "    \n",
    "    display(flow_summary.head(15))\n",
    "    \n",
    "    # Show most accessed data assets\n",
    "    asset_access = data_flow.groupby('asset_name').agg({\n",
    "        'relationship_type': 'count',\n",
    "        'operation_name': 'nunique'\n",
    "    }).reset_index()\n",
    "    asset_access.columns = ['asset_name', 'total_accesses', 'unique_operations']\n",
    "    asset_access = asset_access.sort_values('total_accesses', ascending=False)\n",
    "    \n",
    "    print(\"\\nüéØ MOST ACCESSED DATA ASSETS:\")\n",
    "    display(asset_access.head(10))\n",
    "else:\n",
    "    print(\"No data flow relationships found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "table-references",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced table reference analysis using SQL semantics\n",
    "table_references = execute_query(\"\"\"\n",
    "    MATCH (op:Node)\n",
    "    WHERE op.node_type = 'operation' AND op.properties CONTAINS 'sql_semantics'\n",
    "    RETURN \n",
    "        op.name as operation_name,\n",
    "        op.properties.sql_semantics as sql_semantics_raw\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìã TABLE REFERENCES FROM SQL SEMANTICS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "table_usage = {}\n",
    "join_patterns = {}\n",
    "\n",
    "for idx, row in table_references.iterrows():\n",
    "    try:\n",
    "        sql_semantics = json.loads(row['sql_semantics_raw']) if isinstance(row['sql_semantics_raw'], str) else row['sql_semantics_raw']\n",
    "        \n",
    "        # Track table usage\n",
    "        for table in sql_semantics.get('tables', []):\n",
    "            table_name = table['name']\n",
    "            if table_name not in table_usage:\n",
    "                table_usage[table_name] = {\n",
    "                    'operations': set(),\n",
    "                    'with_alias': 0,\n",
    "                    'schemas': set()\n",
    "                }\n",
    "            \n",
    "            table_usage[table_name]['operations'].add(row['operation_name'])\n",
    "            if table.get('alias'):\n",
    "                table_usage[table_name]['with_alias'] += 1\n",
    "            if table.get('schema'):\n",
    "                table_usage[table_name]['schemas'].add(table['schema'])\n",
    "        \n",
    "        # Track JOIN patterns\n",
    "        for join in sql_semantics.get('joins', []):\n",
    "            join_type = join['join_type']\n",
    "            if join_type not in join_patterns:\n",
    "                join_patterns[join_type] = 0\n",
    "            join_patterns[join_type] += 1\n",
    "            \n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        continue\n",
    "\n",
    "# Display table usage summary\n",
    "if table_usage:\n",
    "    table_df = pd.DataFrame([\n",
    "        {\n",
    "            'table_name': name,\n",
    "            'operation_count': len(info['operations']),\n",
    "            'operations': ', '.join(list(info['operations'])[:3]) + ('...' if len(info['operations']) > 3 else ''),\n",
    "            'alias_usage': info['with_alias'],\n",
    "            'schemas': ', '.join(info['schemas']) if info['schemas'] else 'None'\n",
    "        }\n",
    "        for name, info in table_usage.items()\n",
    "    ]).sort_values('operation_count', ascending=False)\n",
    "    \n",
    "    display(table_df)\n",
    "    \n",
    "    print(f\"\\nüîó JOIN PATTERN DISTRIBUTION:\")\n",
    "    for join_type, count in sorted(join_patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   ‚Ä¢ {join_type}: {count} occurrences\")\nelse:\n",
    "    print(\"No table references found in SQL semantics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection-analysis",
   "metadata": {},
   "source": [
    "## 5. Connection and Parameter Analysis\n",
    "\n",
    "Examine connection patterns and parameter usage for migration planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connection-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze connections\n",
    "connections = execute_query(\"\"\"\n",
    "    MATCH (conn:Node)\n",
    "    WHERE conn.node_type = 'connection'\n",
    "    OPTIONAL MATCH (op:Node)-[:USES_CONNECTION]->(conn)\n",
    "    WHERE op.node_type = 'operation'\n",
    "    RETURN \n",
    "        conn.name as connection_name,\n",
    "        conn.properties.connection_type as connection_type,\n",
    "        conn.properties.server_name as server_name,\n",
    "        conn.properties.database_name as database_name,\n",
    "        count(op) as used_by_operations\n",
    "    ORDER BY used_by_operations DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üîå CONNECTION ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "display(connections)\n",
    "\n",
    "# Analyze parameters\n",
    "parameters = execute_query(\"\"\"\n",
    "    MATCH (param:Node)\n",
    "    WHERE param.node_type = 'parameter'\n",
    "    OPTIONAL MATCH (op:Node)-[:USES_PARAMETER]->(param)\n",
    "    WHERE op.node_type = 'operation'\n",
    "    RETURN \n",
    "        param.name as parameter_name,\n",
    "        param.properties.data_type as data_type,\n",
    "        param.properties.default_value as default_value,\n",
    "        count(op) as used_by_operations\n",
    "    ORDER BY used_by_operations DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìù PARAMETER ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "display(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration-readiness",
   "metadata": {},
   "source": [
    "## 6. Migration Readiness Assessment\n",
    "\n",
    "Assess how ready each package is for automated migration based on available metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "migration-readiness-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migration readiness analysis\n",
    "migration_readiness = execute_query(\"\"\"\n",
    "    MATCH (pkg:Node)\n",
    "    WHERE pkg.node_type = 'pipeline'\n",
    "    OPTIONAL MATCH (pkg)-[:CONTAINS]->(op:Node)\n",
    "    WHERE op.node_type = 'operation'\n",
    "    WITH pkg, \n",
    "         count(op) as total_operations,\n",
    "         sum(CASE WHEN op.properties CONTAINS 'sql_semantics' THEN 1 ELSE 0 END) as operations_with_sql_semantics\n",
    "    OPTIONAL MATCH (pkg)-[:CONTAINS*]->(asset:Node)\n",
    "    WHERE asset.node_type = 'data_asset'\n",
    "    WITH pkg, total_operations, operations_with_sql_semantics, count(DISTINCT asset) as data_assets\n",
    "    OPTIONAL MATCH (pkg)-[:CONTAINS*]->(conn:Node)\n",
    "    WHERE conn.node_type = 'connection'\n",
    "    RETURN \n",
    "        pkg.name as package_name,\n",
    "        total_operations,\n",
    "        operations_with_sql_semantics,\n",
    "        round(100.0 * operations_with_sql_semantics / CASE WHEN total_operations = 0 THEN 1 ELSE total_operations END, 1) as sql_semantics_coverage,\n",
    "        data_assets,\n",
    "        count(DISTINCT conn) as connections\n",
    "    ORDER BY sql_semantics_coverage DESC, total_operations DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ MIGRATION READINESS ASSESSMENT:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Add readiness score calculation\n",
    "def calculate_readiness_score(row):\n",
    "    \"\"\"Calculate migration readiness score (0-100).\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # SQL semantics coverage (40% weight)\n",
    "    score += (row['sql_semantics_coverage'] / 100) * 40\n",
    "    \n",
    "    # Package complexity bonus (30% weight - inverse relationship)\n",
    "    complexity_score = min(30, max(0, 30 - (row['total_operations'] - 5) * 2))\n",
    "    score += complexity_score\n",
    "    \n",
    "    # Data asset mapping (20% weight)\n",
    "    asset_score = min(20, row['data_assets'] * 4)  # Up to 20 points\n",
    "    score += asset_score\n",
    "    \n",
    "    # Connection clarity (10% weight)\n",
    "    conn_score = min(10, row['connections'] * 5)  # Up to 10 points\n",
    "    score += conn_score\n",
    "    \n",
    "    return round(score, 1)\n",
    "\n",
    "if not migration_readiness.empty:\n",
    "    migration_readiness['readiness_score'] = migration_readiness.apply(calculate_readiness_score, axis=1)\n",
    "    \n",
    "    # Add readiness category\n",
    "    def get_readiness_category(score):\n",
    "        if score >= 80: return \"üü¢ High\"\n",
    "        elif score >= 60: return \"üü° Medium\"\n",
    "        elif score >= 40: return \"üü† Low\"\n",
    "        else: return \"üî¥ Manual\"\n",
    "    \n",
    "    migration_readiness['readiness_category'] = migration_readiness['readiness_score'].apply(get_readiness_category)\n",
    "    \n",
    "    # Sort by readiness score\n",
    "    migration_readiness = migration_readiness.sort_values('readiness_score', ascending=False)\n",
    "    \n",
    "    display(migration_readiness)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Readiness score distribution\n",
    "    ax1.bar(migration_readiness['package_name'], migration_readiness['readiness_score'])\n",
    "    ax1.set_title('Migration Readiness Scores')\n",
    "    ax1.set_xlabel('Package')\n",
    "    ax1.set_ylabel('Readiness Score (0-100)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.axhline(y=80, color='green', linestyle='--', alpha=0.7, label='High Readiness')\n",
    "    ax1.axhline(y=60, color='orange', linestyle='--', alpha=0.7, label='Medium Readiness')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # SQL semantics coverage vs operations\n",
    "    ax2.scatter(migration_readiness['total_operations'], migration_readiness['sql_semantics_coverage'], \n",
    "               s=migration_readiness['readiness_score']*2, alpha=0.6)\n",
    "    ax2.set_title('SQL Semantics Coverage vs Package Complexity')\n",
    "    ax2.set_xlabel('Total Operations')\n",
    "    ax2.set_ylabel('SQL Semantics Coverage (%)')\n",
    "    \n",
    "    # Add package labels\n",
    "    for idx, row in migration_readiness.iterrows():\n",
    "        ax2.annotate(row['package_name'], \n",
    "                    (row['total_operations'], row['sql_semantics_coverage']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä READINESS SUMMARY:\")\n",
    "    readiness_summary = migration_readiness['readiness_category'].value_counts()\n",
    "    for category, count in readiness_summary.items():\n",
    "        print(f\"   {category}: {count} packages\")\n",
    "    \n",
    "    avg_score = migration_readiness['readiness_score'].mean()\n",
    "    print(f\"\\n   üìà Average Readiness Score: {avg_score:.1f}/100\")\n",
    "    \n",
    "    high_ready = len(migration_readiness[migration_readiness['readiness_score'] >= 80])\n",
    "    total_packages = len(migration_readiness)\n",
    "    print(f\"   üéØ High-Readiness Packages: {high_ready}/{total_packages} ({100*high_ready/total_packages:.1f}%)\")\nelse:\n",
    "    print(\"No packages found for migration readiness assessment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration-recommendations",
   "metadata": {},
   "source": [
    "## 7. Migration Recommendations\n",
    "\n",
    "Based on the analysis above, here are specific recommendations for each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "migration-recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° MIGRATION RECOMMENDATIONS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not migration_readiness.empty:\n",
    "    for idx, row in migration_readiness.iterrows():\n",
    "        package_name = row['package_name']\n",
    "        score = row['readiness_score']\n",
    "        coverage = row['sql_semantics_coverage']\n",
    "        \n",
    "        print(f\"\\nüì¶ {package_name}\")\n",
    "        print(f\"   Readiness: {row['readiness_category']} ({score}/100)\")\n",
    "        \n",
    "        # Specific recommendations\n",
    "        recommendations = []\n",
    "        \n",
    "        if score >= 80:\n",
    "            recommendations.append(\"‚úÖ Ready for automated migration\")\n",
    "            if coverage == 100:\n",
    "                recommendations.append(\"üöÄ Perfect SQL semantics coverage - use code generators\")\n",
    "            else:\n",
    "                recommendations.append(f\"üìä {coverage}% SQL coverage - minor manual validation needed\")\n",
    "        elif score >= 60:\n",
    "            recommendations.append(\"‚ö†Ô∏è  Semi-automated migration possible\")\n",
    "            if coverage < 50:\n",
    "                recommendations.append(\"üîç Improve SQL semantics extraction first\")\n",
    "            recommendations.append(\"üë• Require migration expert review\")\n",
    "        else:\n",
    "            recommendations.append(\"üî¥ Manual migration required\")\n",
    "            if coverage == 0:\n",
    "                recommendations.append(\"üìù No SQL semantics captured - start with parser enhancement\")\n",
    "            recommendations.append(\"üõ†Ô∏è  Consider package refactoring before migration\")\n",
    "        \n",
    "        # Effort estimation\n",
    "        if score >= 80:\n",
    "            effort = \"2-4 hours (mostly automated)\"\n",
    "        elif score >= 60:\n",
    "            effort = \"1-2 days (semi-automated + review)\"\n",
    "        elif score >= 40:\n",
    "            effort = \"3-5 days (significant manual work)\"\n",
    "        else:\n",
    "            effort = \"1-2 weeks (full manual migration)\"\n",
    "        \n",
    "        recommendations.append(f\"‚è±Ô∏è  Estimated effort: {effort}\")\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            print(f\"   {rec}\")\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # Overall migration strategy\n",
    "    total_estimated_hours = 0\n",
    "    for idx, row in migration_readiness.iterrows():\n",
    "        score = row['readiness_score']\n",
    "        if score >= 80: total_estimated_hours += 3\n",
    "        elif score >= 60: total_estimated_hours += 24\n",
    "        elif score >= 40: total_estimated_hours += 64\n",
    "        else: total_estimated_hours += 120\n",
    "    \n",
    "    print(f\"\\nüéØ OVERALL MIGRATION STRATEGY:\")\n",
    "    print(f\"   üìä Total estimated effort: {total_estimated_hours} hours ({total_estimated_hours/8:.1f} person-days)\")\n",
    "    print(f\"   üöÄ High-priority packages: {len(migration_readiness[migration_readiness['readiness_score'] >= 80])}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Medium-priority packages: {len(migration_readiness[(migration_readiness['readiness_score'] >= 60) & (migration_readiness['readiness_score'] < 80)])}\")\n",
    "    print(f\"   üî¥ Manual migration packages: {len(migration_readiness[migration_readiness['readiness_score'] < 60])}\")\n",
    "    \n",
    "    print(f\"\\n   üí° Recommended approach:\")\n",
    "    print(f\"   1. Start with high-readiness packages for quick wins\")\n",
    "    print(f\"   2. Use automated code generators where SQL semantics coverage > 80%\")\n",
    "    print(f\"   3. Enhance parser for medium-readiness packages\")\n",
    "    print(f\"   4. Plan manual migration workshops for complex packages\")\nelse:\n",
    "    print(\"No migration readiness data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive SSIS package structure analysis with enhanced SQL semantics metadata. Key insights:\n",
    "\n",
    "### Key Capabilities Demonstrated:\n",
    "1. **Package Structure Analysis** - Comprehensive overview of SSIS packages and their complexity\n",
    "2. **SQL Semantics Integration** - Enhanced metadata for accurate migration planning\n",
    "3. **Data Flow Mapping** - Complete understanding of data movements and transformations\n",
    "4. **Migration Readiness Assessment** - Automated scoring and categorization\n",
    "5. **Strategic Recommendations** - Data-driven migration planning\n",
    "\n",
    "### Enhanced Features:\n",
    "- **Direct Property Queries** - Bypassing outdated materialized views for real-time analysis\n",
    "- **SQL Semantics Metadata** - JOIN relationships, table references, and column transformations\n",
    "- **Migration Scoring** - Quantitative readiness assessment for prioritization\n",
    "- **Code Generation Ready** - Metadata structured for automated migration code generation\n",
    "\n",
    "### Next Steps:\n",
    "- Use this analysis to prioritize migration efforts\n",
    "- Leverage high-readiness packages for automated code generation\n",
    "- Enhance SQL semantics coverage for medium-readiness packages\n",
    "- Plan detailed migration workshops for complex packages requiring manual intervention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}