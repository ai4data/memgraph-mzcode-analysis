{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-overview",
   "metadata": {},
   "source": [
    "# 04 - Advanced Graph Queries for Migration Planning\n",
    "\n",
    "This notebook demonstrates advanced graph query patterns using the enhanced SQL semantics metadata,\n",
    "focusing on complex analysis patterns that support sophisticated migration planning strategies.\n",
    "\n",
    "## Key Features Covered:\n",
    "- Multi-hop relationship traversals\n",
    "- Pattern matching with SQL semantics filters\n",
    "- Graph algorithms for migration optimization\n",
    "- Performance analysis and query optimization\n",
    "- Real-world migration scenarios and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import pymgclient\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Any, Tuple, Set\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Connection configuration\n",
    "HOST = \"localhost\"\n",
    "PORT = 7687\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"Create Memgraph connection.\"\"\"\n",
    "    return pymgclient.connect(host=HOST, port=PORT)\n",
    "\n",
    "def execute_query(query: str, params: Dict = None, show_timing: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Execute query and return results as DataFrame with optional timing.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with get_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, params or {})\n",
    "        \n",
    "        columns = [desc[0] for desc in cursor.description] if cursor.description else []\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        result = pd.DataFrame(rows, columns=columns)\n",
    "    \n",
    "    if show_timing:\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è  Query executed in {execution_time:.3f} seconds\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def display_query_stats(result_df: pd.DataFrame, query_name: str):\n",
    "    \"\"\"Display statistics about query results.\"\"\"\n",
    "    print(f\"üìä {query_name} Results: {len(result_df)} rows, {len(result_df.columns)} columns\")\n",
    "    if len(result_df) > 0:\n",
    "        print(f\"   Memory usage: ~{result_df.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-hop-traversals",
   "metadata": {},
   "source": [
    "## 1. Multi-Hop Relationship Traversals\n",
    "\n",
    "Explore complex relationships across multiple levels of the SSIS package hierarchy using advanced graph traversal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "end-to-end-lineage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced data lineage analysis with multi-hop traversals\n",
    "print(\"üîç ADVANCED DATA LINEAGE ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Query 1: End-to-end data lineage with SQL semantics\n",
    "lineage_query = \"\"\"\n",
    "    MATCH path = (source:Node)-[:READS_FROM|WRITES_TO|CONTAINS|REFERENCES*1..6]-(target:Node)\n",
    "    WHERE source.node_type = 'data_asset' AND target.node_type = 'data_asset'\n",
    "          AND source.name <> target.name\n",
    "    WITH path, source, target, \n",
    "         [node IN nodes(path) WHERE node.node_type = 'operation' AND node.properties CONTAINS 'sql_semantics'] as sql_ops\n",
    "    WHERE size(sql_ops) > 0\n",
    "    RETURN \n",
    "        source.name as source_table,\n",
    "        target.name as target_table,\n",
    "        length(path) as path_length,\n",
    "        size(sql_ops) as sql_operations_count,\n",
    "        [op IN sql_ops | op.name][0..3] as sample_operations\n",
    "    ORDER BY sql_operations_count DESC, path_length ASC\n",
    "    LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "lineage_results = execute_query(lineage_query, show_timing=True)\n",
    "display_query_stats(lineage_results, \"End-to-End Lineage\")\n",
    "\n",
    "if not lineage_results.empty:\n",
    "    print(f\"\\nüìã DATA LINEAGE WITH SQL SEMANTICS:\")\n",
    "    display(lineage_results.head(10))\n",
    "    \n",
    "    # Analyze lineage complexity\n",
    "    print(f\"\\nüìä LINEAGE COMPLEXITY ANALYSIS:\")\n",
    "    avg_path_length = lineage_results['path_length'].mean()\n",
    "    max_path_length = lineage_results['path_length'].max()\n",
    "    sql_coverage = (lineage_results['sql_operations_count'] > 0).mean() * 100\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Average path length: {avg_path_length:.1f} hops\")\n",
    "    print(f\"   ‚Ä¢ Maximum path length: {max_path_length} hops\")\n",
    "    print(f\"   ‚Ä¢ Paths with SQL semantics: {sql_coverage:.1f}%\")\n",
    "    \n",
    "    # Find critical data transformation paths\n",
    "    critical_paths = lineage_results[lineage_results['sql_operations_count'] >= 2]\n",
    "    print(f\"\\nüéØ CRITICAL TRANSFORMATION PATHS:\")\n",
    "    print(f\"   Paths with 2+ SQL operations: {len(critical_paths)}\")\n",
    "    \n",
    "    if not critical_paths.empty:\n",
    "        for idx, row in critical_paths.head(5).iterrows():\n",
    "            operations_str = ', '.join(row['sample_operations'][:2]) + ('...' if len(row['sample_operations']) > 2 else '')\n",
    "            print(f\"   ‚Ä¢ {row['source_table']} ‚Üí {row['target_table']}\")\n",
    "            print(f\"     Path: {row['path_length']} hops, {row['sql_operations_count']} SQL ops\")\n",
    "            print(f\"     Operations: {operations_str}\")\nelse:\n",
    "    print(\"‚ùå No data lineage paths found with SQL semantics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "package-interaction-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Complex package interaction networks\n",
    "print(f\"\\nüîó COMPLEX PACKAGE INTERACTION ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "interaction_query = \"\"\"\n",
    "    MATCH (pkg1:Node)-[:CONTAINS*]->(op1:Node)-[r:READS_FROM|WRITES_TO]->(asset:Node)<-[r2:READS_FROM|WRITES_TO]-(op2:Node)<-[:CONTAINS*]-(pkg2:Node)\n",
    "    WHERE pkg1.node_type = 'pipeline' AND pkg2.node_type = 'pipeline' \n",
    "          AND asset.node_type = 'data_asset'\n",
    "          AND pkg1.name <> pkg2.name\n",
    "          AND (op1.properties CONTAINS 'sql_semantics' OR op2.properties CONTAINS 'sql_semantics')\n",
    "    WITH pkg1, pkg2, asset, \n",
    "         collect(DISTINCT op1.name) as pkg1_operations,\n",
    "         collect(DISTINCT op2.name) as pkg2_operations,\n",
    "         type(r) as interaction_type1,\n",
    "         type(r2) as interaction_type2\n",
    "    RETURN \n",
    "        pkg1.name as package1,\n",
    "        pkg2.name as package2,\n",
    "        asset.name as shared_asset,\n",
    "        interaction_type1 + \"/\" + interaction_type2 as interaction_pattern,\n",
    "        size(pkg1_operations) as pkg1_op_count,\n",
    "        size(pkg2_operations) as pkg2_op_count,\n",
    "        pkg1_operations[0..2] as sample_pkg1_ops,\n",
    "        pkg2_operations[0..2] as sample_pkg2_ops\n",
    "    ORDER BY pkg1_op_count + pkg2_op_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "interaction_results = execute_query(interaction_query, show_timing=True)\n",
    "display_query_stats(interaction_results, \"Package Interactions\")\n",
    "\n",
    "if not interaction_results.empty:\n",
    "    print(f\"\\nüìã PACKAGE INTERACTION NETWORK:\")\n",
    "    display(interaction_results.head(12))\n",
    "    \n",
    "    # Analyze interaction patterns\n",
    "    print(f\"\\nüìä INTERACTION PATTERN ANALYSIS:\")\n",
    "    pattern_counts = interaction_results['interaction_pattern'].value_counts()\n",
    "    \n",
    "    for pattern, count in pattern_counts.items():\n",
    "        print(f\"   ‚Ä¢ {pattern}: {count} interactions\")\n",
    "    \n",
    "    # Identify high-impact shared assets\n",
    "    asset_impact = interaction_results.groupby('shared_asset').agg({\n",
    "        'package1': 'nunique',\n",
    "        'package2': 'nunique',\n",
    "        'pkg1_op_count': 'sum',\n",
    "        'pkg2_op_count': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    asset_impact['total_packages'] = asset_impact['package1'] + asset_impact['package2']\n",
    "    asset_impact['total_operations'] = asset_impact['pkg1_op_count'] + asset_impact['pkg2_op_count']\n",
    "    asset_impact = asset_impact.sort_values('total_operations', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüéØ HIGH-IMPACT SHARED ASSETS:\")\n",
    "    print(\"   (Assets involved in the most package interactions)\")\n",
    "    for idx, row in asset_impact.head(8).iterrows():\n",
    "        print(f\"   ‚Ä¢ {row['shared_asset']}: {row['total_packages']} packages, {row['total_operations']} operations\")\nelse:\n",
    "    print(\"‚ùå No complex package interactions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sql-semantics-traversal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Advanced SQL semantics traversal patterns\n",
    "print(f\"\\nüîç SQL SEMANTICS TRAVERSAL PATTERNS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find operations connected through table references in SQL semantics\n",
    "sql_traversal_query = \"\"\"\n",
    "    MATCH (op1:Node), (op2:Node)\n",
    "    WHERE op1.node_type = 'operation' AND op2.node_type = 'operation'\n",
    "          AND op1.properties CONTAINS 'sql_semantics' AND op2.properties CONTAINS 'sql_semantics'\n",
    "          AND op1.name <> op2.name\n",
    "    WITH op1, op2, \n",
    "         op1.properties.sql_semantics as sql1_raw,\n",
    "         op2.properties.sql_semantics as sql2_raw\n",
    "    RETURN \n",
    "        op1.name as operation1,\n",
    "        op2.name as operation2,\n",
    "        sql1_raw,\n",
    "        sql2_raw\n",
    "    LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "sql_traversal_results = execute_query(sql_traversal_query, show_timing=True)\n",
    "\n",
    "if not sql_traversal_results.empty:\n",
    "    # Analyze semantic connections\n",
    "    semantic_connections = []\n",
    "    \n",
    "    for idx, row in sql_traversal_results.iterrows():\n",
    "        try:\n",
    "            # Parse SQL semantics\n",
    "            sql1 = json.loads(row['sql1_raw']) if isinstance(row['sql1_raw'], str) else row['sql1_raw']\n",
    "            sql2 = json.loads(row['sql2_raw']) if isinstance(row['sql2_raw'], str) else row['sql2_raw']\n",
    "            \n",
    "            # Extract table names\n",
    "            tables1 = {t['name'] for t in sql1.get('tables', [])}\n",
    "            tables2 = {t['name'] for t in sql2.get('tables', [])}\n",
    "            \n",
    "            # Find shared tables\n",
    "            shared_tables = tables1.intersection(tables2)\n",
    "            \n",
    "            if shared_tables:\n",
    "                # Analyze JOIN patterns\n",
    "                joins1 = sql1.get('joins', [])\n",
    "                joins2 = sql2.get('joins', [])\n",
    "                \n",
    "                semantic_connections.append({\n",
    "                    'operation1': row['operation1'],\n",
    "                    'operation2': row['operation2'],\n",
    "                    'shared_tables': list(shared_tables),\n",
    "                    'shared_table_count': len(shared_tables),\n",
    "                    'op1_join_count': len(joins1),\n",
    "                    'op2_join_count': len(joins2),\n",
    "                    'op1_table_count': len(tables1),\n",
    "                    'op2_table_count': len(tables2),\n",
    "                    'connection_strength': len(shared_tables) / max(len(tables1), len(tables2), 1)\n",
    "                })\n",
    "        \n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            continue\n",
    "    \n",
    "    if semantic_connections:\n",
    "        semantic_df = pd.DataFrame(semantic_connections)\n",
    "        semantic_df = semantic_df.sort_values('connection_strength', ascending=False)\n",
    "        \n",
    "        print(f\"üìä SQL SEMANTIC CONNECTIONS FOUND: {len(semantic_df)}\")\n",
    "        display_query_stats(semantic_df, \"Semantic Connections\")\n",
    "        \n",
    "        print(f\"\\nüîó STRONGEST SEMANTIC CONNECTIONS:\")\n",
    "        for idx, row in semantic_df.head(10).iterrows():\n",
    "            shared_str = ', '.join(row['shared_tables'][:3]) + ('...' if len(row['shared_tables']) > 3 else '')\n",
    "            print(f\"   ‚Ä¢ {row['operation1']} ‚Üî {row['operation2']}\")\n",
    "            print(f\"     Shared tables: {shared_str}\")\n",
    "            print(f\"     Connection strength: {row['connection_strength']:.2f}\")\n",
    "            print(f\"     JOINs: {row['op1_join_count']} / {row['op2_join_count']}\")\n",
    "        \n",
    "        # Visualize connection network\n",
    "        if len(semantic_df) > 2:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Create network graph\n",
    "            G = nx.Graph()\n",
    "            \n",
    "            # Add edges with weights\n",
    "            for idx, row in semantic_df.head(20).iterrows():  # Limit for readability\n",
    "                G.add_edge(row['operation1'], row['operation2'], \n",
    "                          weight=row['connection_strength'],\n",
    "                          shared_tables=len(row['shared_tables']))\n",
    "            \n",
    "            # Position nodes\n",
    "            pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "            \n",
    "            # Draw network\n",
    "            edges = G.edges(data=True)\n",
    "            weights = [edge[2]['weight'] for edge in edges]\n",
    "            \n",
    "            nx.draw_networkx_nodes(G, pos, node_size=300, node_color='lightblue', alpha=0.7)\n",
    "            nx.draw_networkx_edges(G, pos, width=[w*5 for w in weights], alpha=0.6, edge_color='gray')\n",
    "            nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "            \n",
    "            plt.title('SQL Semantic Connection Network\\n(Edge thickness = connection strength)')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n            \n    else:\n        print(\"‚ùå No semantic connections found between operations.\")\nelse:\n    print(\"‚ùå No operations with SQL semantics found for traversal analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern-matching",
   "metadata": {},
   "source": [
    "## 2. Advanced Pattern Matching with SQL Semantics\n",
    "\n",
    "Use sophisticated pattern matching to identify specific migration scenarios and optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anti-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern matching for migration anti-patterns and optimization opportunities\n",
    "print(\"‚ö†Ô∏è MIGRATION ANTI-PATTERN DETECTION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Anti-pattern 1: Cartesian products (missing JOIN conditions)\n",
    "cartesian_query = \"\"\"\n",
    "    MATCH (op:Node)\n",
    "    WHERE op.node_type = 'operation' AND op.properties CONTAINS 'sql_semantics'\n",
    "    WITH op, op.properties.sql_semantics as sql_raw\n",
    "    RETURN \n",
    "        op.name as operation_name,\n",
    "        op.properties.operation_type as operation_type,\n",
    "        sql_raw\n",
    "\"\"\"\n",
    "\n",
    "operations_for_analysis = execute_query(cartesian_query)\n",
    "\n",
    "anti_patterns = {\n",
    "    'cartesian_products': [],\n",
    "    'complex_joins': [],\n",
    "    'inefficient_patterns': [],\n",
    "    'migration_challenges': []\n",
    "}\n",
    "\n",
    "optimization_opportunities = {\n",
    "    'index_candidates': [],\n",
    "    'view_consolidation': [],\n",
    "    'query_rewrite': []\n",
    "}\n",
    "\n",
    "for idx, row in operations_for_analysis.iterrows():\n",
    "    try:\n",
    "        sql_semantics = json.loads(row['sql_raw']) if isinstance(row['sql_raw'], str) else row['sql_raw']\n",
    "        \n",
    "        tables = sql_semantics.get('tables', [])\n",
    "        joins = sql_semantics.get('joins', [])\n",
    "        columns = sql_semantics.get('columns', [])\n",
    "        \n",
    "        # Detect anti-patterns\n",
    "        \n",
    "        # 1. Potential Cartesian products (multiple tables, no JOINs)\n",
    "        if len(tables) > 1 and len(joins) == 0:\n",
    "            anti_patterns['cartesian_products'].append({\n",
    "                'operation': row['operation_name'],\n",
    "                'table_count': len(tables),\n",
    "                'tables': [t['name'] for t in tables],\n",
    "                'risk_level': 'HIGH' if len(tables) > 3 else 'MEDIUM'\n",
    "            })\n",
    "        \n",
    "        # 2. Complex JOINs (many tables with complex conditions)\n",
    "        if len(joins) > 0:\n",
    "            complex_conditions = sum(1 for j in joins if len(j.get('condition', '').split()) > 10)\n",
    "            if len(joins) >= 4 or complex_conditions > 0:\n",
    "                anti_patterns['complex_joins'].append({\n",
    "                    'operation': row['operation_name'],\n",
    "                    'join_count': len(joins),\n",
    "                    'complex_conditions': complex_conditions,\n",
    "                    'join_types': [j['join_type'] for j in joins],\n",
    "                    'risk_level': 'HIGH' if len(joins) >= 6 else 'MEDIUM'\n",
    "                })\n",
    "        \n",
    "        # 3. Inefficient patterns (SELECT *, many columns)\n",
    "        select_all_pattern = any('*' in c.get('expression', '') for c in columns)\n",
    "        many_columns = len(columns) > 20\n",
    "        \n",
    "        if select_all_pattern or many_columns:\n",
    "            anti_patterns['inefficient_patterns'].append({\n",
    "                'operation': row['operation_name'],\n",
    "                'select_all': select_all_pattern,\n",
    "                'column_count': len(columns),\n",
    "                'table_count': len(tables),\n",
    "                'performance_impact': 'HIGH' if select_all_pattern and len(tables) > 2 else 'MEDIUM'\n",
    "            })\n",
    "        \n",
    "        # Identify optimization opportunities\n",
    "        \n",
    "        # 1. Index candidates (frequent column references in JOINs)\n",
    "        join_columns = set()\n",
    "        for join in joins:\n",
    "            condition = join.get('condition', '')\n",
    "            # Simple pattern matching for equality conditions\n",
    "            if '=' in condition:\n",
    "                parts = condition.split('=')\n",
    "                for part in parts:\n",
    "                    if '.' in part.strip():\n",
    "                        col = part.strip().split('.')[-1]\n",
    "                        join_columns.add(col)\n",
    "        \n",
    "        if join_columns:\n",
    "            optimization_opportunities['index_candidates'].append({\n",
    "                'operation': row['operation_name'],\n",
    "                'suggested_indexes': list(join_columns),\n",
    "                'tables': [t['name'] for t in tables],\n",
    "                'priority': 'HIGH' if len(join_columns) > 2 else 'MEDIUM'\n",
    "            })\n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        continue\n",
    "\n",
    "# Display anti-pattern analysis results\n",
    "print(f\"üìä ANTI-PATTERN DETECTION SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Potential Cartesian Products: {len(anti_patterns['cartesian_products'])}\")\n",
    "print(f\"   ‚Ä¢ Complex JOINs: {len(anti_patterns['complex_joins'])}\")\n",
    "print(f\"   ‚Ä¢ Inefficient Patterns: {len(anti_patterns['inefficient_patterns'])}\")\n",
    "print(f\"   ‚Ä¢ Index Candidates: {len(optimization_opportunities['index_candidates'])}\")\n",
    "\n",
    "# Detailed anti-pattern analysis\n",
    "if anti_patterns['cartesian_products']:\n",
    "    print(f\"\\n‚ö†Ô∏è  CARTESIAN PRODUCT RISKS:\")\n",
    "    for pattern in anti_patterns['cartesian_products'][:5]:\n",
    "        tables_str = ', '.join(pattern['tables'][:3]) + ('...' if len(pattern['tables']) > 3 else '')\n",
    "        print(f\"   ‚Ä¢ {pattern['operation']} ({pattern['risk_level']} risk)\")\n",
    "        print(f\"     Tables: {tables_str} ({pattern['table_count']} total)\")\n",
    "        print(f\"     ‚ö° Recommendation: Add explicit JOIN conditions\")\n",
    "\n",
    "if anti_patterns['complex_joins']:\n",
    "    print(f\"\\nüîó COMPLEX JOIN PATTERNS:\")\n",
    "    for pattern in anti_patterns['complex_joins'][:5]:\n",
    "        join_types_str = ', '.join(set(pattern['join_types'][:3]))\n",
    "        print(f\"   ‚Ä¢ {pattern['operation']} ({pattern['risk_level']} complexity)\")\n",
    "        print(f\"     JOINs: {pattern['join_count']}, Complex conditions: {pattern['complex_conditions']}\")\n",
    "        print(f\"     Types: {join_types_str}\")\n",
    "        print(f\"     ‚ö° Recommendation: Consider query decomposition\")\n",
    "\n",
    "if optimization_opportunities['index_candidates']:\n",
    "    print(f\"\\nüìà INDEX OPTIMIZATION OPPORTUNITIES:\")\n",
    "    for opp in optimization_opportunities['index_candidates'][:5]:\n",
    "        indexes_str = ', '.join(opp['suggested_indexes'][:3])\n",
    "        tables_str = ', '.join(opp['tables'][:2])\n",
    "        print(f\"   ‚Ä¢ {opp['operation']} ({opp['priority']} priority)\")\n",
    "        print(f\"     Suggested indexes: {indexes_str}\")\n",
    "        print(f\"     Target tables: {tables_str}\")\n",
    "\n",
    "# Migration complexity scoring based on anti-patterns\n",
    "if any(anti_patterns.values()):\n",
    "    print(f\"\\nüéØ MIGRATION COMPLEXITY BY ANTI-PATTERNS:\")\n",
    "    \n",
    "    complexity_scores = {}\n",
    "    \n",
    "    # Score operations based on anti-patterns found\n",
    "    all_operations = set()\n",
    "    for pattern_list in anti_patterns.values():\n",
    "        for pattern in pattern_list:\n",
    "            all_operations.add(pattern['operation'])\n",
    "    \n",
    "    for op in all_operations:\n",
    "        score = 0\n",
    "        issues = []\n",
    "        \n",
    "        # Check each anti-pattern category\n",
    "        for cart in anti_patterns['cartesian_products']:\n",
    "            if cart['operation'] == op:\n",
    "                score += 15 if cart['risk_level'] == 'HIGH' else 10\n",
    "                issues.append(f\"Cartesian product risk ({cart['table_count']} tables)\")\n",
    "        \n",
    "        for join in anti_patterns['complex_joins']:\n",
    "            if join['operation'] == op:\n",
    "                score += 12 if join['risk_level'] == 'HIGH' else 8\n",
    "                issues.append(f\"Complex JOINs ({join['join_count']} joins)\")\n",
    "        \n",
    "        for ineff in anti_patterns['inefficient_patterns']:\n",
    "            if ineff['operation'] == op:\n",
    "                score += 8 if ineff['performance_impact'] == 'HIGH' else 5\n",
    "                issues.append(f\"Inefficient patterns ({ineff['column_count']} columns)\")\n",
    "        \n",
    "        complexity_scores[op] = {'score': score, 'issues': issues}\n",
    "    \n",
    "    # Sort by complexity score\n",
    "    sorted_complexity = sorted(complexity_scores.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    \n",
    "    print(f\"   Operations ranked by migration complexity:\")\n",
    "    for op, data in sorted_complexity[:8]:\n",
    "        category = \"üî¥ High\" if data['score'] >= 20 else \"üü° Medium\" if data['score'] >= 10 else \"üü¢ Low\"\n",
    "        print(f\"   ‚Ä¢ {op} - {category} ({data['score']} points)\")\n",
    "        print(f\"     Issues: {', '.join(data['issues'][:2])}\")\nelse:\n    print(\"‚úÖ No significant anti-patterns detected in current dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "migration-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern matching for specific migration scenarios\n",
    "print(f\"\\nüéØ MIGRATION SCENARIO PATTERN MATCHING:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Scenario 1: ETL to ELT transformation candidates\n",
    "etl_to_elt_query = \"\"\"\n",
    "    MATCH (pkg:Node)-[:CONTAINS]->(op:Node)-[:WRITES_TO]->(target:Node)\n",
    "    WHERE pkg.node_type = 'pipeline' AND op.node_type = 'operation' \n",
    "          AND target.node_type = 'data_asset'\n",
    "          AND op.properties CONTAINS 'sql_semantics'\n",
    "    OPTIONAL MATCH (op)-[:READS_FROM]->(source:Node)\n",
    "    WHERE source.node_type = 'data_asset'\n",
    "    WITH pkg, op, target, count(DISTINCT source) as source_count,\n",
    "         op.properties.sql_semantics as sql_raw\n",
    "    RETURN \n",
    "        pkg.name as package_name,\n",
    "        op.name as operation_name,\n",
    "        target.name as target_table,\n",
    "        source_count,\n",
    "        sql_raw\n",
    "    ORDER BY source_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "etl_candidates = execute_query(etl_to_elt_query, show_timing=True)\n",
    "\n",
    "if not etl_candidates.empty:\n",
    "    print(f\"üìä ETL TO ELT TRANSFORMATION CANDIDATES:\")\n",
    "    \n",
    "    elt_recommendations = []\n",
    "    \n",
    "    for idx, row in etl_candidates.iterrows():\n",
    "        try:\n",
    "            sql_semantics = json.loads(row['sql_raw']) if isinstance(row['sql_raw'], str) else row['sql_raw']\n",
    "            \n",
    "            tables = sql_semantics.get('tables', [])\n",
    "            joins = sql_semantics.get('joins', [])\n",
    "            columns = sql_semantics.get('columns', [])\n",
    "            \n",
    "            # Calculate ELT suitability score\n",
    "            elt_score = 0\n",
    "            factors = []\n",
    "            \n",
    "            # Multiple source tables favor ELT\n",
    "            if row['source_count'] >= 3:\n",
    "                elt_score += 25\n",
    "                factors.append(f\"{row['source_count']} sources\")\n",
    "            \n",
    "            # Complex JOINs are better in target system\n",
    "            if len(joins) >= 2:\n",
    "                elt_score += 20\n",
    "                factors.append(f\"{len(joins)} JOINs\")\n",
    "            \n",
    "            # Many columns suggest data warehouse loading\n",
    "            if len(columns) >= 10:\n",
    "                elt_score += 15\n",
    "                factors.append(f\"{len(columns)} columns\")\n",
    "            \n",
    "            # SQL-heavy transformations are ELT candidates\n",
    "            sql_complexity = len(joins) + len([c for c in columns if c.get('alias')])\n",
    "            if sql_complexity >= 5:\n",
    "                elt_score += 10\n",
    "                factors.append(\"SQL complexity\")\n",
    "            \n",
    "            if elt_score >= 30:  # Threshold for ELT recommendation\n",
    "                elt_recommendations.append({\n",
    "                    'package': row['package_name'],\n",
    "                    'operation': row['operation_name'],\n",
    "                    'target': row['target_table'],\n",
    "                    'elt_score': elt_score,\n",
    "                    'factors': factors,\n",
    "                    'source_count': row['source_count'],\n",
    "                    'join_count': len(joins)\n",
    "                })\n",
    "        \n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            continue\n",
    "    \n",
    "    if elt_recommendations:\n",
    "        elt_df = pd.DataFrame(elt_recommendations)\n",
    "        elt_df = elt_df.sort_values('elt_score', ascending=False)\n",
    "        \n",
    "        print(f\"   üü¢ Strong ELT candidates found: {len(elt_df)}\")\n",
    "        \n",
    "        for idx, row in elt_df.head(8).iterrows():\n",
    "            factors_str = ', '.join(row['factors'][:3])\n",
    "            print(f\"   ‚Ä¢ {row['operation']} (Score: {row['elt_score']})\")\n",
    "            print(f\"     Package: {row['package']}, Target: {row['target']}\")\n",
    "            print(f\"     ELT factors: {factors_str}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå No strong ELT candidates identified.\")\nelse:\n    print(\"‚ùå No operations found for ETL/ELT analysis.\")\n\n# Scenario 2: Real-time streaming candidates\nprint(f\"\\nüîÑ REAL-TIME STREAMING MIGRATION CANDIDATES:\")\nprint(\"=\" * 50)\n\n# Look for patterns that suggest real-time processing needs\nstreaming_indicators = {\n    'frequent_small_batches': [],\n    'incremental_patterns': [],\n    'low_latency_targets': []\n}\n\n# This would require additional metadata about execution frequency and data volumes\n# For demonstration, we'll identify patterns that commonly benefit from streaming\n\nfor idx, row in etl_candidates.iterrows():\n    try:\n        sql_semantics = json.loads(row['sql_raw']) if isinstance(row['sql_raw'], str) else row['sql_raw']\n        \n        # Look for incremental patterns in column names or SQL\n        columns = sql_semantics.get('columns', [])\n        original_query = sql_semantics.get('original_query', '').upper()\n        \n        # Check for incremental loading patterns\n        incremental_indicators = [\n            'TIMESTAMP', 'DATETIME', 'MODIFIED', 'UPDATED', 'CREATED',\n            'DATE', 'DELTA', 'INCREMENTAL', 'LAST_MODIFIED'\n        ]\n        \n        has_incremental = any(indicator in original_query for indicator in incremental_indicators)\n        \n        if has_incremental:\n            streaming_indicators['incremental_patterns'].append({\n                'operation': row['operation_name'],\n                'package': row['package_name'],\n                'target': row['target_table'],\n                'indicators_found': [ind for ind in incremental_indicators if ind in original_query]\n            })\n    \n    except (json.JSONDecodeError, TypeError):\n        continue\n\nif streaming_indicators['incremental_patterns']:\n    print(f\"   üîÑ Incremental/Streaming candidates: {len(streaming_indicators['incremental_patterns'])}\")\n    \n    for candidate in streaming_indicators['incremental_patterns'][:5]:\n        indicators_str = ', '.join(candidate['indicators_found'][:3])\n        print(f\"   ‚Ä¢ {candidate['operation']}\")\n        print(f\"     Indicators: {indicators_str}\")\n        print(f\"     üí° Consider: Kafka/Event streaming migration\")\nelse:\n    print(\"   ‚ùå No clear streaming candidates identified.\")\n\nprint(f\"\\nüí° MIGRATION STRATEGY RECOMMENDATIONS:\")\nprint(\"Based on pattern analysis:\")\nif elt_recommendations:\n    print(f\"   üéØ {len(elt_recommendations)} operations suitable for ELT approach\")\n    print(f\"   üìã Recommended platforms: Snowflake, BigQuery, Synapse\")\nif streaming_indicators['incremental_patterns']:\n    print(f\"   üîÑ {len(streaming_indicators['incremental_patterns'])} operations for streaming migration\")\n    print(f\"   üìã Recommended platforms: Kafka + Spark, Azure Event Hubs\")\nif anti_patterns:\n    total_anti_patterns = sum(len(patterns) for patterns in anti_patterns.values())\n    print(f\"   ‚ö†Ô∏è  {total_anti_patterns} anti-patterns requiring manual review\")\n    print(f\"   üìã Recommended approach: Staged migration with optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-algorithms",
   "metadata": {},
   "source": [
    "## 3. Graph Algorithms for Migration Optimization\n",
    "\n",
    "Apply graph algorithms to optimize migration sequencing, identify critical paths, and minimize dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-path-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical path analysis for migration sequencing\n",
    "print(\"üéØ MIGRATION CRITICAL PATH ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Build comprehensive dependency graph\n",
    "dependency_query = \"\"\"\n",
    "    MATCH (source:Node)-[r]->(target:Node)\n",
    "    WHERE (source.node_type = 'pipeline' OR source.node_type = 'operation' OR source.node_type = 'data_asset')\n",
    "          AND (target.node_type = 'pipeline' OR target.node_type = 'operation' OR target.node_type = 'data_asset')\n",
    "          AND type(r) IN ['CONTAINS', 'READS_FROM', 'WRITES_TO', 'DEPENDS_ON', 'REFERENCES']\n",
    "    RETURN \n",
    "        source.name as source_name,\n",
    "        source.node_type as source_type,\n",
    "        target.name as target_name,\n",
    "        target.node_type as target_type,\n",
    "        type(r) as relationship_type\n",
    "\"\"\"\n",
    "\n",
    "dependencies = execute_query(dependency_query, show_timing=True)\n",
    "display_query_stats(dependencies, \"Dependencies\")\n",
    "\n",
    "if not dependencies.empty:\n",
    "    # Create NetworkX graph for analysis\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    nodes_added = set()\n",
    "    for idx, row in dependencies.iterrows():\n",
    "        source_id = f\"{row['source_type']}:{row['source_name']}\"\n",
    "        target_id = f\"{row['target_type']}:{row['target_name']}\"\n",
    "        \n",
    "        if source_id not in nodes_added:\n",
    "            G.add_node(source_id, \n",
    "                      name=row['source_name'], \n",
    "                      node_type=row['source_type'])\n",
    "            nodes_added.add(source_id)\n",
    "        \n",
    "        if target_id not in nodes_added:\n",
    "            G.add_node(target_id, \n",
    "                      name=row['target_name'], \n",
    "                      node_type=row['target_type'])\n",
    "            nodes_added.add(target_id)\n",
    "        \n",
    "        # Add edge with relationship type\n",
    "        G.add_edge(source_id, target_id, \n",
    "                  relationship=row['relationship_type'])\n",
    "    \n",
    "    print(f\"üìä DEPENDENCY GRAPH ANALYSIS:\")\n",
    "    print(f\"   ‚Ä¢ Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"   ‚Ä¢ Edges: {G.number_of_edges()}\")\n",
    "    print(f\"   ‚Ä¢ Is DAG: {nx.is_directed_acyclic_graph(G)}\")\n",
    "    \n",
    "    # Check for cycles (problematic for migration)\n",
    "    if not nx.is_directed_acyclic_graph(G):\n",
    "        try:\n",
    "            cycles = list(nx.simple_cycles(G))\n",
    "            print(f\"   ‚ö†Ô∏è  Cycles detected: {len(cycles)}\")\n",
    "            \n",
    "            for i, cycle in enumerate(cycles[:3]):  # Show first 3 cycles\n",
    "                cycle_names = [G.nodes[node]['name'] for node in cycle]\n",
    "                print(f\"      Cycle {i+1}: {' ‚Üí '.join(cycle_names[:4])}{'...' if len(cycle_names) > 4 else ''}\")\n",
    "        except:\n",
    "            print(f\"   ‚ö†Ô∏è  Complex cycles detected - detailed analysis needed\")\n",
    "    \n",
    "    # Calculate centrality metrics for migration priority\n",
    "    try:\n",
    "        in_degree_centrality = nx.in_degree_centrality(G)\n",
    "        out_degree_centrality = nx.out_degree_centrality(G)\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        \n",
    "        # Focus on pipeline nodes for migration planning\n",
    "        pipeline_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'pipeline']\n",
    "        \n",
    "        if pipeline_nodes:\n",
    "            print(f\"\\nüéØ PACKAGE MIGRATION PRIORITY (by centrality):\")\n",
    "            \n",
    "            # Create priority ranking\n",
    "            priority_scores = []\n",
    "            for node in pipeline_nodes:\n",
    "                priority_score = (\n",
    "                    out_degree_centrality.get(node, 0) * 0.4 +  # Data producers\n",
    "                    betweenness_centrality.get(node, 0) * 0.4 + # Critical connectors\n",
    "                    (1 - in_degree_centrality.get(node, 0)) * 0.2  # Less dependent\n",
    "                )\n",
    "                \n",
    "                priority_scores.append({\n",
    "                    'package': G.nodes[node]['name'],\n",
    "                    'node_id': node,\n",
    "                    'priority_score': priority_score,\n",
    "                    'out_degree': out_degree_centrality.get(node, 0),\n",
    "                    'in_degree': in_degree_centrality.get(node, 0),\n",
    "                    'betweenness': betweenness_centrality.get(node, 0)\n",
    "                })\n",
    "            \n",
    "            # Sort by priority score\n",
    "            priority_scores.sort(key=lambda x: x['priority_score'], reverse=True)\n",
    "            \n",
    "            print(f\"   Migration wave recommendations:\")\n",
    "            \n",
    "            # Wave 1: High priority (data producers)\n",
    "            wave1 = [p for p in priority_scores if p['priority_score'] > 0.3]\n",
    "            if wave1:\n",
    "                print(f\"\\n   üöÄ Wave 1 (Data Producers): {len(wave1)} packages\")\n",
    "                for pkg in wave1[:5]:\n",
    "                    print(f\"      ‚Ä¢ {pkg['package']} (Score: {pkg['priority_score']:.3f})\")\n",
    "                    print(f\"        Out-degree: {pkg['out_degree']:.3f}, Betweenness: {pkg['betweenness']:.3f}\")\n",
    "            \n",
    "            # Wave 2: Medium priority\n",
    "            wave2 = [p for p in priority_scores if 0.1 <= p['priority_score'] <= 0.3]\n",
    "            if wave2:\n",
    "                print(f\"\\n   üîÑ Wave 2 (Intermediate): {len(wave2)} packages\")\n",
    "                for pkg in wave2[:3]:\n",
    "                    print(f\"      ‚Ä¢ {pkg['package']} (Score: {pkg['priority_score']:.3f})\")\n",
    "            \n",
    "            # Wave 3: Low priority (data consumers)\n",
    "            wave3 = [p for p in priority_scores if p['priority_score'] < 0.1]\n",
    "            if wave3:\n",
    "                print(f\"\\n   üì• Wave 3 (Data Consumers): {len(wave3)} packages\")\n",
    "                for pkg in wave3[:3]:\n",
    "                    print(f\"      ‚Ä¢ {pkg['package']} (Score: {pkg['priority_score']:.3f})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Centrality analysis failed: {e}\")\n",
    "    \n",
    "    # Find longest paths (critical migration paths)\n",
    "    if nx.is_directed_acyclic_graph(G):\n",
    "        try:\n",
    "            # Find all simple paths between pipeline nodes\n",
    "            pipeline_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'pipeline']\n",
    "            \n",
    "            if len(pipeline_nodes) >= 2:\n",
    "                print(f\"\\nüìè CRITICAL MIGRATION PATHS:\")\n",
    "                \n",
    "                longest_paths = []\n",
    "                \n",
    "                # Find paths between pipeline pairs\n",
    "                for source in pipeline_nodes[:5]:  # Limit for performance\n",
    "                    for target in pipeline_nodes[:5]:\n",
    "                        if source != target:\n",
    "                            try:\n",
    "                                paths = list(nx.all_simple_paths(G, source, target, cutoff=8))\n",
    "                                for path in paths:\n",
    "                                    longest_paths.append({\n",
    "                                        'path': path,\n",
    "                                        'length': len(path),\n",
    "                                        'source': G.nodes[source]['name'],\n",
    "                                        'target': G.nodes[target]['name']\n",
    "                                    })\n",
    "                            except nx.NetworkXNoPath:\n",
    "                                continue\n",
    "                \n",
    "                if longest_paths:\n",
    "                    # Sort by path length\n",
    "                    longest_paths.sort(key=lambda x: x['length'], reverse=True)\n",
    "                    \n",
    "                    print(f\"   Longest dependency chains found: {len(longest_paths)}\")\n",
    "                    \n",
    "                    for i, path_info in enumerate(longest_paths[:5]):\n",
    "                        path_names = [G.nodes[node]['name'] for node in path_info['path']]\n",
    "                        print(f\"   {i+1}. {path_info['source']} ‚Üí {path_info['target']}\")\n",
    "                        print(f\"      Length: {path_info['length']} hops\")\n",
    "                        print(f\"      Path: {' ‚Üí '.join(path_names[:4])}{'...' if len(path_names) > 4 else ''}\")\n                \n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è  Path analysis failed: {e}\")\n    \n    # Resource contention analysis\n    print(f\"\\nüè≠ RESOURCE CONTENTION ANALYSIS:\")\n    \n    # Find data assets accessed by multiple packages\n    asset_usage = dependencies[\n        (dependencies['source_type'] == 'operation') & \n        (dependencies['target_type'] == 'data_asset') &\n        (dependencies['relationship_type'].isin(['READS_FROM', 'WRITES_TO']))\n    ]\n    \n    if not asset_usage.empty:\n        # Group by asset and count unique packages accessing it\n        asset_contention = asset_usage.groupby('target_name').agg({\n            'source_name': 'nunique',\n            'relationship_type': lambda x: list(x)\n        }).reset_index()\n        \n        asset_contention.columns = ['asset_name', 'operation_count', 'access_types']\n        asset_contention = asset_contention[asset_contention['operation_count'] > 1]\n        asset_contention = asset_contention.sort_values('operation_count', ascending=False)\n        \n        if not asset_contention.empty:\n            print(f\"   Shared assets requiring coordination: {len(asset_contention)}\")\n            \n            for idx, row in asset_contention.head(8).iterrows():\n                access_summary = Counter(row['access_types'])\n                access_str = ', '.join([f\"{k}: {v}\" for k, v in access_summary.items()])\n                print(f\"   ‚Ä¢ {row['asset_name']}\")\n                print(f\"     Used by {row['operation_count']} operations\")\n                print(f\"     Access patterns: {access_str}\")\n                print(f\"     üí° Coordination needed: {'High' if row['operation_count'] > 5 else 'Medium'} priority\")\n        else:\n            print(f\"   ‚úÖ No significant resource contention detected\")\n    else:\n        print(f\"   ‚ùå No asset usage data available for contention analysis\")\nelse:\n    print(\"‚ùå No dependency data available for graph analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clustering-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering analysis for migration grouping\n",
    "print(f\"\\nüîó PACKAGE CLUSTERING FOR MIGRATION GROUPING:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not dependencies.empty and 'G' in locals():\n",
    "    try:\n",
    "        # Convert to undirected graph for clustering\n",
    "        G_undirected = G.to_undirected()\n",
    "        \n",
    "        # Get only pipeline nodes for clustering\n",
    "        pipeline_nodes = [n for n in G_undirected.nodes() if G_undirected.nodes[n]['node_type'] == 'pipeline']\n",
    "        \n",
    "        if len(pipeline_nodes) >= 3:\n",
    "            # Create subgraph with only pipelines and their connections\n",
    "            pipeline_subgraph = G_undirected.subgraph(pipeline_nodes)\n",
    "            \n",
    "            if pipeline_subgraph.number_of_edges() > 0:\n",
    "                # Use connected components as natural clusters\n",
    "                components = list(nx.connected_components(pipeline_subgraph))\n",
    "                \n",
    "                print(f\"üìä CLUSTERING ANALYSIS RESULTS:\")\n",
    "                print(f\"   ‚Ä¢ Connected components: {len(components)}\")\n",
    "                print(f\"   ‚Ä¢ Pipeline nodes analyzed: {len(pipeline_nodes)}\")\n",
    "                \n",
    "                # Analyze each cluster\n",
    "                cluster_analysis = []\n",
    "                \n",
    "                for i, component in enumerate(components):\n",
    "                    if len(component) > 1:  # Only analyze multi-node clusters\n",
    "                        cluster_nodes = list(component)\n",
    "                        cluster_names = [G_undirected.nodes[node]['name'] for node in cluster_nodes]\n",
    "                        \n",
    "                        # Calculate cluster metrics\n",
    "                        cluster_subgraph = pipeline_subgraph.subgraph(cluster_nodes)\n",
    "                        \n",
    "                        cluster_info = {\n",
    "                            'cluster_id': i + 1,\n",
    "                            'size': len(cluster_nodes),\n",
    "                            'packages': cluster_names,\n",
    "                            'density': nx.density(cluster_subgraph),\n",
    "                            'edges': cluster_subgraph.number_of_edges()\n",
    "                        }\n",
    "                        \n",
    "                        cluster_analysis.append(cluster_info)\n",
    "                \n",
    "                if cluster_analysis:\n",
    "                    # Sort clusters by size and density\n",
    "                    cluster_analysis.sort(key=lambda x: (x['size'], x['density']), reverse=True)\n",
    "                    \n",
    "                    print(f\"\\nüéØ MIGRATION CLUSTER RECOMMENDATIONS:\")\n",
    "                    \n",
    "                    for cluster in cluster_analysis[:5]:  # Show top 5 clusters\n",
    "                        packages_str = ', '.join(cluster['packages'][:3])\n",
    "                        if len(cluster['packages']) > 3:\n",
    "                            packages_str += f\" (+{len(cluster['packages']) - 3} more)\"\n",
    "                        \n",
    "                        complexity = \"High\" if cluster['density'] > 0.6 else \"Medium\" if cluster['density'] > 0.3 else \"Low\"\n",
    "                        \n",
    "                        print(f\"   üîó Cluster {cluster['cluster_id']}: {cluster['size']} packages\")\n",
    "                        print(f\"      Packages: {packages_str}\")\n",
    "                        print(f\"      Connectivity: {cluster['edges']} connections, {cluster['density']:.2f} density\")\n",
    "                        print(f\"      Migration complexity: {complexity}\")\n",
    "                        \n",
    "                        # Migration recommendations based on cluster characteristics\n",
    "                        if cluster['size'] <= 3 and cluster['density'] > 0.5:\n",
    "                            recommendation = \"‚úÖ Migrate together (tight coupling)\"\n",
    "                        elif cluster['size'] > 5:\n",
    "                            recommendation = \"‚ö†Ô∏è Consider sub-clustering (large group)\"\n",
    "                        elif cluster['density'] < 0.3:\n",
    "                            recommendation = \"üîÑ Can migrate separately (loose coupling)\"\n",
    "                        else:\n",
    "                            recommendation = \"üéØ Good migration wave candidate\"\n",
    "                        \n",
    "                        print(f\"      Recommendation: {recommendation}\")\n",
    "                        print()\n                    \n                    # Overall clustering insights\n                    total_clustered = sum(cluster['size'] for cluster in cluster_analysis)\n                    isolated_packages = len(pipeline_nodes) - total_clustered\n                    \n                    print(f\"üìà CLUSTERING INSIGHTS:\")\n                    print(f\"   ‚Ä¢ Packages in clusters: {total_clustered}\")\n                    print(f\"   ‚Ä¢ Isolated packages: {isolated_packages}\")\n                    print(f\"   ‚Ä¢ Average cluster size: {total_clustered / len(cluster_analysis):.1f}\")\n                    \n                    # Migration wave strategy based on clustering\n                    print(f\"\\nüöÄ CLUSTER-BASED MIGRATION STRATEGY:\")\n                    \n                    tight_clusters = [c for c in cluster_analysis if c['density'] > 0.5]\n                    loose_clusters = [c for c in cluster_analysis if c['density'] <= 0.5]\n                    \n                    if tight_clusters:\n                        print(f\"   Wave 1 - Tight clusters ({len(tight_clusters)} groups):\")\n                        for cluster in tight_clusters[:3]:\n                            print(f\"      ‚Ä¢ Cluster {cluster['cluster_id']}: {cluster['size']} packages\")\n                    \n                    if loose_clusters:\n                        print(f\"   Wave 2 - Loose clusters ({len(loose_clusters)} groups):\")\n                        for cluster in loose_clusters[:3]:\n                            print(f\"      ‚Ä¢ Cluster {cluster['cluster_id']}: {cluster['size']} packages\")\n                    \n                    if isolated_packages > 0:\n                        print(f\"   Wave 3 - Isolated packages: {isolated_packages} individual migrations\")\n                \n                else:\n                    print(\"   ‚ùå No significant clusters detected (packages are mostly independent)\")\n            \n            else:\n                print(\"   ‚ùå No connections found between pipeline nodes\")\n        \n        else:\n            print(f\"   ‚ùå Insufficient pipelines for clustering analysis ({len(pipeline_nodes)} found)\")\n    \n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è  Clustering analysis failed: {e}\")\n\nelse:\n    print(\"‚ùå No graph data available for clustering analysis.\")\n\n# Summary of all graph algorithm insights\nprint(f\"\\nüìã GRAPH ALGORITHM INSIGHTS SUMMARY:\")\nprint(\"=\" * 50)\nprint(\"Migration optimization recommendations:\")\nprint(\"   1. üéØ Use centrality analysis for migration wave prioritization\")\nprint(\"   2. üìè Identify critical paths to prevent bottlenecks\")\nprint(\"   3. üîó Group tightly coupled packages for joint migration\")\nprint(\"   4. üè≠ Coordinate shared resource migrations\")\nprint(\"   5. ‚ö†Ô∏è  Resolve circular dependencies before migration\")\nprint(\"   6. üìä Monitor resource contention during parallel migrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-analysis",
   "metadata": {},
   "source": [
    "## 4. Performance Analysis and Query Optimization\n",
    "\n",
    "Analyze query performance patterns and identify optimization opportunities for both current analysis and future migrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query performance analysis and optimization\n",
    "print(\"‚ö° QUERY PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Performance test different query patterns\n",
    "performance_tests = [\n",
    "    {\n",
    "        'name': 'Simple Node Count',\n",
    "        'query': 'MATCH (n:Node) RETURN count(n) as node_count',\n",
    "        'description': 'Basic node counting performance'\n",
    "    },\n",
    "    {\n",
    "        'name': 'SQL Semantics Filter',\n",
    "        'query': \"MATCH (n:Node) WHERE n.properties CONTAINS 'sql_semantics' RETURN count(n) as sql_nodes\",\n",
    "        'description': 'Property-based filtering performance'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Multi-hop Traversal',\n",
    "        'query': 'MATCH (a:Node)-[*1..3]-(b:Node) WHERE a.node_type = \"pipeline\" RETURN count(DISTINCT b) as connected_nodes LIMIT 1000',\n",
    "        'description': 'Multi-hop relationship traversal'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Complex Pattern Match',\n",
    "        'query': '''MATCH (pkg:Node)-[:CONTAINS]->(op:Node)-[:READS_FROM|WRITES_TO]->(asset:Node)\n",
    "                    WHERE pkg.node_type = 'pipeline' AND op.node_type = 'operation' AND asset.node_type = 'data_asset'\n",
    "                    RETURN count(*) as pattern_matches LIMIT 500''',\n",
    "        'description': 'Complex pattern matching with type filters'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Aggregation Query',\n",
    "        'query': '''MATCH (n:Node) \n",
    "                    RETURN n.node_type as type, count(*) as count, \n",
    "                           avg(size(keys(n.properties))) as avg_properties\n",
    "                    ORDER BY count DESC''',\n",
    "        'description': 'Aggregation with property analysis'\n",
    "    }\n]\n\n# Run performance tests\nperformance_results = []\n\nprint(f\"üî¨ RUNNING PERFORMANCE TESTS:\")\nprint(\"=\" * 50)\n\nfor test in performance_tests:\n    print(f\"\\n   Testing: {test['name']}\")\n    \n    # Run test multiple times for average\n    execution_times = []\n    \n    for run in range(3):  # 3 runs for average\n        start_time = time.time()\n        try:\n            result = execute_query(test['query'])\n            execution_time = time.time() - start_time\n            execution_times.append(execution_time)\n        except Exception as e:\n            print(f\"      ‚ùå Test failed: {e}\")\n            execution_times.append(float('inf'))\n            break\n    \n    if execution_times and all(t != float('inf') for t in execution_times):\n        avg_time = np.mean(execution_times)\n        min_time = min(execution_times)\n        max_time = max(execution_times)\n        \n        performance_results.append({\n            'test_name': test['name'],\n            'description': test['description'],\n            'avg_time': avg_time,\n            'min_time': min_time,\n            'max_time': max_time,\n            'result_count': len(result) if 'result' in locals() else 0\n        })\n        \n        print(f\"      ‚úÖ Avg: {avg_time:.3f}s, Range: {min_time:.3f}s - {max_time:.3f}s\")\n    else:\n        print(f\"      ‚ùå Test failed or inconsistent results\")\n\n# Analyze performance results\nif performance_results:\n    perf_df = pd.DataFrame(performance_results)\n    perf_df = perf_df.sort_values('avg_time')\n    \n    print(f\"\\nüìä PERFORMANCE ANALYSIS RESULTS:\")\n    print(\"=\" * 50)\n    \n    display(perf_df[['test_name', 'avg_time', 'min_time', 'max_time', 'result_count']])\n    \n    # Performance insights\n    print(f\"\\nüí° PERFORMANCE INSIGHTS:\")\n    \n    fastest = perf_df.iloc[0]\n    slowest = perf_df.iloc[-1]\n    \n    print(f\"   üèÜ Fastest query: {fastest['test_name']} ({fastest['avg_time']:.3f}s)\")\n    print(f\"   üêå Slowest query: {slowest['test_name']} ({slowest['avg_time']:.3f}s)\")\n    print(f\"   üìà Performance ratio: {slowest['avg_time'] / fastest['avg_time']:.1f}x difference\")\n    \n    # Categorize performance\n    fast_queries = perf_df[perf_df['avg_time'] < 0.1]\n    medium_queries = perf_df[(perf_df['avg_time'] >= 0.1) & (perf_df['avg_time'] < 1.0)]\n    slow_queries = perf_df[perf_df['avg_time'] >= 1.0]\n    \n    print(f\"\\n   Performance categories:\")\n    print(f\"   üü¢ Fast (< 0.1s): {len(fast_queries)} queries\")\n    print(f\"   üü° Medium (0.1-1.0s): {len(medium_queries)} queries\")\n    print(f\"   üî¥ Slow (> 1.0s): {len(slow_queries)} queries\")\n    \n    # Visualization\n    plt.figure(figsize=(12, 6))\n    \n    # Performance comparison\n    bars = plt.bar(range(len(perf_df)), perf_df['avg_time'], \n                   color=['green' if t < 0.1 else 'orange' if t < 1.0 else 'red' for t in perf_df['avg_time']])\n    \n    plt.xlabel('Query Type')\n    plt.ylabel('Average Execution Time (seconds)')\n    plt.title('Query Performance Comparison')\n    plt.xticks(range(len(perf_df)), perf_df['test_name'], rotation=45, ha='right')\n    \n    # Add value labels on bars\n    for i, bar in enumerate(bars):\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n                f'{height:.3f}s', ha='center', va='bottom', fontsize=9)\n    \n    plt.tight_layout()\n    plt.show()\n\nelse:\n    print(\"‚ùå No performance test results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization-recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query optimization recommendations\n",
    "print(f\"\\nüîß QUERY OPTIMIZATION RECOMMENDATIONS:\")\n",
    "print(\"=\" * 80)\n\n# Analyze graph database statistics for optimization insights\nstats_query = \"\"\"\n    MATCH (n:Node)\n    WITH n.node_type as node_type, count(*) as node_count,\n         avg(size(keys(n.properties))) as avg_properties,\n         max(size(keys(n.properties))) as max_properties\n    RETURN node_type, node_count, avg_properties, max_properties\n    ORDER BY node_count DESC\n\"\"\"\n\nstats_result = execute_query(stats_query)\n\nif not stats_result.empty:\n    print(f\"üìä GRAPH DATABASE STATISTICS:\")\n    display(stats_result)\n    \n    total_nodes = stats_result['node_count'].sum()\n    dominant_type = stats_result.iloc[0]\n    \n    print(f\"\\nüìà DATABASE INSIGHTS:\")\n    print(f\"   ‚Ä¢ Total nodes: {total_nodes:,}\")\n    print(f\"   ‚Ä¢ Dominant node type: {dominant_type['node_type']} ({dominant_type['node_count']:,} nodes)\")\n    print(f\"   ‚Ä¢ Average properties per node: {stats_result['avg_properties'].mean():.1f}\")\n    print(f\"   ‚Ä¢ Max properties in single node: {stats_result['max_properties'].max():.0f}\")\n    \n    # Generate optimization recommendations\n    optimization_recommendations = []\n    \n    # 1. Indexing recommendations\n    if total_nodes > 1000:\n        optimization_recommendations.append({\n            'category': 'üîç Indexing',\n            'priority': 'HIGH',\n            'recommendation': 'Create indexes on node_type property for faster filtering',\n            'impact': 'Significant performance improvement for type-based queries'\n        })\n    \n    # 2. Property-based recommendations\n    heavy_property_types = stats_result[stats_result['avg_properties'] > 10]\n    if not heavy_property_types.empty:\n        optimization_recommendations.append({\n            'category': 'üì¶ Property Storage',\n            'priority': 'MEDIUM',\n            'recommendation': f'Consider property normalization for {heavy_property_types[\"node_type\"].tolist()}',\n            'impact': 'Reduced memory usage and faster property access'\n        })\n    \n    # 3. Query pattern recommendations based on performance tests\n    if performance_results:\n        slow_patterns = [r for r in performance_results if r['avg_time'] > 1.0]\n        if slow_patterns:\n            optimization_recommendations.append({\n                'category': '‚ö° Query Patterns',\n                'priority': 'HIGH',\n                'recommendation': f'Optimize slow query patterns: {[p[\"test_name\"] for p in slow_patterns]}',\n                'impact': 'Faster analysis and reduced resource consumption'\n            })\n    \n    # 4. Migration-specific recommendations\n    optimization_recommendations.extend([\n        {\n            'category': 'üéØ Migration Queries',\n            'priority': 'HIGH',\n            'recommendation': 'Use LIMIT clauses for large traversals during migration analysis',\n            'impact': 'Prevents memory issues and timeouts'\n        },\n        {\n            'category': 'üîÑ Batch Processing',\n            'priority': 'MEDIUM',\n            'recommendation': 'Process SQL semantics parsing in batches of 100-500 operations',\n            'impact': 'Better memory management and progress tracking'\n        },\n        {\n            'category': 'üíæ Caching',\n            'priority': 'MEDIUM',\n            'recommendation': 'Cache frequently accessed migration patterns and dependency graphs',\n            'impact': 'Faster repeated analysis and reduced database load'\n        }\n    ])\n    \n    # Display recommendations\n    print(f\"\\nüí° OPTIMIZATION RECOMMENDATIONS:\")\n    print(\"=\" * 50)\n    \n    # Group by priority\n    high_priority = [r for r in optimization_recommendations if r['priority'] == 'HIGH']\n    medium_priority = [r for r in optimization_recommendations if r['priority'] == 'MEDIUM']\n    \n    if high_priority:\n        print(f\"\\n   üî¥ HIGH PRIORITY:\")\n        for rec in high_priority:\n            print(f\"      {rec['category']}: {rec['recommendation']}\")\n            print(f\"         Impact: {rec['impact']}\")\n    \n    if medium_priority:\n        print(f\"\\n   üü° MEDIUM PRIORITY:\")\n        for rec in medium_priority:\n            print(f\"      {rec['category']}: {rec['recommendation']}\")\n            print(f\"         Impact: {rec['impact']}\")\n    \n    # Specific SQL semantics optimization\n    print(f\"\\nüîç SQL SEMANTICS OPTIMIZATION:\")\n    print(\"   Specific recommendations for SQL semantics queries:\")\n    print(\"   \"\n          \"1. üìù Pre-filter operations with SQL semantics before complex traversals\")\n    print(\"   2. üéØ Use property existence checks (CONTAINS) before JSON parsing\")\n    print(\"   3. üîÑ Batch process SQL semantics extraction for better performance\")\n    print(\"   4. üíæ Consider materializing frequently accessed SQL pattern summaries\")\n    print(\"   5. üìä Use aggregation queries instead of client-side processing\")\n    \n    # Migration-specific query patterns\n    print(f\"\\nüöÄ MIGRATION QUERY BEST PRACTICES:\")\n    print(\"   Optimized patterns for migration analysis:\")\n    print(\"   1. Use directed relationship traversals: MATCH (a)-[:SPECIFIC_TYPE]->(b)\")\n    print(\"   2. Limit traversal depth: MATCH (a)-[*1..3]-(b)\")\n    print(\"   3. Filter early: WHERE conditions before relationship traversals\")\n    print(\"   4. Use COLLECT for aggregating related data\")\n    print(\"   5. PROFILE queries during development to identify bottlenecks\")\nelse:\n    print(\"‚ùå No database statistics available for optimization analysis\")\n\n# Memory and resource usage insights\nprint(f\"\\nüíæ RESOURCE USAGE INSIGHTS:\")\nprint(\"=\" * 50)\n\n# Estimate memory usage patterns\nif not stats_result.empty:\n    # Rough memory estimation (this is approximate)\n    estimated_memory_per_node = 1024  # bytes (very rough estimate)\n    total_estimated_memory = total_nodes * estimated_memory_per_node\n    \n    print(f\"   üìä Estimated resource usage:\")\n    print(f\"      ‚Ä¢ Nodes in memory: ~{total_nodes:,}\")\n    print(f\"      ‚Ä¢ Estimated memory: ~{total_estimated_memory / (1024*1024):.1f} MB\")\n    \n    # Resource usage recommendations\n    if total_nodes > 10000:\n        print(f\"   ‚ö†Ô∏è  Large dataset detected - consider:\")\n        print(f\"      ‚Ä¢ Implementing query result pagination\")\n        print(f\"      ‚Ä¢ Using streaming results for large analyses\")\n        print(f\"      ‚Ä¢ Setting up query timeout limits\")\n    elif total_nodes > 50000:\n        print(f\"   üî¥ Very large dataset - critical optimizations needed:\")\n        print(f\"      ‚Ä¢ Implement database sharding by package or domain\")\n        print(f\"      ‚Ä¢ Use read replicas for analysis queries\")\n        print(f\"      ‚Ä¢ Consider data archiving strategies\")\n    else:\n        print(f\"   ‚úÖ Dataset size is manageable for current operations\")\n\nprint(f\"\\nüéØ NEXT STEPS FOR OPTIMIZATION:\")\nprint(\"1. Implement high-priority recommendations first\")\nprint(\"2. Monitor query performance with PROFILE commands\")\nprint(\"3. Set up performance benchmarks for migration queries\")\nprint(\"4. Consider database tuning based on workload patterns\")\nprint(\"5. Plan for scalability as SSIS portfolio grows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-world-scenarios",
   "metadata": {},
   "source": [
    "## 5. Real-World Migration Scenarios\n",
    "\n",
    "Apply advanced querying techniques to solve realistic migration challenges and planning scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenario-1-risk-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Risk Assessment for Large-Scale Migration\n",
    "print(\"üéØ SCENARIO 1: LARGE-SCALE MIGRATION RISK ASSESSMENT:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Context: Planning migration of 50+ SSIS packages with tight deadlines\")\n\n# Comprehensive risk analysis query\nrisk_assessment_query = \"\"\"\n    MATCH (pkg:Node)\n    WHERE pkg.node_type = 'pipeline'\n    OPTIONAL MATCH (pkg)-[:CONTAINS]->(op:Node)\n    WHERE op.node_type = 'operation'\n    WITH pkg, \n         count(op) as total_operations,\n         sum(CASE WHEN op.properties CONTAINS 'sql_semantics' THEN 1 ELSE 0 END) as operations_with_sql,\n         collect(op.properties.operation_type) as operation_types,\n         collect(CASE WHEN op.properties CONTAINS 'sql_semantics' THEN op.properties.sql_semantics ELSE null END) as sql_list\n    OPTIONAL MATCH (pkg)-[:CONTAINS*]->(asset:Node)\n    WHERE asset.node_type = 'data_asset'\n    WITH pkg, total_operations, operations_with_sql, operation_types, sql_list,\n         count(DISTINCT asset) as data_assets\n    OPTIONAL MATCH (pkg)-[:CONTAINS*]->(conn:Node)\n    WHERE conn.node_type = 'connection'\n    RETURN \n        pkg.name as package_name,\n        total_operations,\n        operations_with_sql,\n        operation_types,\n        sql_list,\n        data_assets,\n        count(DISTINCT conn) as connections\n\"\"\"\n\nrisk_data = execute_query(risk_assessment_query, show_timing=True)\n\nif not risk_data.empty:\n    # Calculate comprehensive risk scores\n    risk_analysis = []\n    \n    for idx, row in risk_data.iterrows():\n        risk_factors = {\n            'package_name': row['package_name'],\n            'complexity_risk': 0,\n            'dependency_risk': 0,\n            'technical_risk': 0,\n            'resource_risk': 0,\n            'timeline_risk': 0\n        }\n        \n        # Complexity risk (based on operations and diversity)\n        ops_count = row['total_operations'] or 0\n        if ops_count > 20:\n            risk_factors['complexity_risk'] += 30\n        elif ops_count > 10:\n            risk_factors['complexity_risk'] += 15\n        \n        # Operation type diversity increases complexity\n        unique_op_types = len(set(row['operation_types'] or []))\n        if unique_op_types > 8:\n            risk_factors['complexity_risk'] += 20\n        elif unique_op_types > 5:\n            risk_factors['complexity_risk'] += 10\n        \n        # Technical risk (SQL semantics coverage)\n        sql_coverage = (row['operations_with_sql'] or 0) / max(ops_count, 1)\n        if sql_coverage < 0.3:\n            risk_factors['technical_risk'] += 40  # High risk - poor automation\n        elif sql_coverage < 0.7:\n            risk_factors['technical_risk'] += 20\n        else:\n            risk_factors['technical_risk'] -= 10  # Lower risk - good automation\n        \n        # Dependency risk (many data assets = complex dependencies)\n        assets = row['data_assets'] or 0\n        if assets > 15:\n            risk_factors['dependency_risk'] += 25\n        elif assets > 8:\n            risk_factors['dependency_risk'] += 15\n        \n        # Connection complexity\n        conns = row['connections'] or 0\n        if conns > 5:\n            risk_factors['resource_risk'] += 15\n        \n        # SQL complexity analysis\n        complex_sql_count = 0\n        total_joins = 0\n        \n        for sql_raw in (row['sql_list'] or []):\n            if sql_raw:\n                try:\n                    sql_data = json.loads(sql_raw) if isinstance(sql_raw, str) else sql_raw\n                    joins = sql_data.get('joins', [])\n                    total_joins += len(joins)\n                    \n                    if len(joins) > 3:\n                        complex_sql_count += 1\n                except (json.JSONDecodeError, TypeError):\n                    continue\n        \n        if complex_sql_count > 0:\n            risk_factors['technical_risk'] += complex_sql_count * 10\n        \n        # Timeline risk (based on total complexity)\n        estimated_days = (\n            ops_count * 0.5 +  # Operations\n            unique_op_types * 0.8 +  # Diversity penalty\n            assets * 0.3 +  # Data assets\n            complex_sql_count * 2  # Complex SQL\n        )\n        \n        if estimated_days > 15:  # More than 3 weeks\n            risk_factors['timeline_risk'] += 30\n        elif estimated_days > 8:\n            risk_factors['timeline_risk'] += 15\n        \n        # Calculate overall risk score\n        total_risk = sum(risk_factors[k] for k in risk_factors if k != 'package_name')\n        \n        risk_factors.update({\n            'total_risk_score': total_risk,\n            'estimated_days': estimated_days,\n            'sql_coverage': sql_coverage * 100,\n            'complex_sql_operations': complex_sql_count\n        })\n        \n        risk_analysis.append(risk_factors)\n    \n    # Convert to DataFrame and analyze\n    risk_df = pd.DataFrame(risk_analysis)\n    risk_df = risk_df.sort_values('total_risk_score', ascending=False)\n    \n    # Add risk categories\n    def get_risk_category(score):\n        if score >= 80: return \"üî¥ Critical\"\n        elif score >= 60: return \"üü† High\"\n        elif score >= 40: return \"üü° Medium\"\n        elif score >= 20: return \"üü¢ Low\"\n        else: return \"‚úÖ Minimal\"\n    \n    risk_df['risk_category'] = risk_df['total_risk_score'].apply(get_risk_category)\n    \n    print(f\"üìä RISK ASSESSMENT RESULTS ({len(risk_df)} packages analyzed):\")\n    display_cols = ['package_name', 'risk_category', 'total_risk_score', 'estimated_days', \n                   'sql_coverage', 'complex_sql_operations']\n    display(risk_df[display_cols].head(15))\n    \n    # Risk distribution analysis\n    print(f\"\\nüìà RISK DISTRIBUTION:\")\n    risk_distribution = risk_df['risk_category'].value_counts()\n    for category, count in risk_distribution.items():\n        percentage = (count / len(risk_df)) * 100\n        print(f\"   {category}: {count} packages ({percentage:.1f}%)\")\n    \n    # Critical packages requiring immediate attention\n    critical_packages = risk_df[risk_df['total_risk_score'] >= 80]\n    high_risk_packages = risk_df[(risk_df['total_risk_score'] >= 60) & (risk_df['total_risk_score'] < 80)]\n    \n    if not critical_packages.empty:\n        print(f\"\\nüö® CRITICAL RISK PACKAGES ({len(critical_packages)} packages):\")\n        for idx, pkg in critical_packages.head(5).iterrows():\n            print(f\"   ‚Ä¢ {pkg['package_name']} (Score: {pkg['total_risk_score']:.0f})\")\n            print(f\"     Est. effort: {pkg['estimated_days']:.1f} days, SQL coverage: {pkg['sql_coverage']:.0f}%\")\n            \n            # Specific risk breakdown\n            top_risks = []\n            if pkg['complexity_risk'] > 20: top_risks.append(\"High complexity\")\n            if pkg['technical_risk'] > 20: top_risks.append(\"Technical challenges\")\n            if pkg['dependency_risk'] > 15: top_risks.append(\"Complex dependencies\")\n            if pkg['timeline_risk'] > 15: top_risks.append(\"Timeline pressure\")\n            \n            if top_risks:\n                print(f\"     Key risks: {', '.join(top_risks)}\")\n    \n    # Migration strategy recommendations\n    print(f\"\\nüéØ MIGRATION STRATEGY RECOMMENDATIONS:\")\n    print(\"=\" * 50)\n    \n    total_estimated_days = risk_df['estimated_days'].sum()\n    critical_days = critical_packages['estimated_days'].sum() if not critical_packages.empty else 0\n    high_risk_days = high_risk_packages['estimated_days'].sum() if not high_risk_packages.empty else 0\n    \n    print(f\"üìä EFFORT ESTIMATION:\")\n    print(f\"   ‚Ä¢ Total estimated effort: {total_estimated_days:.0f} person-days\")\n    print(f\"   ‚Ä¢ Critical packages: {critical_days:.0f} days ({len(critical_packages)} packages)\")\n    print(f\"   ‚Ä¢ High-risk packages: {high_risk_days:.0f} days ({len(high_risk_packages)} packages)\")\n    print(f\"   ‚Ä¢ Team size needed (6-month timeline): {total_estimated_days / 120:.1f} FTE\")\n    \n    print(f\"\\nüöÄ RECOMMENDED APPROACH:\")\n    \n    if not critical_packages.empty:\n        print(f\"   Phase 1 - Critical Risk Mitigation:\")\n        print(f\"      ‚Ä¢ Focus on {len(critical_packages)} critical packages first\")\n        print(f\"      ‚Ä¢ Assign senior developers and architects\")\n        print(f\"      ‚Ä¢ Implement enhanced SQL parsing for low-coverage packages\")\n        print(f\"      ‚Ä¢ Estimated duration: {critical_days / 3:.0f} weeks (3-person team)\")\n    \n    if not high_risk_packages.empty:\n        print(f\"   Phase 2 - High Risk Management:\")\n        print(f\"      ‚Ä¢ Address {len(high_risk_packages)} high-risk packages\")\n        print(f\"      ‚Ä¢ Use learnings from Phase 1\")\n        print(f\"      ‚Ä¢ Implement parallel migration tracks\")\n        print(f\"      ‚Ä¢ Estimated duration: {high_risk_days / 4:.0f} weeks (4-person team)\")\n    \n    medium_low_packages = risk_df[risk_df['total_risk_score'] < 60]\n    if not medium_low_packages.empty:\n        print(f\"   Phase 3 - Bulk Migration:\")\n        print(f\"      ‚Ä¢ Migrate remaining {len(medium_low_packages)} packages\")\n        print(f\"      ‚Ä¢ Use automated tools and code generation\")\n        print(f\"      ‚Ä¢ Parallel execution with multiple teams\")\n        \n    # Risk mitigation strategies\n    print(f\"\\n‚ö†Ô∏è  RISK MITIGATION STRATEGIES:\")\n    \n    low_sql_coverage = risk_df[risk_df['sql_coverage'] < 50]\n    if not low_sql_coverage.empty:\n        print(f\"   üìù SQL Coverage Issues ({len(low_sql_coverage)} packages):\")\n        print(f\"      ‚Ä¢ Enhance SSIS parser for better SQL extraction\")\n        print(f\"      ‚Ä¢ Manual SQL analysis for critical operations\")\n        print(f\"      ‚Ä¢ Build test cases before migration\")\n    \n    complex_packages = risk_df[risk_df['complex_sql_operations'] > 2]\n    if not complex_packages.empty:\n        print(f\"   üîó Complex SQL Operations ({len(complex_packages)} packages):\")\n        print(f\"      ‚Ä¢ SQL expert review required\")\n        print(f\"      ‚Ä¢ Consider query optimization opportunities\")\n        print(f\"      ‚Ä¢ Plan for extended testing phase\")\n    \n    large_packages = risk_df[risk_df['estimated_days'] > 10]\n    if not large_packages.empty:\n        print(f\"   üì¶ Large Package Complexity ({len(large_packages)} packages):\")\n        print(f\"      ‚Ä¢ Consider package decomposition\")\n        print(f\"      ‚Ä¢ Implement incremental migration approach\")\n        print(f\"      ‚Ä¢ Plan for extended UAT period\")\n    \n    # Visualization\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # Risk score distribution\n    ax1.hist(risk_df['total_risk_score'], bins=10, alpha=0.7, edgecolor='black')\n    ax1.set_title('Risk Score Distribution')\n    ax1.set_xlabel('Total Risk Score')\n    ax1.set_ylabel('Number of Packages')\n    ax1.axvline(risk_df['total_risk_score'].mean(), color='red', linestyle='--', label='Average')\n    ax1.legend()\n    \n    # Risk category pie chart\n    risk_counts = risk_df['risk_category'].value_counts()\n    colors = ['red' if 'Critical' in cat else 'orange' if 'High' in cat else 'yellow' if 'Medium' in cat else 'green' for cat in risk_counts.index]\n    ax2.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)\n    ax2.set_title('Risk Category Distribution')\n    \n    # Effort vs Risk scatter\n    scatter = ax3.scatter(risk_df['estimated_days'], risk_df['total_risk_score'], \n                         alpha=0.6, s=60)\n    ax3.set_xlabel('Estimated Days')\n    ax3.set_ylabel('Total Risk Score')\n    ax3.set_title('Effort vs Risk Analysis')\n    \n    # SQL Coverage vs Risk\n    ax4.scatter(risk_df['sql_coverage'], risk_df['total_risk_score'], alpha=0.6, s=60)\n    ax4.set_xlabel('SQL Coverage (%)')\n    ax4.set_ylabel('Total Risk Score')\n    ax4.set_title('SQL Coverage vs Risk')\n    \n    plt.tight_layout()\n    plt.show()\n\nelse:\n    print(\"‚ùå No package data available for risk assessment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenario-2-platform-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Multi-Platform Migration Strategy\n",
    "print(f\"\\nüéØ SCENARIO 2: MULTI-PLATFORM MIGRATION STRATEGY:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Context: Organization wants to migrate to different platforms based on use case\")\n\n# Platform assignment logic based on package characteristics\nif not risk_df.empty:\n    platform_recommendations = []\n    \n    for idx, row in risk_df.iterrows():\n        pkg_name = row['package_name']\n        \n        # Get original package data for detailed analysis\n        pkg_data = risk_data[risk_data['package_name'] == pkg_name].iloc[0]\n        \n        platform_scores = {\n            'Spark/Databricks': 0,\n            'dbt/Snowflake': 0,\n            'Azure Data Factory': 0,\n            'AWS Glue': 0,\n            'Custom Python': 0\n        }\n        \n        # Scoring logic based on package characteristics\n        ops_count = pkg_data['total_operations'] or 0\n        assets_count = pkg_data['data_assets'] or 0\n        sql_coverage = (pkg_data['operations_with_sql'] or 0) / max(ops_count, 1)\n        \n        # Analyze SQL complexity from the package data\n        complex_joins = 0\n        total_tables = 0\n        \n        for sql_raw in (pkg_data['sql_list'] or []):\n            if sql_raw:\n                try:\n                    sql_data = json.loads(sql_raw) if isinstance(sql_raw, str) else sql_raw\n                    joins = sql_data.get('joins', [])\n                    tables = sql_data.get('tables', [])\n                    \n                    if len(joins) > 2:\n                        complex_joins += 1\n                    total_tables += len(tables)\n                except (json.JSONDecodeError, TypeError):\n                    continue\n        \n        # Platform scoring\n        \n        # Spark/Databricks - good for complex data processing\n        if complex_joins > 0:\n            platform_scores['Spark/Databricks'] += 30\n        if assets_count > 5:\n            platform_scores['Spark/Databricks'] += 25\n        if ops_count > 10:\n            platform_scores['Spark/Databricks'] += 20\n        if sql_coverage > 0.7:\n            platform_scores['Spark/Databricks'] += 15\n        \n        # dbt/Snowflake - good for analytics and SQL-heavy workloads\n        if sql_coverage > 0.8:\n            platform_scores['dbt/Snowflake'] += 35\n        if complex_joins > 0:\n            platform_scores['dbt/Snowflake'] += 25\n        if total_tables > 3:\n            platform_scores['dbt/Snowflake'] += 20\n        if assets_count <= 8:  # Not too many assets\n            platform_scores['dbt/Snowflake'] += 10\n        \n        # Azure Data Factory - good for orchestration and simple ETL\n        if sql_coverage < 0.5:  # Less SQL-dependent\n            platform_scores['Azure Data Factory'] += 30\n        if ops_count <= 8:  # Simpler packages\n            platform_scores['Azure Data Factory'] += 25\n        if complex_joins == 0:  # No complex joins\n            platform_scores['Azure Data Factory'] += 20\n        \n        # AWS Glue - similar to ADF but with better Spark integration\n        if sql_coverage < 0.6:\n            platform_scores['AWS Glue'] += 25\n        if ops_count > 5 and ops_count <= 15:\n            platform_scores['AWS Glue'] += 20\n        if assets_count > 3:\n            platform_scores['AWS Glue'] += 15\n        \n        # Custom Python - for very complex or unique cases\n        if row['total_risk_score'] > 80:  # High complexity\n            platform_scores['Custom Python'] += 40\n        if sql_coverage < 0.3:  # Poor automation potential\n            platform_scores['Custom Python'] += 30\n        if complex_joins > 3:  # Very complex SQL\n            platform_scores['Custom Python'] += 25\n        \n        # Determine best platform\n        best_platform = max(platform_scores, key=platform_scores.get)\n        best_score = platform_scores[best_platform]\n        \n        # Get second best for comparison\n        sorted_platforms = sorted(platform_scores.items(), key=lambda x: x[1], reverse=True)\n        second_best = sorted_platforms[1][0] if len(sorted_platforms) > 1 else \"None\"\n        second_score = sorted_platforms[1][1] if len(sorted_platforms) > 1 else 0\n        \n        platform_recommendations.append({\n            'package_name': pkg_name,\n            'primary_platform': best_platform,\n            'primary_score': best_score,\n            'secondary_platform': second_best,\n            'secondary_score': second_score,\n            'confidence': 'High' if best_score > second_score + 20 else 'Medium' if best_score > second_score + 10 else 'Low',\n            'risk_score': row['total_risk_score'],\n            'estimated_days': row['estimated_days'],\n            'sql_coverage': sql_coverage * 100\n        })\n    \n    platform_df = pd.DataFrame(platform_recommendations)\n    platform_df = platform_df.sort_values('primary_score', ascending=False)\n    \n    print(f\"üìä PLATFORM ASSIGNMENT RESULTS:\")\n    display_cols = ['package_name', 'primary_platform', 'confidence', 'primary_score', \n                   'secondary_platform', 'risk_score']\n    display(platform_df[display_cols].head(15))\n    \n    # Platform distribution analysis\n    print(f\"\\nüìà PLATFORM DISTRIBUTION:\")\n    platform_distribution = platform_df['primary_platform'].value_counts()\n    \n    for platform, count in platform_distribution.items():\n        percentage = (count / len(platform_df)) * 100\n        avg_confidence = platform_df[platform_df['primary_platform'] == platform]['confidence'].value_counts()\n        high_confidence = avg_confidence.get('High', 0)\n        confidence_rate = (high_confidence / count) * 100 if count > 0 else 0\n        \n        print(f\"   ‚Ä¢ {platform}: {count} packages ({percentage:.1f}%)\")\n        print(f\"     High confidence assignments: {confidence_rate:.0f}%\")\n    \n    # Migration effort by platform\n    print(f\"\\n‚è±Ô∏è  MIGRATION EFFORT BY PLATFORM:\")\n    platform_effort = platform_df.groupby('primary_platform').agg({\n        'estimated_days': ['sum', 'mean'],\n        'risk_score': 'mean',\n        'package_name': 'count'\n    }).round(1)\n    \n    platform_effort.columns = ['total_days', 'avg_days_per_pkg', 'avg_risk_score', 'package_count']\n    platform_effort = platform_effort.sort_values('total_days', ascending=False)\n    \n    display(platform_effort)\n    \n    # Strategic recommendations\n    print(f\"\\nüéØ MULTI-PLATFORM STRATEGY RECOMMENDATIONS:\")\n    print(\"=\" * 60)\n    \n    # Team allocation recommendations\n    total_effort = platform_df['estimated_days'].sum()\n    \n    for platform, data in platform_effort.iterrows():\n        effort_percentage = (data['total_days'] / total_effort) * 100\n        team_size = max(1, round(data['total_days'] / 60))  # Assuming 60 working days per person\n        \n        print(f\"\\n   üìã {platform}:\")\n        print(f\"      ‚Ä¢ Packages: {int(data['package_count'])}\")\n        print(f\"      ‚Ä¢ Total effort: {data['total_days']:.0f} days ({effort_percentage:.1f}% of total)\")\n        print(f\"      ‚Ä¢ Average risk: {data['avg_risk_score']:.0f} (out of 100)\")\n        print(f\"      ‚Ä¢ Recommended team size: {team_size} developers\")\n        \n        # Platform-specific recommendations\n        if platform == 'Spark/Databricks':\n            print(f\"      ‚Ä¢ Skills needed: PySpark, SQL, Scala (optional)\")\n            print(f\"      ‚Ä¢ Focus: Complex data transformations and large datasets\")\n        elif platform == 'dbt/Snowflake':\n            print(f\"      ‚Ä¢ Skills needed: SQL, dbt, data modeling\")\n            print(f\"      ‚Ä¢ Focus: Analytics workloads and data warehousing\")\n        elif platform == 'Azure Data Factory':\n            print(f\"      ‚Ä¢ Skills needed: ADF, Azure services, basic SQL\")\n            print(f\"      ‚Ä¢ Focus: Data movement and simple transformations\")\n        elif platform == 'AWS Glue':\n            print(f\"      ‚Ä¢ Skills needed: AWS Glue, Python/Scala, AWS services\")\n            print(f\"      ‚Ä¢ Focus: Serverless ETL and data cataloging\")\n        elif platform == 'Custom Python':\n            print(f\"      ‚Ä¢ Skills needed: Python, pandas, advanced SQL\")\n            print(f\"      ‚Ä¢ Focus: Complex custom logic and edge cases\")\n    \n    # Migration timeline recommendations\n    print(f\"\\nüìÖ MIGRATION TIMELINE STRATEGY:\")\n    \n    # Sort platforms by average risk (start with lower risk)\n    risk_sorted_platforms = platform_effort.sort_values('avg_risk_score')\n    \n    print(f\"   Recommended migration order (by platform risk):\")\n    \n    for i, (platform, data) in enumerate(risk_sorted_platforms.iterrows(), 1):\n        timeline_months = data['total_days'] / 22  # Assuming 22 working days per month\n        \n        print(f\"   {i}. {platform} ({timeline_months:.1f} months)\")\n        print(f\"      Rationale: Risk {data['avg_risk_score']:.0f}/100, {int(data['package_count'])} packages\")\n        \n        if i == 1:\n            print(f\"      üü¢ Start here: Lower risk, establish patterns\")\n        elif i == len(risk_sorted_platforms):\n            print(f\"      üî¥ Final phase: Apply lessons learned\")\n        else:\n            print(f\"      üü° Middle phase: Scale successful approaches\")\n    \n    # Visualization\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # Platform distribution\n    platform_counts = platform_df['primary_platform'].value_counts()\n    ax1.pie(platform_counts.values, labels=platform_counts.index, autopct='%1.1f%%', startangle=90)\n    ax1.set_title('Platform Assignment Distribution')\n    \n    # Effort by platform\n    ax2.bar(platform_effort.index, platform_effort['total_days'])\n    ax2.set_title('Migration Effort by Platform')\n    ax2.set_xlabel('Platform')\n    ax2.set_ylabel('Total Days')\n    ax2.tick_params(axis='x', rotation=45)\n    \n    # Risk vs Platform scatter\n    platforms = platform_df['primary_platform'].unique()\n    colors = plt.cm.Set3(np.linspace(0, 1, len(platforms)))\n    \n    for i, platform in enumerate(platforms):\n        platform_data = platform_df[platform_df['primary_platform'] == platform]\n        ax3.scatter(platform_data['risk_score'], platform_data['primary_score'], \n                   label=platform, alpha=0.7, c=[colors[i]])\n    \n    ax3.set_xlabel('Risk Score')\n    ax3.set_ylabel('Platform Score')\n    ax3.set_title('Risk vs Platform Score')\n    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    \n    # Confidence distribution\n    confidence_counts = platform_df['confidence'].value_counts()\n    ax4.bar(confidence_counts.index, confidence_counts.values)\n    ax4.set_title('Assignment Confidence Distribution')\n    ax4.set_xlabel('Confidence Level')\n    ax4.set_ylabel('Number of Packages')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nüí° KEY STRATEGIC INSIGHTS:\")\n    print(f\"   1. {platform_distribution.iloc[0]} packages best suited for {platform_distribution.index[0]}\")\n    print(f\"   2. Total migration effort: {total_effort:.0f} person-days (~{total_effort/22:.1f} person-months)\")\n    print(f\"   3. Recommended team size: {max(3, round(total_effort/120))} developers across all platforms\")\n    print(f\"   4. Expected timeline: {total_effort/(22*3):.1f} months with 3-person team\")\n    \n    high_confidence = len(platform_df[platform_df['confidence'] == 'High'])\n    print(f\"   5. High-confidence assignments: {high_confidence}/{len(platform_df)} ({100*high_confidence/len(platform_df):.0f}%)\")\n\nelse:\n    print(\"‚ùå No risk assessment data available for platform recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated advanced graph query patterns and analysis techniques for sophisticated SSIS migration planning:\n",
    "\n",
    "### Key Capabilities Demonstrated:\n",
    "1. **Multi-Hop Relationship Traversals** - Complex data lineage analysis across multiple degrees of separation\n",
    "2. **Advanced Pattern Matching** - Anti-pattern detection and migration scenario identification\n",
    "3. **Graph Algorithms** - Critical path analysis, centrality metrics, and clustering for optimization\n",
    "4. **Performance Analysis** - Query optimization and resource management strategies\n",
    "5. **Real-World Scenarios** - Comprehensive risk assessment and multi-platform migration planning\n",
    "\n",
    "### Advanced Analysis Insights:\n",
    "- **End-to-End Lineage Mapping** - Complete data flow understanding across packages\n",
    "- **Migration Anti-Pattern Detection** - Identification of Cartesian products, complex JOINs, and inefficient patterns\n",
    "- **Critical Path Identification** - Migration sequencing based on dependency analysis\n",
    "- **Resource Contention Analysis** - Coordination requirements for shared assets\n",
    "- **Platform-Specific Optimization** - Intelligent matching of packages to target platforms\n",
    "\n",
    "### Business Value for Migration Planning:\n",
    "- **Risk-Based Prioritization** - Quantitative risk assessment enables optimal resource allocation\n",
    "- **Platform Selection Automation** - Data-driven recommendations for target platform selection\n",
    "- **Timeline Optimization** - Dependency-aware sequencing minimizes migration duration\n",
    "- **Resource Planning** - Accurate effort estimation and team size recommendations\n",
    "- **Quality Assurance** - Anti-pattern detection prevents migration issues\n",
    "\n",
    "### Performance and Scalability:\n",
    "- **Query Optimization Strategies** - Performance tuning for large-scale analysis\n",
    "- **Memory Management** - Resource-conscious approaches for enterprise datasets\n",
    "- **Scalable Analysis Patterns** - Techniques that work across growing SSIS portfolios\n",
    "\n",
    "### Next Steps:\n",
    "- Apply risk assessment methodology to prioritize migration waves\n",
    "- Implement platform-specific migration tracks based on recommendations\n",
    "- Use clustering analysis for team organization and parallel execution\n",
    "- Monitor migration progress using critical path metrics\n",
    "- Leverage anti-pattern detection for quality gates and review processes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}