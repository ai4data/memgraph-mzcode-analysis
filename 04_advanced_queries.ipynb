{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Advanced Queries and Analysis\n",
    "\n",
    "This notebook covers advanced Cypher queries and analysis techniques for the SSIS Northwind graph. You'll learn complex querying patterns, performance optimization, and sophisticated analysis workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection to Memgraph\n",
    "import mgclient\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Connection to Memgraph\n",
    "mg = pymgclient.connect(host='localhost', port=7687)\n",
    "\n",
    "def execute_and_fetch(query, params=None):\n",
    "    \"\"\"Execute query and return results as list of records\"\"\"\n",
    "    cursor = mg.cursor()\n",
    "    cursor.execute(query, params or {})\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def pretty_print(data, title=\"Results\"):\n",
    "    \"\"\"Pretty print query results\"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    if isinstance(data, str):\n",
    "        try:\n",
    "            parsed = json.loads(data)\n",
    "            print(json.dumps(parsed, indent=2))\n",
    "        except:\n",
    "            print(data)\n",
    "    else:\n",
    "        print(json.dumps(data, indent=2, default=str))\n",
    "    print(\"=\" * (len(title) + 8))\n",
    "\n",
    "print(\"✅ Connected to Memgraph successfully\")\n",
    "print(\"📚 Advanced querying toolkit ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Complex Path Analysis\n",
    "\n",
    "Learn how to analyze complex data flows and dependencies using path queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all data transformation paths from source to target\n",
    "data_flow_paths = \"\"\"\n",
    "MATCH path = (source:Node {node_type: 'DATA_ASSET'})-[*1..5]->(target:Node {node_type: 'DATA_ASSET'})\n",
    "WHERE source.name CONTAINS 'Orders' AND target.name CONTAINS 'Sales'\n",
    "RETURN \n",
    "    source.name as source_table,\n",
    "    target.name as target_table,\n",
    "    length(path) as path_length,\n",
    "    [n in nodes(path) | n.name] as transformation_chain\n",
    "ORDER BY path_length\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "paths = execute_and_fetch(data_flow_paths)\n",
    "pretty_print(paths, \"Data Transformation Paths\")\n",
    "\n",
    "# Analysis: Identify the most common transformation patterns\n",
    "print(\"\\n📊 Path Analysis:\")\n",
    "for path in paths:\n",
    "    print(f\"  {path[0]} → {path[1]} (via {path[2]} steps)\")\n",
    "    print(f\"    Chain: {' → '.join(path[3])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find critical nodes that appear in multiple paths (bottlenecks)\n",
    "bottleneck_analysis = \"\"\"\n",
    "MATCH (n:Node)\n",
    "WHERE n.node_type IN ['OPERATION', 'DATA_ASSET']\n",
    "WITH n, \n",
    "     size([(n)-[:READS_FROM]->() | 1]) as incoming_connections,\n",
    "     size([(n)-[:WRITES_TO]->() | 1]) as outgoing_connections\n",
    "WHERE incoming_connections > 1 OR outgoing_connections > 1\n",
    "RETURN \n",
    "    n.name as node_name,\n",
    "    n.node_type as type,\n",
    "    incoming_connections,\n",
    "    outgoing_connections,\n",
    "    (incoming_connections + outgoing_connections) as total_connections\n",
    "ORDER BY total_connections DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "bottlenecks = execute_and_fetch(bottleneck_analysis)\n",
    "pretty_print(bottlenecks, \"Potential Bottlenecks\")\n",
    "\n",
    "print(\"\\n⚠️ Bottleneck Risk Assessment:\")\n",
    "for node in bottlenecks:\n",
    "    risk_level = \"HIGH\" if node[4] > 5 else \"MEDIUM\" if node[4] > 2 else \"LOW\"\n",
    "    print(f\"  {node[0]} ({node[1]}): {node[4]} connections - {risk_level} RISK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Aggregation Queries\n",
    "\n",
    "Complex aggregations to understand system complexity and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive system complexity analysis\n",
    "complexity_analysis = \"\"\"\n",
    "MATCH (p:Node {node_type: 'PIPELINE'})\n",
    "OPTIONAL MATCH (p)-[:CONTAINS]->(op:Node {node_type: 'OPERATION'})\n",
    "OPTIONAL MATCH (op)-[:READS_FROM|WRITES_TO]->(da:Node {node_type: 'DATA_ASSET'})\n",
    "OPTIONAL MATCH (p)-[:USES_CONNECTION]->(conn:Node {node_type: 'CONNECTION'})\n",
    "WITH p,\n",
    "     count(DISTINCT op) as operation_count,\n",
    "     count(DISTINCT da) as data_asset_count,\n",
    "     count(DISTINCT conn) as connection_count,\n",
    "     collect(DISTINCT op.operation_type) as operation_types\n",
    "RETURN \n",
    "    p.name as package_name,\n",
    "    operation_count,\n",
    "    data_asset_count,\n",
    "    connection_count,\n",
    "    size(operation_types) as unique_operation_types,\n",
    "    operation_types,\n",
    "    (operation_count * 2 + data_asset_count + connection_count * 3) as complexity_score\n",
    "ORDER BY complexity_score DESC\n",
    "\"\"\"\n",
    "\n",
    "complexity = execute_and_fetch(complexity_analysis)\n",
    "pretty_print(complexity, \"Package Complexity Analysis\")\n",
    "\n",
    "# Create complexity distribution\n",
    "print(\"\\n📈 Complexity Distribution:\")\n",
    "total_packages = len(complexity)\n",
    "high_complexity = sum(1 for pkg in complexity if pkg[6] > 20)\n",
    "medium_complexity = sum(1 for pkg in complexity if 10 <= pkg[6] <= 20)\n",
    "low_complexity = total_packages - high_complexity - medium_complexity\n",
    "\n",
    "print(f\"  High Complexity (>20): {high_complexity} packages ({high_complexity/total_packages*100:.1f}%)\")\n",
    "print(f\"  Medium Complexity (10-20): {medium_complexity} packages ({medium_complexity/total_packages*100:.1f}%)\")\n",
    "print(f\"  Low Complexity (<10): {low_complexity} packages ({low_complexity/total_packages*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data lineage impact analysis\n",
    "lineage_impact = \"\"\"\n",
    "MATCH (source:Node {node_type: 'DATA_ASSET'})\n",
    "WHERE source.name CONTAINS 'Customer'\n",
    "MATCH path = (source)-[*1..4]->(downstream:Node)\n",
    "WHERE downstream.node_type IN ['DATA_ASSET', 'OPERATION']\n",
    "WITH source, downstream, length(path) as distance\n",
    "RETURN \n",
    "    source.name as source_asset,\n",
    "    count(DISTINCT downstream) as affected_nodes,\n",
    "    collect(DISTINCT downstream.name)[0..5] as sample_affected,\n",
    "    avg(distance) as avg_distance,\n",
    "    max(distance) as max_distance\n",
    "ORDER BY affected_nodes DESC\n",
    "\"\"\"\n",
    "\n",
    "lineage = execute_and_fetch(lineage_impact)\n",
    "pretty_print(lineage, \"Data Lineage Impact Analysis\")\n",
    "\n",
    "print(\"\\n🔄 Impact Assessment:\")\n",
    "for item in lineage:\n",
    "    impact_level = \"CRITICAL\" if item[1] > 10 else \"HIGH\" if item[1] > 5 else \"MODERATE\"\n",
    "    print(f\"  {item[0]}: affects {item[1]} nodes - {impact_level} IMPACT\")\n",
    "    print(f\"    Sample affected: {', '.join(item[2])}\")\n",
    "    print(f\"    Propagation depth: avg {item[3]:.1f}, max {item[4]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pattern Matching and Graph Analytics\n",
    "\n",
    "Advanced pattern matching to identify architectural patterns and anti-patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify ETL patterns (Extract-Transform-Load chains)\n",
    "etl_patterns = \"\"\"\n",
    "MATCH (extract:Node {node_type: 'OPERATION'})-[:READS_FROM]->(source:Node {node_type: 'DATA_ASSET'})\n",
    "MATCH (extract)-[:WRITES_TO]->(intermediate:Node {node_type: 'DATA_ASSET'})\n",
    "MATCH (transform:Node {node_type: 'OPERATION'})-[:READS_FROM]->(intermediate)\n",
    "MATCH (transform)-[:WRITES_TO]->(target:Node {node_type: 'DATA_ASSET'})\n",
    "WHERE extract.operation_type CONTAINS 'Source' \n",
    "  AND transform.operation_type CONTAINS 'Transform'\n",
    "RETURN DISTINCT\n",
    "    source.name as source_table,\n",
    "    extract.name as extract_operation,\n",
    "    intermediate.name as intermediate_stage,\n",
    "    transform.name as transform_operation,\n",
    "    target.name as target_table\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "etl_chains = execute_and_fetch(etl_patterns)\n",
    "pretty_print(etl_chains, \"ETL Pattern Analysis\")\n",
    "\n",
    "print(\"\\n🔧 ETL Chain Analysis:\")\n",
    "for chain in etl_chains:\n",
    "    print(f\"  {chain[0]} ➜ [{chain[1]}] ➜ {chain[2]} ➜ [{chain[3]}] ➜ {chain[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anti-patterns: operations that read from many sources (potential complexity issues)\n",
    "anti_patterns = \"\"\"\n",
    "MATCH (op:Node {node_type: 'OPERATION'})-[:READS_FROM]->(source:Node {node_type: 'DATA_ASSET'})\n",
    "WITH op, collect(DISTINCT source.name) as sources\n",
    "WHERE size(sources) >= 3\n",
    "MATCH (op)<-[:CONTAINS]-(pkg:Node {node_type: 'PIPELINE'})\n",
    "RETURN \n",
    "    pkg.name as package_name,\n",
    "    op.name as operation_name,\n",
    "    op.operation_type as operation_type,\n",
    "    size(sources) as source_count,\n",
    "    sources\n",
    "ORDER BY source_count DESC\n",
    "\"\"\"\n",
    "\n",
    "anti_pattern_results = execute_and_fetch(anti_patterns)\n",
    "pretty_print(anti_pattern_results, \"Potential Anti-Patterns\")\n",
    "\n",
    "print(\"\\n⚠️ Anti-Pattern Analysis:\")\n",
    "for pattern in anti_pattern_results:\n",
    "    risk_level = \"HIGH\" if pattern[3] > 5 else \"MEDIUM\"\n",
    "    print(f\"  {pattern[1]} in {pattern[0]}: reads from {pattern[3]} sources - {risk_level} COMPLEXITY\")\n",
    "    print(f\"    Sources: {', '.join(pattern[4][:3])}{'...' if len(pattern[4]) > 3 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Optimization Queries\n",
    "\n",
    "Queries designed for performance and using materialized views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance: raw query vs materialized view\n",
    "import time\n",
    "\n",
    "# Raw query approach\n",
    "raw_query = \"\"\"\n",
    "MATCH (p:Node {node_type: 'PIPELINE'})\n",
    "MATCH (p)-[:CONTAINS]->(op:Node {node_type: 'OPERATION'})\n",
    "WHERE op.operation_type CONTAINS 'SQL' OR op.sql_command IS NOT NULL\n",
    "MATCH (op)-[:READS_FROM|WRITES_TO]->(table:Node {node_type: 'DATA_ASSET'})\n",
    "RETURN \n",
    "    op.node_id as operation_id,\n",
    "    op.name as operation_name,\n",
    "    CASE WHEN op.sql_command IS NOT NULL THEN 'SQL_TASK' ELSE 'DATA_FLOW' END as sql_type,\n",
    "    collect(DISTINCT table.name) as affected_tables\n",
    "\"\"\"\n",
    "\n",
    "# Materialized view approach\n",
    "materialized_query = \"\"\"\n",
    "MATCH (v:Node {id: 'view:sql_operations_catalog'})\n",
    "RETURN JSON_EXTRACT(v.properties, '$.data') as operations\n",
    "\"\"\"\n",
    "\n",
    "# Performance test\n",
    "print(\"🏁 Performance Comparison:\")\n",
    "\n",
    "# Test raw query\n",
    "start_time = time.time()\n",
    "raw_results = execute_and_fetch(raw_query)\n",
    "raw_time = (time.time() - start_time) * 1000\n",
    "\n",
    "# Test materialized view\n",
    "start_time = time.time()\n",
    "materialized_results = execute_and_fetch(materialized_query)\n",
    "materialized_time = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"  Raw Query: {raw_time:.2f}ms ({len(raw_results)} results)\")\n",
    "print(f\"  Materialized View: {materialized_time:.2f}ms\")\n",
    "print(f\"  Speed Improvement: {raw_time/materialized_time:.1f}x faster\")\n",
    "\n",
    "# Show materialized view data\n",
    "if materialized_results and materialized_results[0][0]:\n",
    "    operations_data = json.loads(materialized_results[0][0])\n",
    "    print(f\"  Materialized View Records: {len(operations_data)}\")\n",
    "    \n",
    "    # Show sample from materialized view\n",
    "    print(\"\\n📋 Sample from Materialized View:\")\n",
    "    for i, op in enumerate(operations_data[:3]):\n",
    "        print(f\"  {i+1}. {op.get('operation_name', 'Unknown')} ({op.get('sql_type', 'Unknown')})\")\n",
    "        print(f\"     Tables: {', '.join(op.get('affected_tables', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient dependency analysis using materialized views\n",
    "dependency_analysis = \"\"\"\n",
    "MATCH (v:Node {id: 'view:cross_package_dependencies'})\n",
    "RETURN JSON_EXTRACT(v.properties, '$.data') as dependencies\n",
    "\"\"\"\n",
    "\n",
    "deps = execute_and_fetch(dependency_analysis)\n",
    "if deps and deps[0][0]:\n",
    "    dependencies_data = json.loads(deps[0][0])\n",
    "    \n",
    "    print(\"🔗 Cross-Package Dependencies (from Materialized View):\")\n",
    "    print(f\"  Total Dependencies: {len(dependencies_data)}\")\n",
    "    \n",
    "    # Group by risk level\n",
    "    risk_groups = {}\n",
    "    for dep in dependencies_data:\n",
    "        risk = dep.get('risk_level', 'UNKNOWN')\n",
    "        if risk not in risk_groups:\n",
    "            risk_groups[risk] = []\n",
    "        risk_groups[risk].append(dep)\n",
    "    \n",
    "    for risk_level, deps_in_level in risk_groups.items():\n",
    "        print(f\"\\n  {risk_level} Risk Dependencies: {len(deps_in_level)}\")\n",
    "        for dep in deps_in_level[:3]:  # Show first 3\n",
    "            print(f\"    {dep.get('source_package', 'Unknown')} → {dep.get('target_package', 'Unknown')}\")\n",
    "            if dep.get('shared_resources'):\n",
    "                print(f\"      Shared: {', '.join(dep['shared_resources'][:2])}\")\n",
    "        if len(deps_in_level) > 3:\n",
    "            print(f\"    ... and {len(deps_in_level) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Analytics Functions\n",
    "\n",
    "Build reusable analytics functions for common analysis patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSISAnalytics:\n",
    "    \"\"\"Custom analytics functions for SSIS graph analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, mg_connection):\n",
    "        self.mg = mg_connection\n",
    "    \n",
    "    def get_package_health_score(self, package_name=None):\n",
    "        \"\"\"Calculate health score for packages based on complexity metrics\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Node {node_type: 'PIPELINE'})\n",
    "        WHERE $package_name IS NULL OR p.name = $package_name\n",
    "        OPTIONAL MATCH (p)-[:CONTAINS]->(op:Node {node_type: 'OPERATION'})\n",
    "        OPTIONAL MATCH (op)-[:READS_FROM|WRITES_TO]->(da:Node {node_type: 'DATA_ASSET'})\n",
    "        OPTIONAL MATCH (p)-[:USES_CONNECTION]->(conn:Node {node_type: 'CONNECTION'})\n",
    "        WITH p,\n",
    "             count(DISTINCT op) as ops,\n",
    "             count(DISTINCT da) as assets,\n",
    "             count(DISTINCT conn) as conns\n",
    "        RETURN \n",
    "            p.name as package,\n",
    "            ops, assets, conns,\n",
    "            CASE \n",
    "                WHEN ops <= 5 AND assets <= 10 AND conns <= 3 THEN 'EXCELLENT'\n",
    "                WHEN ops <= 10 AND assets <= 20 AND conns <= 5 THEN 'GOOD'\n",
    "                WHEN ops <= 20 AND assets <= 40 AND conns <= 8 THEN 'FAIR'\n",
    "                ELSE 'NEEDS_ATTENTION'\n",
    "            END as health_score\n",
    "        ORDER BY ops DESC, assets DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        results = execute_and_fetch(query, {\"package_name\": package_name})\n",
    "        return results\n",
    "    \n",
    "    def find_orphaned_assets(self):\n",
    "        \"\"\"Find data assets that are not connected to any operations\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (da:Node {node_type: 'DATA_ASSET'})\n",
    "        WHERE NOT (da)<-[:READS_FROM|WRITES_TO]-()\n",
    "        RETURN da.name as orphaned_asset, da.asset_type as type\n",
    "        ORDER BY da.name\n",
    "        \"\"\"\n",
    "        \n",
    "        return execute_and_fetch(query)\n",
    "    \n",
    "    def get_migration_readiness(self):\n",
    "        \"\"\"Assess migration readiness based on complexity and dependencies\"\"\"\n",
    "        # Use materialized view for fast analysis\n",
    "        complexity_query = \"\"\"\n",
    "        MATCH (v:Node {id: 'view:complexity_metrics'})\n",
    "        RETURN JSON_EXTRACT(v.properties, '$.data') as complexity\n",
    "        \"\"\"\n",
    "        \n",
    "        deps_query = \"\"\"\n",
    "        MATCH (v:Node {id: 'view:cross_package_dependencies'})\n",
    "        RETURN JSON_EXTRACT(v.properties, '$.data') as dependencies\n",
    "        \"\"\"\n",
    "        \n",
    "        complexity_result = execute_and_fetch(complexity_query)\n",
    "        deps_result = execute_and_fetch(deps_query)\n",
    "        \n",
    "        readiness = {\n",
    "            \"overall_score\": \"CALCULATING\",\n",
    "            \"complexity_score\": 0,\n",
    "            \"dependency_score\": 0,\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        # Analyze complexity\n",
    "        if complexity_result and complexity_result[0][0]:\n",
    "            complexity_data = json.loads(complexity_result[0][0])\n",
    "            high_complexity = sum(1 for item in complexity_data if item.get('complexity_level') == 'HIGH')\n",
    "            total_packages = len(complexity_data)\n",
    "            \n",
    "            if total_packages > 0:\n",
    "                complexity_ratio = high_complexity / total_packages\n",
    "                readiness[\"complexity_score\"] = max(0, 100 - (complexity_ratio * 100))\n",
    "                \n",
    "                if complexity_ratio > 0.3:\n",
    "                    readiness[\"recommendations\"].append(\"Consider simplifying high-complexity packages before migration\")\n",
    "        \n",
    "        # Analyze dependencies\n",
    "        if deps_result and deps_result[0][0]:\n",
    "            deps_data = json.loads(deps_result[0][0])\n",
    "            high_risk_deps = sum(1 for dep in deps_data if dep.get('risk_level') == 'HIGH')\n",
    "            total_deps = len(deps_data)\n",
    "            \n",
    "            if total_deps > 0:\n",
    "                risk_ratio = high_risk_deps / total_deps\n",
    "                readiness[\"dependency_score\"] = max(0, 100 - (risk_ratio * 100))\n",
    "                \n",
    "                if risk_ratio > 0.2:\n",
    "                    readiness[\"recommendations\"].append(\"Address high-risk dependencies before migration\")\n",
    "        \n",
    "        # Overall score\n",
    "        overall = (readiness[\"complexity_score\"] + readiness[\"dependency_score\"]) / 2\n",
    "        if overall >= 80:\n",
    "            readiness[\"overall_score\"] = \"READY\"\n",
    "        elif overall >= 60:\n",
    "            readiness[\"overall_score\"] = \"MOSTLY_READY\"\n",
    "        elif overall >= 40:\n",
    "            readiness[\"overall_score\"] = \"NEEDS_WORK\"\n",
    "        else:\n",
    "            readiness[\"overall_score\"] = \"NOT_READY\"\n",
    "        \n",
    "        return readiness\n",
    "\n",
    "# Initialize analytics\n",
    "analytics = SSISAnalytics(mg)\n",
    "\n",
    "print(\"🧰 Custom Analytics Functions Ready\")\n",
    "print(\"   - get_package_health_score()\")\n",
    "print(\"   - find_orphaned_assets()\")\n",
    "print(\"   - get_migration_readiness()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom analytics functions\n",
    "\n",
    "# 1. Package Health Scores\n",
    "health_scores = analytics.get_package_health_score()\n",
    "print(\"🏥 Package Health Assessment:\")\n",
    "for package in health_scores:\n",
    "    print(f\"  {package[0]}: {package[4]} (ops: {package[1]}, assets: {package[2]}, conns: {package[3]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 2. Orphaned Assets\n",
    "orphaned = analytics.find_orphaned_assets()\n",
    "print(f\"\\n🏝️ Orphaned Assets: {len(orphaned)} found\")\n",
    "for asset in orphaned[:5]:  # Show first 5\n",
    "    print(f\"  {asset[0]} ({asset[1] if asset[1] else 'Unknown type'})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 3. Migration Readiness\n",
    "readiness = analytics.get_migration_readiness()\n",
    "print(f\"\\n🚀 Migration Readiness Assessment:\")\n",
    "print(f\"  Overall Score: {readiness['overall_score']}\")\n",
    "print(f\"  Complexity Score: {readiness['complexity_score']:.1f}/100\")\n",
    "print(f\"  Dependency Score: {readiness['dependency_score']:.1f}/100\")\n",
    "\n",
    "if readiness['recommendations']:\n",
    "    print(f\"\\n  📋 Recommendations:\")\n",
    "    for rec in readiness['recommendations']:\n",
    "        print(f\"    • {rec}\")\n",
    "else:\n",
    "    print(f\"\\n  ✅ No specific recommendations - system looks healthy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Visualization Queries\n",
    "\n",
    "Queries that prepare data for visualization and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data for visualization tools\n",
    "network_data_query = \"\"\"\n",
    "MATCH (n:Node)-[r]->(m:Node)\n",
    "WHERE n.node_type IN ['PIPELINE', 'OPERATION', 'DATA_ASSET'] \n",
    "  AND m.node_type IN ['PIPELINE', 'OPERATION', 'DATA_ASSET']\n",
    "RETURN \n",
    "    n.node_id as source,\n",
    "    n.name as source_name,\n",
    "    n.node_type as source_type,\n",
    "    type(r) as relationship,\n",
    "    m.node_id as target,\n",
    "    m.name as target_name,\n",
    "    m.node_type as target_type\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "network_data = execute_and_fetch(network_data_query)\n",
    "\n",
    "# Convert to format suitable for network visualization libraries\n",
    "nodes = set()\n",
    "edges = []\n",
    "\n",
    "for row in network_data:\n",
    "    source_id, source_name, source_type, rel_type, target_id, target_name, target_type = row\n",
    "    \n",
    "    nodes.add((source_id, source_name, source_type))\n",
    "    nodes.add((target_id, target_name, target_type))\n",
    "    \n",
    "    edges.append({\n",
    "        \"source\": source_id,\n",
    "        \"target\": target_id,\n",
    "        \"relationship\": rel_type,\n",
    "        \"source_name\": source_name,\n",
    "        \"target_name\": target_name\n",
    "    })\n",
    "\n",
    "# Convert nodes to list\n",
    "nodes_list = [{\"id\": node[0], \"name\": node[1], \"type\": node[2]} for node in nodes]\n",
    "\n",
    "print(f\"📊 Network Visualization Data:\")\n",
    "print(f\"  Nodes: {len(nodes_list)}\")\n",
    "print(f\"  Edges: {len(edges)}\")\n",
    "\n",
    "# Show type distribution\n",
    "type_counts = {}\n",
    "for node in nodes_list:\n",
    "    node_type = node['type']\n",
    "    type_counts[node_type] = type_counts.get(node_type, 0) + 1\n",
    "\n",
    "print(f\"\\n  Node Type Distribution:\")\n",
    "for node_type, count in type_counts.items():\n",
    "    print(f\"    {node_type}: {count}\")\n",
    "\n",
    "# Show relationship distribution\n",
    "rel_counts = {}\n",
    "for edge in edges:\n",
    "    rel_type = edge['relationship']\n",
    "    rel_counts[rel_type] = rel_counts.get(rel_type, 0) + 1\n",
    "\n",
    "print(f\"\\n  Relationship Type Distribution:\")\n",
    "for rel_type, count in rel_counts.items():\n",
    "    print(f\"    {rel_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical data for tree visualizations\n",
    "hierarchy_query = \"\"\"\n",
    "MATCH (p:Node {node_type: 'PIPELINE'})\n",
    "OPTIONAL MATCH (p)-[:CONTAINS]->(op:Node {node_type: 'OPERATION'})\n",
    "OPTIONAL MATCH (op)-[:READS_FROM|WRITES_TO]->(da:Node {node_type: 'DATA_ASSET'})\n",
    "RETURN \n",
    "    p.name as package,\n",
    "    collect(DISTINCT {\n",
    "        name: op.name,\n",
    "        type: op.operation_type,\n",
    "        assets: [(op)-[:READS_FROM|WRITES_TO]->(asset:Node {node_type: 'DATA_ASSET'}) | asset.name]\n",
    "    }) as operations\n",
    "\"\"\"\n",
    "\n",
    "hierarchy_data = execute_and_fetch(hierarchy_query)\n",
    "\n",
    "print(\"🌳 Hierarchical Data Structure:\")\n",
    "for package_data in hierarchy_data[:3]:  # Show first 3 packages\n",
    "    package_name = package_data[0]\n",
    "    operations = package_data[1]\n",
    "    \n",
    "    print(f\"\\n📦 {package_name}\")\n",
    "    for op in operations[:3]:  # Show first 3 operations\n",
    "        if op and op.get('name'):\n",
    "            print(f\"  ├── {op['name']} ({op.get('type', 'Unknown')})\")\n",
    "            if op.get('assets'):\n",
    "                for asset in op['assets'][:2]:  # Show first 2 assets\n",
    "                    print(f\"  │   └── {asset}\")\n",
    "                if len(op['assets']) > 2:\n",
    "                    print(f\"  │   └── ... and {len(op['assets']) - 2} more\")\n",
    "    \n",
    "    if len(operations) > 3:\n",
    "        print(f\"  └── ... and {len(operations) - 3} more operations\")\n",
    "\n",
    "print(f\"\\n📊 Summary: {len(hierarchy_data)} packages prepared for hierarchical visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query Performance Tips\n",
    "\n",
    "Best practices for writing efficient Cypher queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: different query patterns\n",
    "import time\n",
    "\n",
    "def time_query(query_name, query, params=None):\n",
    "    \"\"\"Time a query execution\"\"\"\n",
    "    start_time = time.time()\n",
    "    results = execute_and_fetch(query, params)\n",
    "    execution_time = (time.time() - start_time) * 1000\n",
    "    return execution_time, len(results)\n",
    "\n",
    "print(\"⚡ Query Performance Comparison:\")\n",
    "\n",
    "# 1. Inefficient: No filtering early\n",
    "inefficient_query = \"\"\"\n",
    "MATCH (n:Node)-[r]->(m:Node)\n",
    "WHERE n.node_type = 'PIPELINE' AND m.node_type = 'OPERATION'\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    "\n",
    "# 2. Efficient: Filter early in MATCH\n",
    "efficient_query = \"\"\"\n",
    "MATCH (n:Node {node_type: 'PIPELINE'})-[r]->(m:Node {node_type: 'OPERATION'})\n",
    "RETURN count(*)\n",
    "\"\"\"\n",
    "\n",
    "# 3. Most efficient: Use materialized view when available\n",
    "materialized_query = \"\"\"\n",
    "MATCH (v:Node {id: 'view:graph_summary_stats'})\n",
    "RETURN JSON_EXTRACT(v.properties, '$.data') as stats\n",
    "\"\"\"\n",
    "\n",
    "# Test queries\n",
    "time1, results1 = time_query(\"Inefficient (filter in WHERE)\", inefficient_query)\n",
    "time2, results2 = time_query(\"Efficient (filter in MATCH)\", efficient_query)\n",
    "time3, results3 = time_query(\"Materialized View\", materialized_query)\n",
    "\n",
    "print(f\"  1. Inefficient query: {time1:.2f}ms\")\n",
    "print(f\"  2. Efficient query: {time2:.2f}ms ({time1/time2:.1f}x faster)\")\n",
    "print(f\"  3. Materialized view: {time3:.2f}ms ({time1/time3:.1f}x faster)\")\n",
    "\n",
    "print(\"\\n📝 Performance Tips:\")\n",
    "print(\"  • Filter early in MATCH clauses, not WHERE clauses\")\n",
    "print(\"  • Use materialized views for frequently-accessed data\")\n",
    "print(\"  • Limit results with LIMIT when doing exploratory queries\")\n",
    "print(\"  • Use OPTIONAL MATCH only when necessary\")\n",
    "print(\"  • Consider using WITH to break complex queries into steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered advanced querying techniques including:\n",
    "\n",
    "1. **Complex Path Analysis** - Multi-hop queries for data lineage and bottleneck detection\n",
    "2. **Advanced Aggregations** - Comprehensive system complexity analysis\n",
    "3. **Pattern Matching** - ETL pattern detection and anti-pattern identification\n",
    "4. **Performance Optimization** - Using materialized views for fast queries\n",
    "5. **Custom Analytics** - Building reusable analysis functions\n",
    "6. **Visualization Data** - Preparing data for network and hierarchical visualizations\n",
    "7. **Performance Tips** - Best practices for efficient Cypher queries\n",
    "\n",
    "### Key Takeaways:\n",
    "- Materialized views provide significant performance benefits (10-100x faster)\n",
    "- Pattern matching can identify architectural issues early\n",
    "- Custom analytics functions make complex analysis reusable\n",
    "- Early filtering in MATCH clauses improves query performance\n",
    "- The graph structure enables sophisticated path and dependency analysis\n",
    "\n",
    "### Next Steps:\n",
    "- Combine these techniques in migration analysis scenarios\n",
    "- Build custom dashboards using the visualization data formats\n",
    "- Create monitoring queries for ongoing system health assessment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}